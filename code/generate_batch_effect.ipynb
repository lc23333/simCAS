{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fd3bc3-532d-4a95-85d2-2d9f49e754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import os\n",
    "from scipy import sparse\n",
    "import scipy.io as sio\n",
    "import scanpy as sc\n",
    "from Bio import Phylo\n",
    "from io import StringIO\n",
    "import logging\n",
    "from scipy.optimize import fsolve\n",
    "import random\n",
    "import threading\n",
    "import scipy.stats as stats\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from scipy.stats import logser\n",
    "\n",
    "from scipy.special import rel_entr\n",
    "from statsmodels.discrete.count_model import (ZeroInflatedNegativeBinomialP, ZeroInflatedPoisson,\n",
    "                                              ZeroInflatedGeneralizedPoisson)\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import nbinom\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87402a-1fd4-4376-99e6-c009c9155fdf",
   "metadata": {},
   "source": [
    "# count数据 - Buenrostro2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840b918-3486-42d6-bdc0-8fb27d70e633",
   "metadata": {},
   "source": [
    "## extrinsic: library size effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4a12dcca-0e8e-4674-9b1c-0adc988b5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取参数\n",
    "prefix_='Buenrostro_2018'\n",
    "# prefix_='Forebrain'\n",
    "# prefix_='MCA/Cerebellum'\n",
    "resultdir='/data1/lichen/code/second/scATAC_integration/data/scATACdata_total/process/{0}/'.format(prefix_)\n",
    "peak_mean=pd.read_csv(resultdir+'peak_mean_log.csv',index_col=0)\n",
    "lib_size=pd.read_csv(resultdir+'library_size_log.csv',index_col=0)\n",
    "nozero=pd.read_csv(resultdir+'nozero_log.csv',index_col=0)\n",
    "\n",
    "peak_mean=np.array(peak_mean['peak mean'])\n",
    "lib_size=np.array(lib_size['library size'])\n",
    "nozero=np.array(nozero['nozero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "09984f29-53cf-4e32-b66c-2346bb534d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数，后续所有函数中的参数都在下面给出定义\n",
    "\n",
    "n_peak         =len(peak_mean) # peak数目\n",
    "n_cell_total   =1500 #总共的细胞数目\n",
    "rand_seed      =2022 #随机种子\n",
    "zero_prob      =0.5  #对于peak_effect的置零个数\n",
    "zero_set       ='all'#'by_row'指的是对于每一个peak的effect vector进行置零；'all'指的是随机在所有的index中选择进行置零\n",
    "effect_mean    =0 #生成effect vector的均值\n",
    "effect_sd      =1 #生成effect vector的方差\n",
    "\n",
    "min_popsize    =50 #离散模式下设定的细胞群的最小数目\n",
    "min_pop        ='A' #离散模式下设定最小细胞群的名称，注意需要与下面的tree_text一致\n",
    "tree_text      =[\"((A:0.5,B:0.5):1,C:1);\", #注：前三个用来仿真离散模式，只标叶子结点名称就行；后两个为连续模式的仿真树，与标准newick形式略有不同\n",
    "                 \"((A:0.4,B:0.4,C:0.4):1,(D:1,E:1):1);\",\n",
    "                \"(((A:0.2,B:0.2):0.2,C:0.4):0.5,(D:1,E:1):1);\",\n",
    "                '((((A:1, B:1)C:1,(D:0.2, E:0.2)F:1)G:1, H:3)S)',\n",
    "                \"(((A:1,B:1)C:1,(D:1,E:1)F:1)S)\"]\n",
    "pops_name      =[['A','B','C'],\n",
    "                ['A','B','C','D','E'],\n",
    "                ['A','B','C','D','E'],\n",
    "                 ['A','B','C'],\n",
    "                ]  # 输入不同节点的名字，离散模式只需要输入叶子节点的名称就行，注意这里需要与tree_text的前三个顺序保持一致\n",
    "pops_size      =[600,600,300] # 设置不同cluster的细胞数目，None则直接取平均\n",
    "# pops_size      =None\n",
    "\n",
    "\n",
    "embed_mean_same=1 # 对embedding非差异特征采样的均值\n",
    "embed_sd_same  =0.5 # 对embedding的非差异特征采样的方差\n",
    "embed_mean_diff=1 # 对embedding差异特征采样的均值\n",
    "embed_sd_diff  =0.5 # 对embedding的差异特征采样的方差\n",
    "\n",
    "len_cell_embed =12   #仿真细胞的低维特征的特征个数\n",
    "n_embed_diff   =10 # 使得cell embedding不同的特征维度数目\n",
    "n_embed_same   =len_cell_embed-n_embed_diff\n",
    "\n",
    "simu_type      ='discrete' # continuous/discrete/single/cell_type\n",
    "correct_iter   =2 # 使用参数进行修正的迭代次数\n",
    "activation     ='exp_linear' #对参数矩阵矫正的方式，在连续和离散的条件下使用'exp'，在仿真celltype的时候应该使用'sigmod'\n",
    "\n",
    "two_embeds     =True  # true表明peak mean和library size通过两个不同的矩阵排序对应得到；False 表明通过一个矩阵的值排序对应得到\n",
    "\n",
    "adata_dir      =resultdir+'adata_forsimulation.h5ad' # 为了进行cell_type simulation\n",
    "lib_simu       ='estimate' # 在仿真cell_type时用的参数，’real‘表示直接使用真实的library_size参数，‘estimate’表示从估计的分布中采样\n",
    "distribution   ='Poisson' # 数据的分布，如果二值化就是’Bernoulli‘，count就是‘Poisson’\n",
    "\n",
    "bw_pm          =1e-4 #分别为对peak mean、library_size、nozero的核密度估计的窗宽；注：bw_pm若取的过大可能会导致采样的peak mean小于0而报错\n",
    "bw_lib         =0.05\n",
    "bw_nozero      =0.05\n",
    "\n",
    "real_param     =False # 是否使用真实的参数，True则为直接使用真实参数，False\n",
    "\n",
    "log            =None\n",
    "\n",
    "fix_seed(rand_seed)\n",
    "\n",
    "k_dict,pi_dict={},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d88763e3-830a-4b17-b371-b718cd627c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********start generate effect vector...**********\n",
      "**********generate effect finished!**********\n",
      "**********start generate cell embedding...**********\n",
      "**********generate cell embedding finished**********\n"
     ]
    }
   ],
   "source": [
    "# 生成effect和embedding\n",
    "print(\"**********start generate effect vector...**********\")\n",
    "peak_effect,lib_size_effect=Get_Effect(n_peak,n_cell_total,\n",
    "                len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd)\n",
    "print(\"**********generate effect finished!**********\")\n",
    "\n",
    "print(\"**********start generate cell embedding...**********\")\n",
    "embeds_peak,meta=Get_Discrete_Embedding(pops_name[0],min_popsize,tree_text[0],\n",
    "             n_cell_total,pops_size,\n",
    "             embed_mean_same,embed_sd_same,\n",
    "              embed_mean_diff,embed_sd_diff,\n",
    "             n_embed_diff,n_embed_same,rand_seed)\n",
    "embeds_lib,meta=Get_Discrete_Embedding(pops_name[0],min_popsize,tree_text[0],\n",
    "             n_cell_total,pops_size,\n",
    "             embed_mean_same,embed_sd_same,\n",
    "              embed_mean_diff,embed_sd_diff,\n",
    "             n_embed_diff,n_embed_same,rand_seed+1)\n",
    "embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "print(\"**********generate cell embedding finished**********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4ee9bd3b-f09e-420d-ba77-f77ebeb3e294",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_iter 1\n",
      "correct_iter 2\n",
      "**********start ZIP correction...**********\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lichen/anaconda3/envs/pytorch2/lib/python3.8/site-packages/scipy/optimize/minpack.py:162: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1200\n",
      "1400\n",
      "**********ZIP correction finished!**********\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(rand_seed)\n",
    "if distribution=='Bernoulli' and np.max(np.exp(peak_mean)-1)>1:\n",
    "    raise ValueError('you data may not be Bernoulli distribution!')\n",
    "\n",
    "if real_param: #如果直接使用真实参数，peak mean直接按照真实参数来，lib size抽样\n",
    "    param_pm=np.sort(peak_mean,axis=0).ravel()\n",
    "    param_lib=np.sort(np.random.choice(lib_size,size=n_cell_total),axis=0).ravel()\n",
    "    param_nozero=np.sort(np.random.choice(nozero,size=n_cell_total),axis=0).ravel()\n",
    "else:\n",
    "    # kde\n",
    "    kde_pm = KernelDensity(kernel='gaussian', bandwidth=bw_pm).fit(peak_mean.reshape(-1,1))\n",
    "    kde_lib = KernelDensity(kernel='gaussian', bandwidth=bw_lib).fit(lib_size.reshape(-1,1))\n",
    "    kde_nozero = KernelDensity(kernel='gaussian', bandwidth=bw_nozero).fit(nozero.reshape(-1,1))\n",
    "\n",
    "    # 从kde中采样并进行排序（从小到大）\n",
    "    param_pm=kde_pm.sample(n_peak,random_state=rand_seed)\n",
    "    param_lib=kde_lib.sample(n_cell_total,random_state=rand_seed)\n",
    "    param_nozero=kde_nozero.sample(n_cell_total,random_state=rand_seed)\n",
    "\n",
    "    param_pm=np.sort(param_pm,axis=0).ravel()\n",
    "    param_lib=np.sort(param_lib,axis=0).ravel()\n",
    "    param_nozero=np.sort(param_nozero,axis=0).ravel()\n",
    "\n",
    "\n",
    "# 从模拟矩阵的参数顺序对应到采样的真实参数\n",
    "X_peak=np.dot(peak_effect,embeds_peak)# peak*cell\n",
    "X_peak=Activation(X_peak,method=activation) # 防止出现负值\n",
    "rank=np.arange(len(X_peak))[np.mean(X_peak,axis=1).argsort().argsort()]\n",
    "param_pm=param_pm[rank]\n",
    "\n",
    "if two_embeds:\n",
    "    X_lib=np.dot(lib_size_effect,embeds_lib).ravel()\n",
    "else:\n",
    "    X_lib=np.dot(lib_size_effect,embeds_peak).ravel()\n",
    "rank = np.arange(len(X_lib))[X_lib.argsort().argsort()]\n",
    "param_lib=param_lib[rank]\n",
    "param_nozero=param_nozero[rank]\n",
    "\n",
    "# 对参数进行修正\n",
    "# X_peak维度是peak*cell\n",
    "simu_param_peak=X_peak.copy()\n",
    "if distribution=='Poisson':\n",
    "    for i in range(correct_iter):\n",
    "        print('correct_iter '+str(i+1))\n",
    "        simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=1).reshape(-1,1)+1e-8)*((np.exp(param_pm)-1).reshape(-1,1))*simu_param_peak.shape[1]\n",
    "        simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=0).reshape(1,-1)+1e-8)*((np.exp(param_lib)-1).reshape(1,-1))\n",
    "\n",
    "    simu_param_lib=np.exp(param_lib)-1\n",
    "    simu_param_nozero=np.exp(param_nozero)-1\n",
    "    #--------使用poisson分布生成ATAC\n",
    "    lambdas=simu_param_peak.copy()\n",
    "    # lambdas=simu_param_peak*(simu_param_lib.reshape(1,-1))\n",
    "\n",
    "    # 对sparsity进行修正\n",
    "    lambdas_sum=np.sum(lambdas,axis=0)\n",
    "    \n",
    "    lambdas_sum_copy=lambdas_sum.copy()\n",
    "\n",
    "#     print(\"**********start ZIP correction...**********\")\n",
    "#     k_list,pi_list=[],[]\n",
    "#     # 求解每个cell中lambda扩大的倍数和置零的比例\n",
    "#     for i in range(n_cell_total):\n",
    "#         iter_=i\n",
    "#         # print('{}:'.format(i),simu_param_lib[iter_]/(lambdas_sum[iter_]))\n",
    "#         # print(i)\n",
    "#         def solve_function(unsolved_value):\n",
    "#             k,pi=unsolved_value[0],unsolved_value[1]\n",
    "#             return [\n",
    "#                 k*(1-pi)-simu_param_lib[iter_]/(lambdas_sum[iter_]),\n",
    "#                 n_peak*pi+(1-pi)*np.sum(np.exp(-lambdas[:,iter_]*k))-(n_peak-simu_param_nozero[iter_])\n",
    "#             ]\n",
    "\n",
    "#         solved=fsolve(solve_function,[3,0.5],maxfev=2000)\n",
    "#         k,pi=solved[0],solved[1]\n",
    "#         simu1=k*(1-pi)*(lambdas_sum[iter_])\n",
    "#         real1=simu_param_lib[iter_]\n",
    "#         if abs(simu1-real1)/real1>0.1:\n",
    "#             print('=================================')\n",
    "#             print(i)\n",
    "#             print(simu1,real1)\n",
    "#             # print('=================================')\n",
    "#             solved=fsolve(solve_function,[20,0.5],maxfev=2000)\n",
    "#         simu1=solved[0]*(1-solved[1])*(lambdas_sum[iter_])\n",
    "#         real1=simu_param_lib[iter_]\n",
    "#         if abs(simu1-real1)/real1>0.1:\n",
    "#             print(i)\n",
    "#             print(simu1,real1)\n",
    "#             print(\"=================================\")\n",
    "\n",
    "#         k_list.append(solved[0])\n",
    "#         pi_list.append(solved[1])\n",
    "        \n",
    "#         # print('{}:'.format(i),solved[0],solved[1])\n",
    "        \n",
    "#         # break\n",
    "#     # 对每个cell的lambda置零并扩大相应倍数\n",
    "#     for i in range(n_cell_total):\n",
    "#         if k_list[i]==3 or k_list[i]==20 or pi_list[i]<0:\n",
    "#             continue\n",
    "#         a=lambdas[:,i]*k_list[i]\n",
    "#         # print(i)\n",
    "#         # print(k_list[i],pi_list[i])\n",
    "#         # print(\"=============================\")\n",
    "#         # b=atac_counts[:,i]\n",
    "#         a[np.random.choice(n_peak,replace=False,size=int(pi_list[i]*n_peak))]=0\n",
    "#         lambdas[:,i]=a\n",
    "#     print(\"**********ZIP correction finished!**********\")\n",
    "    \n",
    "    print(\"**********start ZIP correction...**********\")\n",
    "    batch_size = 1000 # 并行数目，全局字典\n",
    "    k_dict,pi_dict={},{}\n",
    "    for i in range(0,n_cell_total,batch_size):\n",
    "        if i+batch_size<=n_cell_total:\n",
    "            my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, i+batch_size)]\n",
    "        else:\n",
    "            my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, n_cell_total)]\n",
    "        for thread_ in my_thread:\n",
    "            thread_.start()\n",
    "        for thread_ in my_thread:\n",
    "            thread_.join()\n",
    "    # 对每个cell的lambda置零并扩大相应倍数\n",
    "    for i in range(n_cell_total):\n",
    "        if k_dict[i]==3 or k_dict[i]==20 or pi_dict[i]<0:\n",
    "            continue\n",
    "        a=lambdas[:,i]*k_dict[i]\n",
    "        # b=atac_counts[:,i]\n",
    "        a[np.random.choice(n_peak,replace=False,size=int(pi_dict[i]*n_peak))]=0\n",
    "        lambdas[:,i]=a\n",
    "\n",
    "    print(\"**********ZIP correction finished!**********\")\n",
    "\n",
    "    atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "elif distribution=='Bernoulli':\n",
    "    for i in range(correct_iter):\n",
    "        print('correct_iter '+str(i+1))\n",
    "        simu_param_peak=Bernoulli_pm_correction(simu_param_peak,param_pm)\n",
    "        simu_param_peak=Bernoulli_lib_correction(simu_param_peak,param_lib)\n",
    "    atac_counts=np.random.binomial(1,p=simu_param_peak,size=simu_param_peak.shape)\n",
    "\n",
    "# return atac_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f9da3ed1-c325-49f7-a2d5-6f37ac7f0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=np.random.normal(8,0.5,lambdas.shape)\n",
    "lambdas_noise=lambdas+noise\n",
    "lambdas_noise[lambdas_noise<0]=0\n",
    "temp=lambdas.copy()\n",
    "temp[temp>0]=1\n",
    "lambdas_noise=lambdas_noise*temp\n",
    "atac_counts=np.random.poisson(lambdas_noise, lambdas_noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3c7e670f-61f7-448e-9971-801ce3b4eff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27491/3698852136.py:3: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  adata_noise=anndata.AnnData(X=lambdas.T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the noise is: 1.8826382228322531\n",
      "the noise is: 0.6635215176695732\n",
      "the noise is: 0.5\n"
     ]
    }
   ],
   "source": [
    "# 生成anndata并按照每个celltype的平均library size按比例添加batch effect\n",
    "import anndata\n",
    "adata_noise=anndata.AnnData(X=lambdas.T)\n",
    "adata_noise.obs['celltype']=meta\n",
    "# adata_noise.var=adata.var\n",
    "\n",
    "celltype_noise=np.unique(adata_noise.obs.celltype)\n",
    "lib_size_list=[]\n",
    "for celltype_ in celltype_noise:\n",
    "    adata_part=adata_noise[adata_noise.obs.celltype.isin([celltype_]),:]\n",
    "    lib_size_tmp=np.mean(cal_lib(adata_part))\n",
    "    lib_size_list.append(lib_size_tmp)\n",
    "min_lib_size=min(lib_size_list)\n",
    "\n",
    "array_list=[]\n",
    "for i,celltype_ in enumerate(celltype_noise):\n",
    "    adata_part=adata_noise[adata_noise.obs.celltype.isin([celltype_]),:]\n",
    "    temp=adata_part.X.copy()\n",
    "    print('the noise is:',0.5*lib_size_list[i]/min_lib_size)\n",
    "    lambdas_tmp=temp+np.random.normal(0.5*lib_size_list[i]/min_lib_size,0.5,temp.shape)\n",
    "    lambdas_tmp[lambdas_tmp<0]=0\n",
    "    temp[temp>0]=1\n",
    "    lambdas_tmp=lambdas_tmp*temp\n",
    "    array_list.append(lambdas_tmp)\n",
    "    \n",
    "lambdas_noise=np.vstack(array_list).T\n",
    "atac_counts=np.random.poisson(lambdas_noise, lambdas_noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e342ae96-3a40-422d-b109-4e822e79ef81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'C'], dtype=object)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celltype_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6c1da11e-7703-4ad6-94a1-0adb608f7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(lambdas_tmp),lambdas_tmp.shape\n",
    "# lambdas_tmp[lambdas_tmp<0]=0\n",
    "# np.min(lambdas_tmp),adata_part.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8ac5ddad-1d92-4a8c-9aef-70ce4180ea3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, (300, 169221), (900, 169221), (169221, 900))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array_list),array_list[0].shape,np.vstack(array_list).shape,atac_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "14c367d0-560d-4e55-9469-f564fec95d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.vstack(array_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e20c04d3-726a-46fc-b8e1-8a70b17a999f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********start generate effect vector...**********\n",
      "**********generate effect finished!**********\n",
      "**********start generate cell embedding...**********\n",
      "simulation type is discrete\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation type is \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(simu_type))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simu_type\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscrete\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 重复两次获得两个矩阵，后续使用参数two_embeds决定是用两个矩阵还是用一个\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     embeds_peak,meta\u001b[38;5;241m=\u001b[39m\u001b[43mGet_Discrete_Embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpops_name\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_popsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtree_text\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mn_cell_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpops_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                 \u001b[49m\u001b[43membed_mean_same\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_sd_same\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                  \u001b[49m\u001b[43membed_mean_diff\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_sd_diff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mn_embed_diff\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_embed_same\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrand_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     embeds_lib,meta\u001b[38;5;241m=\u001b[39mGet_Discrete_Embedding(pops_name[\u001b[38;5;241m1\u001b[39m],min_popsize,tree_text[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     19\u001b[0m                  n_cell_total,pops_size,\n\u001b[1;32m     20\u001b[0m                  embed_mean_same,embed_sd_same,\n\u001b[1;32m     21\u001b[0m                   embed_mean_diff,embed_sd_diff,\n\u001b[1;32m     22\u001b[0m                  n_embed_diff,n_embed_same,rand_seed\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m     embeds_peak,embeds_lib\u001b[38;5;241m=\u001b[39membeds_peak\u001b[38;5;241m.\u001b[39mvalues,embeds_lib\u001b[38;5;241m.\u001b[39mvalues\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mGet_Discrete_Embedding\u001b[0;34m(pops_name, min_popsize, tree_text, n_cell_total, pops_size, embed_mean_same, embed_sd_same, embed_mean_diff, embed_sd_diff, n_embed_diff, n_embed_same, rand_seed)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pop_name_size\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (i,name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pops_name):\n\u001b[0;32m---> 33\u001b[0m         pop_name_size[name]\u001b[38;5;241m=\u001b[39m\u001b[43mpops_size\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# 将float转化为int \u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m pop_name_size\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 生成effect和embedding\n",
    "print(\"**********start generate effect vector...**********\")\n",
    "peak_effect,lib_size_effect=Get_Effect(n_peak,n_cell_total,\n",
    "                len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd)\n",
    "print(\"**********generate effect finished!**********\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"**********start generate cell embedding...**********\")\n",
    "print(\"simulation type is {0}\".format(simu_type))\n",
    "if simu_type=='discrete':\n",
    "    # 重复两次获得两个矩阵，后续使用参数two_embeds决定是用两个矩阵还是用一个\n",
    "    embeds_peak,meta=Get_Discrete_Embedding(pops_name[1],min_popsize,tree_text[1],\n",
    "                 n_cell_total,pops_size,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed)\n",
    "    embeds_lib,meta=Get_Discrete_Embedding(pops_name[1],min_popsize,tree_text[1],\n",
    "                 n_cell_total,pops_size,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed+1)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished**********\")\n",
    "    # 获得count\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero,\n",
    "                                real_param)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='continuous':\n",
    "    embeds_param={}\n",
    "    embeds_peak,meta=Get_Continuous_Embedding(tree_text[4],n_cell_total,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed)\n",
    "    embeds_lib,meta=Get_Continuous_Embedding(tree_text[4],n_cell_total,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed+1)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished**********\")\n",
    "    \n",
    "    print(\"**********start generate counts...**********\")\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero,\n",
    "                               real_param)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='single':\n",
    "    embeds_param={}\n",
    "    embeds_peak,meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "    embeds_lib,meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished!**********\")\n",
    "    \n",
    "    \n",
    "    print(\"**********start generate counts...**********\")\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='cell_type':\n",
    "    adata=sc.read_h5ad(adata_dir)\n",
    "    counts_list,celltype_list,embed_peak_list,embed_lib_list=[],[],[],[]\n",
    "    lambdas_list,simu_param_nozero_list,simu_param_lib_list,simu_param_pm_list=[],[],[],[]#新加的list用来重新对lambdas进行spasity的修正\n",
    "    celltypes=np.unique(adata.obs.celltype)\n",
    "    for i in range(len(celltypes)):\n",
    "    # 可以分为直接从真实数据中进行采样或是从核密度估计中采样特定细胞数目，先做直接从真实数据中采样的结果\n",
    "        # print(celltypes[i])\n",
    "        print(\"simulating cell type: {}...\".format(celltypes[i]))\n",
    "        adata_part=adata[adata.obs.celltype==celltypes[i],:]\n",
    "\n",
    "        # 对每个celltype单独进行仿真\n",
    "        counts,embed_peak,embed_lib,lambdas,simu_param_nozero,simu_param_lib,simu_param_pm=Get_Celltype_Counts(adata_part,two_embeds,\n",
    "                                            embed_mean_same,embed_sd_same,\n",
    "                     n_embed_diff,n_embed_same,correct_iter,lib_simu=lib_simu,n_cell_total=None,\n",
    "                                        distribution=distribution,activation=activation,\n",
    "                    bw_pm=bw_pm,bw_lib=bw_lib,bw_nozero=bw_nozero,rand_seed=rand_seed) # peak*cell\n",
    "\n",
    "        counts_list.append(counts)\n",
    "        embed_peak_list.append(embed_peak)\n",
    "        embed_lib_list.append(embed_lib)\n",
    "        celltype_list.append([celltypes[i]]*counts.shape[1])\n",
    "        lambdas_list.append(lambdas)\n",
    "        simu_param_nozero_list.append(simu_param_nozero)\n",
    "        simu_param_lib_list.append(simu_param_lib)\n",
    "        simu_param_pm_list.append(simu_param_pm)\n",
    "        \n",
    "    if distribution=='Poisson':\n",
    "        # atac_counts=np.hstack(counts_list)\n",
    "        meta=np.hstack(celltype_list)\n",
    "        embeds_peak=np.hstack(embed_peak_list)\n",
    "        embeds_lib=np.hstack(embed_lib_list)\n",
    "        #对整体lambdas进行sparsity修正\n",
    "        lambdas=np.hstack(lambdas_list)\n",
    "        simu_param_nozero=np.hstack(simu_param_nozero_list)\n",
    "        simu_param_lib=np.hstack(simu_param_lib_list)\n",
    "        simu_param_pm=peak_mean\n",
    "\n",
    "        lambdas_sum=np.sum(lambdas,axis=0)\n",
    "            \n",
    "        print(\"**********start ZIP correction...**********\")\n",
    "        batch_size = 1000 # 并行数目，全局字典\n",
    "        global k_dict,pi_dict\n",
    "        for i in range(0,n_cell_total,batch_size):\n",
    "            if i+batch_size<=n_cell_total:\n",
    "                my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, i+batch_size)]\n",
    "            else:\n",
    "                my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, n_cell_total)]\n",
    "            for thread_ in my_thread:\n",
    "                thread_.start()\n",
    "            for thread_ in my_thread:\n",
    "                thread_.join()\n",
    "        # 对每个cell的lambda置零并扩大相应倍数\n",
    "        for i in range(n_cell_total):\n",
    "            if k_dict[i]==3 or k_dict[i]==20 or pi_dict[i]<0 or k_dict[i]<0:\n",
    "                continue\n",
    "            a=lambdas[:,i]*k_dict[i]\n",
    "            # b=atac_counts[:,i]\n",
    "            a[np.random.choice(n_peak,replace=False,size=int(pi_dict[i]*n_peak))]=0\n",
    "            lambdas[:,i]=a\n",
    "            \n",
    "        print(\"**********ZIP correction finished!**********\")\n",
    "\n",
    "        # # spasity矫正完之后再来一轮peak mean和library size的矫正，保证都符合实际\n",
    "        # lambdas_copy=lambdas.copy()\n",
    "        # lambdas_copy=lambdas_copy/(np.sum(lambdas_copy,axis=1).reshape(-1,1)+1e-8)*(simu_param_pm.reshape(-1,1))*lambdas_copy.shape[1]\n",
    "        # lambdas_copy=lambdas_copy/(np.sum(lambdas_copy,axis=0).reshape(1,-1)+1e-8)*(simu_param_lib.reshape(1,-1))\n",
    "\n",
    "        atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "        \n",
    "    elif distribution=='Bernoulli':\n",
    "        atac_counts=np.hstack(counts_list)\n",
    "        meta=np.hstack(celltype_list)\n",
    "        embeds_peak=np.hstack(embed_peak_list)\n",
    "        embeds_lib=np.hstack(embed_lib_list)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('wrong distribution input!')\n",
    "    \n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "\n",
    "else:\n",
    "    raise ValueError('wrong simulation type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "523ba5dc-53ed-45e3-b0c9-06c6d6c24395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(atac_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7a5ee6d3-bd10-4490-b444-d4d5790d928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrete_embed12_diff10_cell1500_EmbedSd0.5_EffectSd1_prob0.5_CorrectIter2_TwoEmbeddTrue_activation_exp_linear_ExtrinsicEffect23\n"
     ]
    }
   ],
   "source": [
    "# -----------------保存数据\n",
    "\n",
    "# 创建对应的文件夹\n",
    "resultdir=\"/data1/lichen/code/second/scATAC_integration/code/simulation/data/{}/\".format(prefix_)\n",
    "\n",
    "prefix='_'.join([simu_type,'embed'+str(len_cell_embed),\n",
    "                'diff'+str(n_embed_diff),'cell'+str(n_cell_total),\n",
    "                'EmbedSd'+str(embed_sd_diff),'EffectSd'+str(effect_sd),'prob'+str(zero_prob),\n",
    "                'CorrectIter'+str(correct_iter),'TwoEmbedd'+str(two_embeds),\n",
    "                'activation_'+activation+'_ExtrinsicEffect23'])\n",
    "os.makedirs(os.path.join(resultdir,prefix),exist_ok=True)\n",
    "\n",
    "print(prefix)\n",
    "\n",
    "# save mtx\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix.mtx\"),sparse.csr_matrix(atac_counts.T))\n",
    "\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix_embed_pm.mtx\"),sparse.csr_matrix(embeds_peak.T))\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix_embed_lib.mtx\"),sparse.csr_matrix(embeds_lib.T))\n",
    "\n",
    "df=pd.DataFrame({'pop':meta})\n",
    "df.to_csv(os.path.join(resultdir,prefix,\"meta.tsv\"),sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90856637-fad0-42c4-a30c-b175697bc44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchEffect1: 三类，最基本的\n",
    "# BatchEffect2：结果上+N(0.5,0.5)的噪声\n",
    "\n",
    "# 1: exp 三类600、600、300\n",
    "# 2: exp 三类600、600、300 在peak effect上加上N(0.5,0.5)的噪声\n",
    "# 3: exp_linear k=4 三类600、600、300\n",
    "# 4: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(0.5,0.5)的噪声\n",
    "# 5: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(1,0.5)的噪声\n",
    "# 6: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(0,1)的噪声\n",
    "# 7: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(0,2)的噪声\n",
    "# 8: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(0.5,1)的噪声\n",
    "# 9: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(1.5,0.5)的噪声\n",
    "# 10: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(1,1)的噪声\n",
    "# 11: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(1,0.1)的噪声\n",
    "# 12: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(0.1,0.1)的噪声\n",
    "# 13: exp_linear k=4 三类600、600、300 在参数矩阵上加上N(0.5,2)的噪声\n",
    "\n",
    "# 14: exp_linear k=4 seed2023 三类600、600、300\n",
    "# 15: exp_linear k=4 seed2023 三类600、600、300 在参数矩阵上加上N(0.5,0.5)的噪声\n",
    "\n",
    "# 16: exp_linear k=4 seed2023 三类300、300、150\n",
    "# 17: exp_linear k=4 seed2023 三类300、300、150 在参数矩阵上加上N(0.5,0.5)的噪声\n",
    "\n",
    "# 18: exp_linear k=4 seed2024 三类300、300、300\n",
    "# 19: exp_linear k=4 seed2024 三类300、300、300 在参数矩阵上加上N(0.5,0.5)的噪声\n",
    "\n",
    "# 20: exp_linear k=4 seed2022 三类100、100、100\n",
    "# 21: exp_linear k=4 seed2022 三类100、100、100 在参数矩阵上加上N(0.5,0.5)的噪声\n",
    "\n",
    "# 22: exp_linear k=4  三类600、600、300 在参数矩阵上加上与类别libsize成比例的噪声 N(0.1,0.1)\n",
    "# 23: exp_linear k=4  三类600、600、300 在参数矩阵上加上与类别libsize成比例的噪声 N(0.5,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d5b38-258a-44bf-ba3e-fa4791cf83d6",
   "metadata": {},
   "source": [
    "## intrinsic:peak effect batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0aeb8b4d-6236-47f1-92f2-55eb54c72310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取参数\n",
    "prefix_='Buenrostro_2018'\n",
    "# prefix_='Forebrain'\n",
    "# prefix_='MCA/Cerebellum'\n",
    "resultdir='/data1/lichen/code/second/scATAC_integration/data/scATACdata_total/process/{0}/'.format(prefix_)\n",
    "peak_mean=pd.read_csv(resultdir+'peak_mean_log.csv',index_col=0)\n",
    "lib_size=pd.read_csv(resultdir+'library_size_log.csv',index_col=0)\n",
    "nozero=pd.read_csv(resultdir+'nozero_log.csv',index_col=0)\n",
    "\n",
    "peak_mean=np.array(peak_mean['peak mean'])\n",
    "lib_size=np.array(lib_size['library size'])\n",
    "nozero=np.array(nozero['nozero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fa11ab7-8e25-4728-8a00-33f998c7229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数，后续所有函数中的参数都在下面给出定义\n",
    "\n",
    "n_peak         =len(peak_mean) # peak数目\n",
    "n_cell_total   =2000 #总共的细胞数目\n",
    "rand_seed      =2022 #随机种子\n",
    "zero_prob      =0.5  #对于peak_effect的置零个数\n",
    "zero_set       ='all'#'by_row'指的是对于每一个peak的effect vector进行置零；'all'指的是随机在所有的index中选择进行置零\n",
    "effect_mean    =0 #生成effect vector的均值\n",
    "effect_sd      =1 #生成effect vector的方差\n",
    "\n",
    "min_popsize    =300 #离散模式下设定的细胞群的最小数目\n",
    "min_pop        ='A' #离散模式下设定最小细胞群的名称，注意需要与下面的tree_text一致\n",
    "tree_text      =[\"((A:0.5,B:0.5):1,C:1);\", #注：前三个用来仿真离散模式，只标叶子结点名称就行；后两个为连续模式的仿真树，与标准newick形式略有不同\n",
    "                 \"((A:0.4,B:0.4,C:0.4):1,(D:1,E:1):1);\",\n",
    "                \"(((A:0.2,B:0.2):0.2,C:0.4):0.5,(D:1,E:1):1);\",\n",
    "                '((((A:1, B:1)C:1,(D:0.2, E:0.2)F:1)G:1, H:3)S)',\n",
    "                \"(((A:1,B:1)C:1,(D:1,E:1)F:1)S)\"]\n",
    "pops_name      =[['A','B','C'],\n",
    "                ['A','B','C','D','E'],\n",
    "                ['A','B','C','D','E'],\n",
    "                 ['A','B','C'],\n",
    "                ]  # 输入不同节点的名字，离散模式只需要输入叶子节点的名称就行，注意这里需要与tree_text的前三个顺序保持一致\n",
    "pops_size      =[800,800,400] # 设置不同cluster的细胞数目，None则直接取平均\n",
    "# pops_size      =None\n",
    "\n",
    "\n",
    "embed_mean_same=1 # 对embedding非差异特征采样的均值\n",
    "embed_sd_same  =0.5 # 对embedding的非差异特征采样的方差\n",
    "embed_mean_diff=1 # 对embedding差异特征采样的均值\n",
    "embed_sd_diff  =0.5 # 对embedding的差异特征采样的方差\n",
    "\n",
    "len_cell_embed =12   #仿真细胞的低维特征的特征个数\n",
    "n_embed_diff   =10 # 使得cell embedding不同的特征维度数目\n",
    "n_embed_same   =len_cell_embed-n_embed_diff\n",
    "\n",
    "simu_type      ='discrete' # continuous/discrete/single/cell_type\n",
    "correct_iter   =2 # 使用参数进行修正的迭代次数\n",
    "activation     ='exp' #对参数矩阵矫正的方式，在连续和离散的条件下使用'exp'，在仿真celltype的时候应该使用'sigmod'\n",
    "\n",
    "two_embeds     =True  # true表明peak mean和library size通过两个不同的矩阵排序对应得到；False 表明通过一个矩阵的值排序对应得到\n",
    "\n",
    "adata_dir      =resultdir+'adata_forsimulation.h5ad' # 为了进行cell_type simulation\n",
    "lib_simu       ='estimate' # 在仿真cell_type时用的参数，’real‘表示直接使用真实的library_size参数，‘estimate’表示从估计的分布中采样\n",
    "distribution   ='Poisson' # 数据的分布，如果二值化就是’Bernoulli‘，count就是‘Poisson’\n",
    "\n",
    "bw_pm          =1e-4 #分别为对peak mean、library_size、nozero的核密度估计的窗宽；注：bw_pm若取的过大可能会导致采样的peak mean小于0而报错\n",
    "bw_lib         =0.05\n",
    "bw_nozero      =0.05\n",
    "\n",
    "real_param     =False # 是否使用真实的参数，True则为直接使用真实参数，False\n",
    "\n",
    "log            =None\n",
    "\n",
    "fix_seed(rand_seed)\n",
    "\n",
    "k_dict,pi_dict={},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "491b15ab-d5dc-434e-8aa2-a4116df91e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********start generate effect vector...**********\n",
      "**********generate effect finished!**********\n",
      "**********start generate cell embedding...**********\n",
      "**********generate cell embedding finished**********\n"
     ]
    }
   ],
   "source": [
    "# 生成effect和embedding\n",
    "print(\"**********start generate effect vector...**********\")\n",
    "peak_effect,lib_size_effect=Get_Effect(n_peak,n_cell_total,\n",
    "                len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd)\n",
    "print(\"**********generate effect finished!**********\")\n",
    "\n",
    "print(\"**********start generate cell embedding...**********\")\n",
    "embeds_peak,meta=Get_Discrete_Embedding(pops_name[0],min_popsize,tree_text[0],\n",
    "             n_cell_total,pops_size,\n",
    "             embed_mean_same,embed_sd_same,\n",
    "              embed_mean_diff,embed_sd_diff,\n",
    "             n_embed_diff,n_embed_same,rand_seed)\n",
    "embeds_lib,meta=Get_Discrete_Embedding(pops_name[0],min_popsize,tree_text[0],\n",
    "             n_cell_total,pops_size,\n",
    "             embed_mean_same,embed_sd_same,\n",
    "              embed_mean_diff,embed_sd_diff,\n",
    "             n_embed_diff,n_embed_same,rand_seed+1)\n",
    "embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "print(\"**********generate cell embedding finished**********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "69ad47c1-642b-4685-a70f-01d3ecff7a7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_iter 1\n",
      "correct_iter 2\n",
      "**********start ZIP correction...**********\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lichen/anaconda3/envs/pytorch2/lib/python3.8/site-packages/scipy/optimize/minpack.py:162: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27491/3885720559.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  n_peak*pi+(1-pi)*np.sum(np.exp(-lambdas_i*k))-(n_peak-simu_param_nozero_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "600\n",
      "800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lichen/anaconda3/envs/pytorch2/lib/python3.8/site-packages/scipy/optimize/minpack.py:162: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last five Jacobian evaluations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/data1/lichen/anaconda3/envs/pytorch2/lib/python3.8/site-packages/scipy/optimize/minpack.py:162: RuntimeWarning: The number of calls to function has reached maxfev = 2000.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "**********ZIP correction finished!**********\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(rand_seed)\n",
    "if distribution=='Bernoulli' and np.max(np.exp(peak_mean)-1)>1:\n",
    "    raise ValueError('you data may not be Bernoulli distribution!')\n",
    "\n",
    "if real_param: #如果直接使用真实参数，peak mean直接按照真实参数来，lib size抽样\n",
    "    param_pm=np.sort(peak_mean,axis=0).ravel()\n",
    "    param_lib=np.sort(np.random.choice(lib_size,size=n_cell_total),axis=0).ravel()\n",
    "    param_nozero=np.sort(np.random.choice(nozero,size=n_cell_total),axis=0).ravel()\n",
    "else:\n",
    "    # kde\n",
    "    kde_pm = KernelDensity(kernel='gaussian', bandwidth=bw_pm).fit(peak_mean.reshape(-1,1))\n",
    "    kde_lib = KernelDensity(kernel='gaussian', bandwidth=bw_lib).fit(lib_size.reshape(-1,1))\n",
    "    kde_nozero = KernelDensity(kernel='gaussian', bandwidth=bw_nozero).fit(nozero.reshape(-1,1))\n",
    "\n",
    "    # 从kde中采样并进行排序（从小到大）\n",
    "    param_pm=kde_pm.sample(n_peak,random_state=rand_seed)\n",
    "    param_lib=kde_lib.sample(n_cell_total,random_state=rand_seed)\n",
    "    param_nozero=kde_nozero.sample(n_cell_total,random_state=rand_seed)\n",
    "\n",
    "    param_pm=np.sort(param_pm,axis=0).ravel()\n",
    "    param_lib=np.sort(param_lib,axis=0).ravel()\n",
    "    param_nozero=np.sort(param_nozero,axis=0).ravel()\n",
    "\n",
    "\n",
    "# 从模拟矩阵的参数顺序对应到采样的真实参数\n",
    "noise=np.random.normal(0.5,0.5,peak_effect.shape)\n",
    "use_noise=True\n",
    "if use_noise:\n",
    "    X_peak=np.dot(peak_effect+noise,embeds_peak)# peak*cell\n",
    "else:\n",
    "    X_peak=np.dot(peak_effect,embeds_peak)# peak*cell\n",
    "X_peak=Activation(X_peak,method=activation) # 防止出现负值\n",
    "rank=np.arange(len(X_peak))[np.mean(X_peak,axis=1).argsort().argsort()]\n",
    "param_pm=param_pm[rank]\n",
    "\n",
    "if two_embeds:\n",
    "    X_lib=np.dot(lib_size_effect,embeds_lib).ravel()\n",
    "else:\n",
    "    X_lib=np.dot(lib_size_effect,embeds_peak).ravel()\n",
    "rank = np.arange(len(X_lib))[X_lib.argsort().argsort()]\n",
    "param_lib=param_lib[rank]\n",
    "param_nozero=param_nozero[rank]\n",
    "\n",
    "# 对参数进行修正\n",
    "# X_peak维度是peak*cell\n",
    "simu_param_peak=X_peak.copy()\n",
    "if distribution=='Poisson':\n",
    "    for i in range(correct_iter):\n",
    "        print('correct_iter '+str(i+1))\n",
    "        simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=1).reshape(-1,1)+1e-8)*((np.exp(param_pm)-1).reshape(-1,1))*simu_param_peak.shape[1]\n",
    "        simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=0).reshape(1,-1)+1e-8)*((np.exp(param_lib)-1).reshape(1,-1))\n",
    "\n",
    "    simu_param_lib=np.exp(param_lib)-1\n",
    "    simu_param_nozero=np.exp(param_nozero)-1\n",
    "    #--------使用poisson分布生成ATAC\n",
    "    lambdas=simu_param_peak.copy()\n",
    "    # lambdas=simu_param_peak*(simu_param_lib.reshape(1,-1))\n",
    "\n",
    "    # 对sparsity进行修正\n",
    "    lambdas_sum=np.sum(lambdas,axis=0)\n",
    "    \n",
    "    lambdas_sum_copy=lambdas_sum.copy()\n",
    "\n",
    "    \n",
    "    print(\"**********start ZIP correction...**********\")\n",
    "    batch_size = 1000 # 并行数目，全局字典\n",
    "    k_dict,pi_dict={},{}\n",
    "    for i in range(0,n_cell_total,batch_size):\n",
    "        if i+batch_size<=n_cell_total:\n",
    "            my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, i+batch_size)]\n",
    "        else:\n",
    "            my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, n_cell_total)]\n",
    "        for thread_ in my_thread:\n",
    "            thread_.start()\n",
    "        for thread_ in my_thread:\n",
    "            thread_.join()\n",
    "    # 对每个cell的lambda置零并扩大相应倍数\n",
    "    for i in range(n_cell_total):\n",
    "        if k_dict[i]==3 or k_dict[i]==20 or pi_dict[i]<0 or pi_dict[i]>1:\n",
    "            continue\n",
    "        a=lambdas[:,i]*k_dict[i]\n",
    "        # b=atac_counts[:,i]\n",
    "        a[np.random.choice(n_peak,replace=False,size=int(pi_dict[i]*n_peak))]=0\n",
    "        lambdas[:,i]=a\n",
    "\n",
    "    print(\"**********ZIP correction finished!**********\")\n",
    "\n",
    "    atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "elif distribution=='Bernoulli':\n",
    "    for i in range(correct_iter):\n",
    "        print('correct_iter '+str(i+1))\n",
    "        simu_param_peak=Bernoulli_pm_correction(simu_param_peak,param_pm)\n",
    "        simu_param_peak=Bernoulli_lib_correction(simu_param_peak,param_lib)\n",
    "    atac_counts=np.random.binomial(1,p=simu_param_peak,size=simu_param_peak.shape)\n",
    "\n",
    "# return atac_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1de71253-b53a-4dbf-9e50-138ae809e5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2876"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(atac_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "daa681ac-70be-4fad-a672-173b97ed9a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrete_embed12_diff10_cell2000_EmbedSd0.5_EffectSd1_prob0.5_CorrectIter2_TwoEmbeddTrue_activation_exp_BatchEffect17\n"
     ]
    }
   ],
   "source": [
    "# -----------------保存数据\n",
    "\n",
    "# 创建对应的文件夹\n",
    "resultdir=\"/data1/lichen/code/second/scATAC_integration/code/simulation/data/{}/\".format(prefix_)\n",
    "\n",
    "prefix='_'.join([simu_type,'embed'+str(len_cell_embed),\n",
    "                'diff'+str(n_embed_diff),'cell'+str(n_cell_total),\n",
    "                'EmbedSd'+str(embed_sd_diff),'EffectSd'+str(effect_sd),'prob'+str(zero_prob),\n",
    "                'CorrectIter'+str(correct_iter),'TwoEmbedd'+str(two_embeds),\n",
    "                'activation_'+activation+'_BatchEffect17'])\n",
    "os.makedirs(os.path.join(resultdir,prefix),exist_ok=True)\n",
    "\n",
    "print(prefix)\n",
    "\n",
    "# save mtx\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix.mtx\"),sparse.csr_matrix(atac_counts.T))\n",
    "\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix_embed_pm.mtx\"),sparse.csr_matrix(embeds_peak.T))\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix_embed_lib.mtx\"),sparse.csr_matrix(embeds_lib.T))\n",
    "\n",
    "df=pd.DataFrame({'pop':meta})\n",
    "df.to_csv(os.path.join(resultdir,prefix,\"meta.tsv\"),sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73945e1-812d-4061-ba3b-2bbf750f9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchEffect3:三类，最基本的\n",
    "# BatchEffect4:在peak effect上加上N(0,0.5)的噪声\n",
    "# BatchEffect5:在peak effect上加上N(0.5,0.5)的噪声\n",
    "# 发现exp在batch上的效果更好\n",
    "\n",
    "# 6: exp_linear k=4 三类400、400、200\n",
    "# 7: exp_linear k=4 三类400、400、200 在peak effect上加上N(0.5,0.5)的噪声\n",
    "\n",
    "# 8: exp_linear k=6 三类400、400、200\n",
    "# 9: exp_linear k=6 三类400、400、200 在peak effect上加上N(0.5,0.5)的噪声\n",
    "\n",
    "# 10: exp_linear k=10 三类400、400、200\n",
    "# 11: exp_linear k=11 三类400、400、200 在peak effect上加上N(0.5,0.5)的噪声\n",
    "\n",
    "# 12: exp 三类400、400、200\n",
    "# 13: exp 三类400、400、200 在peak effect上加上N(0.5,0.5)的噪声\n",
    "\n",
    "\n",
    "# 14: exp 三类600、600、300\n",
    "# 15: exp 三类600、600、300 在peak effect上加上N(0.5,0.5)的噪声\n",
    "\n",
    "# 16: exp 三类800、800、400\n",
    "# 17: exp 三类800、800、400 在peak effect上加上N(0.5,0.5)的噪声"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b96964-910a-44db-890b-d2bedb9fd808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dadbfbba-f5dc-4264-a645-99d4e4eb6179",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae91e3-84e0-4f0b-b8a6-b8da9e9b1adc",
   "metadata": {},
   "source": [
    "## utils\n",
    "一些功能性函数<br>\n",
    "- cal_xxx计算数据的peak mean、library size、nozero等参数，输入均为anndata形式\n",
    "- Activation: 对矫正之前的参数矩阵使用的激活方式，建议在cell type仿真时使用sigmod，连续或离散<br>仿真时使用exp，可能在Bernoulli分布下需要使用sigmod（未尝试）\n",
    "- create_logger: 用来记录信息\n",
    "- Bernoulli_correction：仿真binary数据时使用的对参数矩阵进行矫正的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06aac551-2568-46da-9d8d-2493cd98ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library size\n",
    "def cal_lib(adata):\n",
    "    return np.array(np.sum(adata.X,axis=1)).ravel()\n",
    "\n",
    "def cal_pm(adata): # peak mean\n",
    "    return np.array(np.mean(adata.X,axis=0)).ravel()\n",
    "\n",
    "def cal_pl(adata):# peak length\n",
    "    start=np.array([int(i.split('_')[1]) for i in adata.var.index])\n",
    "    end=np.array([int(i.split('_')[2]) for i in adata.var.index])\n",
    "    return (end-start).ravel()\n",
    "\n",
    "def cal_spa(adata):# sparsity\n",
    "    X=adata.X.copy()\n",
    "    X[X>0]=1\n",
    "    sparsity=np.sum(X,axis=1)/X.shape[1]\n",
    "    return np.array(sparsity).ravel()\n",
    "\n",
    "def cal_nozero(adata):# sparsity\n",
    "    X=adata.X.copy()\n",
    "    X[X>0]=1\n",
    "    sparsity=np.sum(X,axis=1)\n",
    "    return np.array(sparsity).ravel()\n",
    "\n",
    "def cal_peak_count(adata):\n",
    "    return np.array(np.sum(adata.X,axis=0)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c90aeb15-1c59-4dfe-ba82-b5036973bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Activation(X,method='sigmod'):# 对peak_effect*cell_embedding 的矩阵进行激活操作，防止其值为0\n",
    "#     if method=='sigmod':\n",
    "#         return 1/(1+np.exp(-1*X))\n",
    "#     elif method=='exp':\n",
    "#         return np.exp(X)\n",
    "#     elif method=='exp_linear':\n",
    "#         exp_num=4\n",
    "#         k=exp_num*np.exp(exp_num-1)\n",
    "#         # k=1\n",
    "#         X_act=X\n",
    "#         X_act[X_act>=exp_num]=k*X_act[X_act>=exp_num]+np.exp(exp_num)-exp_num\n",
    "#         X_act[X_act<exp_num]=np.exp(X_act[X_act<exp_num])\n",
    "#         return X_act\n",
    "#     elif method=='exp_sym':\n",
    "#         X_act=X\n",
    "#         exp_level=1.5\n",
    "#         X_act[X_act<=0]=np.power(exp_level,X_act[X_act<=0])\n",
    "#         X_act[X_act>0]=2-np.power(exp_level,-X_act[X_act>0])\n",
    "        \n",
    "#         return X_act\n",
    "#     else:\n",
    "#         raise ValueError('wrong activation method!')\n",
    "\n",
    "def Activation(X,method='sigmod'):# 对peak_effect*cell_embedding 的矩阵进行激活操作，防止其值为0\n",
    "    if method=='sigmod':\n",
    "        return 1/(1+np.exp(-1*X))\n",
    "    elif method=='exp':\n",
    "        return np.exp(X)\n",
    "    elif method=='exp_linear':\n",
    "        exp_num=4\n",
    "        k=np.exp(exp_num)\n",
    "        # k=1\n",
    "        X_act=X.copy()\n",
    "        X_act[X_act>=exp_num]=k*X_act[X_act>=exp_num]+np.exp(exp_num)-exp_num*np.exp(exp_num)\n",
    "        X_act[X_act<exp_num]=np.exp(X_act[X_act<exp_num])\n",
    "        return X_act\n",
    "    elif method=='exp_sym':\n",
    "        X_act=X\n",
    "        exp_level=1.5\n",
    "        X_act[X_act<=0]=np.power(exp_level,X_act[X_act<=0])\n",
    "        X_act[X_act>0]=2-np.power(exp_level,-X_act[X_act>0])\n",
    "        return X_act\n",
    "    elif method=='sigmod_adj':\n",
    "        k=2\n",
    "        A=1\n",
    "        return A/(1+k**(-1*X))\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('wrong activation method!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24127482-fb7c-4f3e-a8b2-e473de38d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(name='', ch=True, fh='', levelname=logging.INFO):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(levelname)\n",
    "    \n",
    "#     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    if ch:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "    if fh:\n",
    "        fh = logging.FileHandler(fh, mode='w')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af26cb4b-e37a-41c6-8211-c53eddc03b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    \"\"\"\n",
    "    Seed all necessary random number generators.\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = random.randint(1, 10000)\n",
    "    # torch.set_num_threads(1)  # Suggested for issues with deadlocks, etc.\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5231d06-bd75-464b-81aa-7f75d0158e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新之后快版本的bernoulli\n",
    "\n",
    "def Bernoulli_pm_correction(X_peak,param_pm): # X_peak:等待矫正的矩阵  param_pm:对应的采样得到的peak_mean\n",
    "    # peak mean correction\n",
    "    peak_p_list=[]\n",
    "    for i in range(0,X_peak.shape[0]):\n",
    "        peak_p=X_peak[i,:] # 单个peak对应的所有cell的值\n",
    "        peak_mean=np.mean(peak_p) # 当前矩阵的peak mean\n",
    "        peak_mean_ex=np.exp(param_pm[i])-1 # 期望的peak mean\n",
    "        \n",
    "        # 若期望的peak_mean都是0\n",
    "        if peak_mean_ex==0 or peak_mean==0:\n",
    "            peak_p_list.append(peak_p*peak_mean_ex)\n",
    "            continue\n",
    "        \n",
    "        if np.max(peak_p)/peak_mean*peak_mean_ex>1:\n",
    "            peak_p_sort=np.sort(peak_p)\n",
    "            idx=len(peak_p_sort)-1\n",
    "            while(1):\n",
    "                weight=(len(peak_p)*peak_mean_ex+idx-len(peak_p))/(np.sum(peak_p_sort[0:idx])+1e-8)\n",
    "                if peak_p_sort[idx-1]*weight<=1:\n",
    "                    # print(idx)\n",
    "                    break\n",
    "\n",
    "                for idx_2 in range(idx,-1,-1):  # 找到*weight<1 的idx\n",
    "                    if peak_p_sort[idx_2-1]*weight<=1:\n",
    "                        # print(idx_2)\n",
    "                        break\n",
    "                    # 如果实在没有idx能够使得值*weight<1,此时就会一直循环，需要及时跳出循环\n",
    "                    if idx_2<=1:\n",
    "                        break\n",
    "                idx=idx_2\n",
    "                if idx_2<=1:\n",
    "                    weight=(len(peak_p)*peak_mean_ex+idx-len(peak_p))/(np.sum(peak_p_sort[0:idx])+1e-8)\n",
    "                    break\n",
    "            peak_p=peak_p*weight\n",
    "            peak_p[peak_p>1]=1\n",
    "        else:\n",
    "            peak_p=peak_p/peak_mean*peak_mean_ex\n",
    "        peak_p_list.append(peak_p)\n",
    "    peak_p_matrix=np.vstack(peak_p_list)\n",
    "    return peak_p_matrix\n",
    "\n",
    "def Bernoulli_lib_correction(X_peak,param_lib):\n",
    "    peak_p_list=[]\n",
    "    for i in range(X_peak.shape[1]):\n",
    "        peak_p=X_peak[:,i]\n",
    "        lib_size=np.sum(peak_p)\n",
    "        lib_size_ex=np.exp(param_lib[i])-1\n",
    "        \n",
    "        # 若期望的library_size都是0\n",
    "        if lib_size_ex==0 or lib_size==0:\n",
    "            peak_p_list.append((peak_p*lib_size_ex).reshape(-1,1))\n",
    "            continue\n",
    "\n",
    "        if np.max(peak_p)/lib_size*lib_size_ex>1:\n",
    "            peak_p_sort=np.sort(peak_p)\n",
    "            idx=len(peak_p_sort)-1\n",
    "            while(1):\n",
    "                weight=(lib_size_ex+idx-len(peak_p))/(np.sum(peak_p_sort[0:idx])+1e-8)\n",
    "                if peak_p_sort[idx-1]*weight<=1:\n",
    "                    break\n",
    "                for idx_2 in range(idx,-1,-1):\n",
    "                    if peak_p_sort[idx_2-1]*weight<=1:\n",
    "                        # print(idx_2)\n",
    "                        break\n",
    "                    # 如果实在没有idx能够使得值*weight<1,此时就会一直循环，需要及时跳出循环\n",
    "                    if idx_2<=1:\n",
    "                        break\n",
    "                idx=idx_2\n",
    "                if idx_2<=1:\n",
    "                    weight=(len(peak_p)*peak_mean_ex+idx-len(peak_p))/(np.sum(peak_p_sort[0:idx])+1e-8)\n",
    "                    break\n",
    "            # 防止出现<1的部分全都是0\n",
    "            if np.sum(peak_p_sort[0:idx])==0:\n",
    "                peak_p[peak_p>1]=1\n",
    "            else:\n",
    "                peak_p=peak_p*weight\n",
    "                peak_p[peak_p>1]=1\n",
    "        else:\n",
    "            peak_p=peak_p/lib_size*lib_size_ex\n",
    "        peak_p_list.append(peak_p.reshape(-1,1))\n",
    "    peak_p_matrix=np.hstack(peak_p_list)\n",
    "    return peak_p_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20f939-2514-4596-8112-75aeb4ded0db",
   "metadata": {},
   "source": [
    "## Get Effect\n",
    "用来获取peak的effect vector: peak num * effect dim <br>\n",
    "和library size的effect vector：cell num * effect dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56964026-34e3-483c-ae92-c6b28f11df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Effect(n_peak,n_cell_total,len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd):\n",
    "    # 生成peak effect和library size effect\n",
    "    # np.random.seed(rand_seed)\n",
    "    peak_effect=np.random.normal(effect_mean,effect_sd,(n_peak,len_cell_embed))\n",
    "    lib_size_effect=np.random.normal(effect_mean,effect_sd,(1,len_cell_embed))\n",
    "    \n",
    "    # 対生成的effect vevtor进行置零\n",
    "    if zero_set=='by_row':\n",
    "        # 对于每个peak的effect进行相同概率的置零\n",
    "        def set_zero(a,zero_prob=0.5):\n",
    "            a[np.random.choice(len(a),replace=False,size=int(len(a)*zero_prob))]=0\n",
    "            return a\n",
    "        peak_effect=np.apply_along_axis(set_zero,1,peak_effect,zero_prob=zero_prob)\n",
    "\n",
    "    if zero_set=='all':\n",
    "        # 对于所有index选择进行置零\n",
    "        indices = np.random.choice(peak_effect.shape[1]*peak_effect.shape[0], replace=False, size=int(peak_effect.shape[1]*peak_effect.shape[0]*zero_prob))\n",
    "        peak_effect[np.unravel_index(indices, peak_effect.shape)] = 0 \n",
    "        \n",
    "    return peak_effect,lib_size_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a378bc-7453-4529-b561-a3a81f99e246",
   "metadata": {},
   "source": [
    "## Get Embedding\n",
    "用来获取cell的embedding的函数，包括discrete embedding、continuous embedding、single embedding，<br>\n",
    "其中single embedding用来在cell type仿真的时候单独仿真每一个cell type的细胞然后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99fe8b7-1166-4398-bba8-db85e187944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same):\n",
    "    embed=np.random.normal(embed_mean_same,embed_sd_same,(n_embed_same+n_embed_diff,n_cell_total))\n",
    "    index=['embedding_'+str(m+1) for m in range(n_embed_same+n_embed_diff)]\n",
    "    columns=['single cluster' for m in  range(n_cell_total)]\n",
    "    df=pd.DataFrame(embed,columns=columns,index=index)\n",
    "    \n",
    "    return df,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b194a272-8b99-4395-92aa-9f395f663b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Discrete_Embedding(pops_name,min_popsize,tree_text,\n",
    "                 n_cell_total,pops_size,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed):\n",
    "    # np.random.seed(rand_seed)\n",
    "    n_pop=len(pops_name)\n",
    "    if(n_cell_total<min_popsize*n_pop):\n",
    "        raise ValueError(\"The size of the smallest population is too big for the total number of cells\")\n",
    "\n",
    "    if not pops_size:\n",
    "        if min_popsize:# 若设定了最小pop的size，则其他pop将原来的细胞数目平均分配\n",
    "            pop_size=np.floor((n_cell_total-min_popsize)/(len(pops_name)-1))\n",
    "            left_over=n_cell_total-min_popsize-pop_size*(len(pops_name)-1)\n",
    "            pop_name_size={} #每个pop对应的size\n",
    "            for name in pops_name:\n",
    "                if name==min_pop:\n",
    "                    pop_name_size[name]=min_popsize\n",
    "                else:\n",
    "                    pop_name_size[name]=pop_size\n",
    "            pop_name_size[pops_name[pops_name.index(min_pop)-1]]+=left_over\n",
    "        else:# 未设置最小pop，直接将每个pop的cell数目均分\n",
    "            pop_size=np.floor((n_cell_total)/(len(pops_name)))\n",
    "            left_over=n_cell_total-pop_size*(len(pops_name))\n",
    "            pop_name_size={}\n",
    "            for name in pops_name:\n",
    "                pop_name_size[name]=pop_size\n",
    "            pop_name_size[pops_name[0]]+=left_over\n",
    "\n",
    "    else:# 若直接对每个pop赋予size\n",
    "        pop_name_size={}\n",
    "        for (i,name) in enumerate(pops_name):\n",
    "            pop_name_size[name]=pops_size[i]\n",
    "    # 将float转化为int \n",
    "    for key,value in pop_name_size.items():\n",
    "        pop_name_size[key]=int(value)\n",
    "\n",
    "    #--------生成不同pop之间的协方差矩阵，这里需要在你的python环境中使用R包ape\n",
    "    ape = importr('ape')\n",
    "    phyla=ape.read_tree(text=tree_text)\n",
    "    corr_matrix=np.array(ape.vcv_phylo(phyla,cor=True))\n",
    "    \n",
    "    corr_matrix=np.eye(len(pops_name))\n",
    "\n",
    "    #--------生成embed\n",
    "        \n",
    "    embed_same,embed_diff=[],[]\n",
    "    #生成差异embedding特征对应的均值，保证不同的pop之间的相关性\n",
    "    embed_diff_mean_mv = multivariate_normal.rvs(mean=[embed_mean_diff]*n_pop, cov=corr_matrix, size=n_embed_diff)\n",
    "    for (j,name) in enumerate(pops_name):\n",
    "        #生成每个pop对应的非差异embed部分\n",
    "        embed_same_pop=np.random.normal(embed_mean_same,embed_sd_same,(n_embed_same,pop_name_size[name]))\n",
    "\n",
    "        #生成每个pop对应的差异embed部分\n",
    "        embed_diff_pop=[]\n",
    "        for k in range(n_embed_diff):\n",
    "            embed=np.random.normal(embed_diff_mean_mv[k,j],embed_sd_diff,(pop_name_size[name],))\n",
    "            embed_diff_pop.append(embed)\n",
    "        embed_diff_pop=np.vstack(embed_diff_pop)\n",
    "\n",
    "        # 对每个pop差异/非差异embed进行汇总\n",
    "        embed_same.append(embed_same_pop) # n_embed_same*pop_size\n",
    "        embed_diff.append(embed_diff_pop) # n_embed_diff*pop_size\n",
    "\n",
    "    # embed_param: len_cell_embed*n_cell_total\n",
    "    embed_same=np.hstack(embed_same)\n",
    "    embed_diff=np.hstack(embed_diff)\n",
    "    embed_param=np.vstack([embed_same,embed_diff])\n",
    "\n",
    "    columns=np.hstack([[name]*pop_name_size[name] for name in pops_name])\n",
    "    index=['same_embedding_'+str(m+1) for m in range(n_embed_same)]+['diff_embedding_'+str(m+1) for m in range(n_embed_diff)]\n",
    "    df=pd.DataFrame(embed_param,columns=columns,index=index)\n",
    "\n",
    "    \n",
    "    return df,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a0f752-d363-4611-bfc3-339bec484a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Tree_Sd(branches,root,depth=0,anchor=0,rand_seed=0):# depth就是到根节点的深度;一个递归函数,用来获取细胞在每个branch上的位置以及enbedding\n",
    "    # np.random.seed(rand_seed)\n",
    "    \n",
    "    start_nodes=[i.split('-')[0] for i in branches]\n",
    "    \n",
    "    df=pd.DataFrame({'branches':[],'cell_places':[],'embeddings':[]})\n",
    "    for i in range(len(start_nodes)): \n",
    "        if root==start_nodes[i]:# 该节点对应的所有branch\n",
    "            branch=branches[i]\n",
    "            start,end,branch_len,n_cells=branch.split('-')[0],\\\n",
    "                    branch.split('-')[1],float(branch.split('-')[2]),int(branch.split('-')[3])\n",
    "            interval=branch_len/(n_cells-1)#获取interval\n",
    "            cell_places=[depth+interval*i for i in range(n_cells-1)]+[depth+branch_len]#以interval为间隔获取cell在branch上的位置\n",
    "            \n",
    "            # 获取单维所有细胞的embedding\n",
    "            embeddings=np.array([0]+list(np.cumsum(np.random.normal(0,np.sqrt(interval),(n_cells-1)))))+anchor\n",
    "            \n",
    "            df_=pd.DataFrame({'branches':[branch]*len(cell_places),'cell_places':cell_places,'embeddings':embeddings})\n",
    "            df=pd.concat([df,df_,Generate_Tree_Sd(branches,end,depth+branch_len,anchor=embeddings[-1])],axis=0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def Get_Continuous_Embedding(tree_text,n_cell_total,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed):\n",
    "    # np.random.seed(rand_seed)\n",
    "    # 构建tree\n",
    "    tree = Phylo.read(StringIO(tree_text), \"newick\")\n",
    "    \n",
    "    # 获取不同的branch，形式为‘X-X-length’\n",
    "    clades = [i for i in tree.find_clades()]\n",
    "    branch_clades=[i for i in clades if i.branch_length]\n",
    "    branches=[tree.get_path(i)[-2:] for i in branch_clades]\n",
    "    branches=[branches[i][0].name+'-'+branches[i][1].name+'-'+str(branch_clades[i].branch_length) for i in range(len(branches))]\n",
    "    \n",
    "    # 获取所有branch的长度\n",
    "    total_branch_len=sum([float(i.split('-')[2]) for i in branches])\n",
    "    \n",
    "    \n",
    "    # 获取每个branch上的细胞数目（按照branch长度进行均分）\n",
    "    n_branches_cell=[]\n",
    "    for i in range(len(branches)):\n",
    "        branch_len=float(branches[i].split('-')[2])\n",
    "        n_cells=np.floor(n_cell_total*(branch_len/total_branch_len))\n",
    "        n_branches_cell.append(n_cells)\n",
    "\n",
    "    # 将偏置加到数目最多的分支上\n",
    "    n_branches_cell[n_branches_cell.index(max(n_branches_cell))]=n_branches_cell[n_branches_cell.index(max(n_branches_cell))]+n_cell_total-sum(n_branches_cell)\n",
    "    n_branches_cell=[int(i) for i in n_branches_cell]\n",
    "    \n",
    "    # 将细胞数目加入branch，最终branch格式：A-B-1.0-200\n",
    "    branches=[branches[i]+'-'+str(n_branches_cell[i]) for i in range(len(branches))]\n",
    "    \n",
    "    # 获取root名字\n",
    "    root=clades[1].name\n",
    "    \n",
    "    # 生成continuous的embedding\n",
    "    embed_same=np.random.normal(embed_mean_same,embed_sd_same,(n_embed_same,n_cell_total))\n",
    "    embed_diff=[]\n",
    "    for i in range(n_embed_diff):\n",
    "        df_continuous=Generate_Tree_Sd(branches,root,depth=0,anchor=embed_mean_diff,rand_seed=rand_seed+i)\n",
    "        embed_diff.append(np.array(df_continuous['embeddings']))\n",
    "    embed_diff=np.vstack(embed_diff)\n",
    "    # print(embed_same.shape,embed_diff.shape)\n",
    "    # print(branches)\n",
    "    embed=np.vstack([embed_same,embed_diff])\n",
    "\n",
    "    \n",
    "    # 加上columns和index\n",
    "    columns=list(df_continuous['branches'])\n",
    "    index=['same_embedding_'+str(m+1) for m in range(n_embed_same)]+['diff_embedding_'+str(m+1) for m in range(n_embed_diff)]\n",
    "    df=pd.DataFrame(embed,columns=columns,index=index)\n",
    "    \n",
    "    return df,columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465886c-0982-4743-aa69-1fa5f676a4a6",
   "metadata": {},
   "source": [
    "## Get count\n",
    "根据给定的effect矩阵和cell embedding矩阵相乘，然后使用从真实参数的分布中采样得到的参数进行修正，<br>\n",
    "修正方式根据数据为count或binary分为poisson分布和bernoulli分布<br>\n",
    "- Get_Tree_Counts: 生成离散或连续数据的时候使用的函数\n",
    "- Get_Celltype_Counts：生成真实数据对应的cell type数据时使用的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bea3572-cff9-4b28-a2fa-04241dbfb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_correction(i,simu_param_lib_i,lambdas_i,lambdas_sum_i,simu_param_nozero_i,n_peak):\n",
    "    global k_dict,pi_dict\n",
    "    # print(i)\n",
    "    if i%200==0:print(i)\n",
    "    def solve_function(unsolved_value):\n",
    "        k,pi=unsolved_value[0],unsolved_value[1]\n",
    "        return [\n",
    "            k*(1-pi)-simu_param_lib_i/(lambdas_sum_i),\n",
    "            n_peak*pi+(1-pi)*np.sum(np.exp(-lambdas_i*k))-(n_peak-simu_param_nozero_i)\n",
    "        ]\n",
    "\n",
    "    solved=fsolve(solve_function,[3,0.5],maxfev=2000)\n",
    "    k,pi=solved[0],solved[1]\n",
    "    simu1=k*(1-pi)*(lambdas_sum_i)\n",
    "    real1=simu_param_lib_i\n",
    "    if abs(simu1-real1)/real1>0.1:\n",
    "        solved=fsolve(solve_function,[20,0.5],maxfev=2000)\n",
    "    k,pi=solved[0],solved[1]\n",
    "\n",
    "    k_dict[i]=solved[0]\n",
    "    pi_dict[i]=solved[1]\n",
    "\n",
    "class zip_correction_thread(threading.Thread):\n",
    "    def __init__(self,i,simu_param_lib_i,lambdas_i,lambdas_sum_i,simu_param_nozero_i,n_peak):\n",
    "        super(zip_correction_thread, self).__init__()\n",
    "        self.i  = i\n",
    "        self.simu_param_lib_i  = simu_param_lib_i\n",
    "        self.lambdas_i  = lambdas_i\n",
    "        self.lambdas_sum_i  = lambdas_sum_i\n",
    "        self.simu_param_nozero_i  = simu_param_nozero_i\n",
    "        self.n_peak  = n_peak\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        zip_correction(self.i,self.simu_param_lib_i,self.lambdas_i,self.lambdas_sum_i,self.simu_param_nozero_i,self.n_peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd23c296-8688-4720-82fe-730609064347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution='Bernoulli',\n",
    "                    activation='exp',bw_pm=1e-4,bw_lib=0.05,bw_nozero=0.05,real_param=True):\n",
    "    # np.random.seed(rand_seed)\n",
    "    if distribution=='Bernoulli' and np.max(np.exp(peak_mean)-1)>1:\n",
    "        raise ValueError('you data may not be Bernoulli distribution!')\n",
    "    \n",
    "    if real_param: #如果直接使用真实参数，peak mean直接按照真实参数来，lib size抽样\n",
    "        param_pm=np.sort(peak_mean,axis=0).ravel()\n",
    "        param_lib=np.sort(np.random.choice(lib_size,size=n_cell_total),axis=0).ravel()\n",
    "        param_nozero=np.sort(np.random.choice(nozero,size=n_cell_total),axis=0).ravel()\n",
    "    else:\n",
    "        # kde\n",
    "        kde_pm = KernelDensity(kernel='gaussian', bandwidth=bw_pm).fit(peak_mean.reshape(-1,1))\n",
    "        kde_lib = KernelDensity(kernel='gaussian', bandwidth=bw_lib).fit(lib_size.reshape(-1,1))\n",
    "        kde_nozero = KernelDensity(kernel='gaussian', bandwidth=bw_nozero).fit(nozero.reshape(-1,1))\n",
    "\n",
    "        # 从kde中采样并进行排序（从小到大）\n",
    "        param_pm=kde_pm.sample(n_peak,random_state=rand_seed)\n",
    "        param_lib=kde_lib.sample(n_cell_total,random_state=rand_seed)\n",
    "        param_nozero=kde_nozero.sample(n_cell_total,random_state=rand_seed)\n",
    "\n",
    "        param_pm=np.sort(param_pm,axis=0).ravel()\n",
    "        param_lib=np.sort(param_lib,axis=0).ravel()\n",
    "        param_nozero=np.sort(param_nozero,axis=0).ravel()\n",
    "\n",
    "\n",
    "\n",
    "        # estimation_dis='one_logser' # 'NB'/'one_logser'/'gamma'/'zero_logser'\n",
    "        \n",
    "#         print('the estimation method is ',estimation_dis)\n",
    "        \n",
    "#         if estimation_dis=='gamma':\n",
    "#             peak_mean_real = np.exp(peak_mean)-1\n",
    "#             peak_mean_sqrt = np.sqrt(peak_mean_real)\n",
    "\n",
    "#             fit_alpha, fit_loc, fit_beta = stats.gamma.fit(peak_mean_sqrt,floc=np.min(peak_mean_sqrt)-0.001)\n",
    "#             peak_mean_sqrt_sample = stats.gamma.rvs(a=fit_alpha, loc=fit_loc, scale=fit_beta, size=n_peak, random_state=rand_seed)\n",
    "#             param_pm = np.sort(peak_mean_sqrt_sample)\n",
    "#             param_pm = np.log(param_pm**2+1)\n",
    "#         elif estimation_dis=='zero_logser':\n",
    "#             peak_count_simu=zero_logser(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "#         elif estimation_dis=='one_logser':\n",
    "#             peak_count_simu=one_logser(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "#         elif estimation_dis=='zero_NB':\n",
    "#             peak_count_simu=zero_NB(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "#         elif estimation_dis=='NB':\n",
    "#             peak_count_simu=NB(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "#         elif estimation_dis=='ZIP':\n",
    "#             peak_count_simu=ZIP(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "            \n",
    "#         elif estimation_dis=='ZINB':\n",
    "#             peak_count_simu=ZINB(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "            \n",
    "#         else:\n",
    "#             raise ValueError('wrong estimation distribution!')\n",
    "        \n",
    "#         #n,random_state = 2,2022\n",
    "#         gmm_lz = GMM(2, random_state=rand_seed)\n",
    "#         gmm_lz.fit(lib_size_log.reshape(-1,1))\n",
    "#         # [sample[0] for sample in gmm.sample(1000)]\n",
    "#         lib_size_log_sample = gmm_lz.sample(n_cell_total)[0].reshape(-1)\n",
    "#         param_lib = np.sort(lib_size_log_sample)\n",
    "        \n",
    "#         non_zero_real = np.exp(nozero)-1\n",
    "#         non_zero_log = np.log(non_zero_real)\n",
    "#         gmm_nz = GMM(2, random_state=rand_seed)\n",
    "#         gmm_nz.fit(non_zero_log.reshape(-1,1))\n",
    "#         # [sample[0] for sample in gmm.sample(1000)]\n",
    "#         non_zero_log_sample = gmm_nz.sample(n_cell_total)[0].reshape(-1)\n",
    "#         param_nozero = np.log(np.exp(np.sort(non_zero_log_sample))+1)\n",
    "\n",
    "    # 从模拟矩阵的参数顺序对应到采样的真实参数\n",
    "    X_peak=np.dot(peak_effect,embeds_peak)# peak*cell\n",
    "    X_peak=Activation(X_peak,method=activation) # 防止出现负值\n",
    "    rank=np.arange(len(X_peak))[np.mean(X_peak,axis=1).argsort().argsort()]\n",
    "    param_pm=param_pm[rank]\n",
    "\n",
    "    if two_embeds:\n",
    "        X_lib=np.dot(lib_size_effect,embeds_lib).ravel()\n",
    "    else:\n",
    "        X_lib=np.dot(lib_size_effect,embeds_peak).ravel()\n",
    "    rank = np.arange(len(X_lib))[X_lib.argsort().argsort()]\n",
    "    param_lib=param_lib[rank]\n",
    "    param_nozero=param_nozero[rank]\n",
    "\n",
    "    # 对参数进行修正\n",
    "    # X_peak维度是peak*cell\n",
    "    simu_param_peak=X_peak\n",
    "    if distribution=='Poisson':\n",
    "        for i in range(correct_iter):\n",
    "            print('correct_iter '+str(i+1))\n",
    "            simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=1).reshape(-1,1)+1e-8)*((np.exp(param_pm)-1).reshape(-1,1))*simu_param_peak.shape[1]\n",
    "            simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=0).reshape(1,-1)+1e-8)*((np.exp(param_lib)-1).reshape(1,-1))\n",
    "            \n",
    "        simu_param_lib=np.exp(param_lib)-1\n",
    "        simu_param_nozero=np.exp(param_nozero)-1\n",
    "        #--------使用poisson分布生成ATAC\n",
    "        lambdas=simu_param_peak\n",
    "        # lambdas=simu_param_peak*(simu_param_lib.reshape(1,-1))\n",
    "        \n",
    "        # 对sparsity进行修正\n",
    "        lambdas_sum=np.sum(lambdas,axis=0)\n",
    "        \n",
    "        print(\"**********start ZIP correction...**********\")\n",
    "        k_list,pi_list=[],[]\n",
    "        # 求解每个cell中lambda扩大的倍数和置零的比例\n",
    "        for i in range(n_cell_total):\n",
    "            iter_=i\n",
    "            # print(i)\n",
    "            def solve_function(unsolved_value):\n",
    "                k,pi=unsolved_value[0],unsolved_value[1]\n",
    "                return [\n",
    "                    k*(1-pi)-simu_param_lib[iter_]/(lambdas_sum[iter_]),\n",
    "                    n_peak*pi+(1-pi)*np.sum(np.exp(-lambdas[:,iter_]*k))-(n_peak-simu_param_nozero[iter_])\n",
    "                ]\n",
    "\n",
    "            solved=fsolve(solve_function,[3,0.5],maxfev=2000)\n",
    "            k,pi=solved[0],solved[1]\n",
    "            simu1=k*(1-pi)*(lambdas_sum[iter_])\n",
    "            real1=simu_param_lib[iter_]\n",
    "            if abs(simu1-real1)/real1>0.1:\n",
    "                print('=================================')\n",
    "                print(i)\n",
    "                print(simu1,real1)\n",
    "                # print('=================================')\n",
    "                solved=fsolve(solve_function,[20,0.5],maxfev=2000)\n",
    "            simu1=solved[0]*(1-solved[1])*(lambdas_sum[iter_])\n",
    "            real1=simu_param_lib[iter_]\n",
    "            if abs(simu1-real1)/real1>0.1:\n",
    "                print(i)\n",
    "                print(simu1,real1)\n",
    "                print(\"=================================\")\n",
    "                \n",
    "            k_list.append(solved[0])\n",
    "            pi_list.append(solved[1])\n",
    "        # 对每个cell的lambda置零并扩大相应倍数\n",
    "        for i in range(n_cell_total):\n",
    "            if k_list[i]==3 or k_list[i]==20 or pi_list[i]<0:\n",
    "                continue\n",
    "            a=lambdas[:,i]*k_list[i]\n",
    "            # print(i)\n",
    "            # print(k_list[i],pi_list[i])\n",
    "            # print(\"=============================\")\n",
    "            # b=atac_counts[:,i]\n",
    "            a[np.random.choice(n_peak,replace=False,size=int(pi_list[i]*n_peak))]=0\n",
    "            lambdas[:,i]=a\n",
    "        print(\"**********ZIP correction finished!**********\")\n",
    "            \n",
    "#         print(\"**********start ZIP correction...**********\")\n",
    "#         batch_size = 1000 # 并行数目，全局字典\n",
    "#         global k_dict,pi_dict\n",
    "#         for i in range(0,n_cell_total,batch_size):\n",
    "#             if i+batch_size<=n_cell_total:\n",
    "#                 my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, i+batch_size)]\n",
    "#             else:\n",
    "#                 my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, n_cell_total)]\n",
    "#             for thread_ in my_thread:\n",
    "#                 thread_.start()\n",
    "#             for thread_ in my_thread:\n",
    "#                 thread_.join()\n",
    "#         # 对每个cell的lambda置零并扩大相应倍数\n",
    "#         for i in range(n_cell_total):\n",
    "#             if k_dict[i]==3 or k_dict[i]==20 or pi_dict[i]<0:\n",
    "#                 continue\n",
    "#             a=lambdas[:,i]*k_dict[i]\n",
    "#             # b=atac_counts[:,i]\n",
    "#             a[np.random.choice(n_peak,replace=False,size=int(pi_dict[i]*n_peak))]=0\n",
    "#             lambdas[:,i]=a\n",
    "            \n",
    "#         print(\"**********ZIP correction finished!**********\")\n",
    "            \n",
    "        atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "    elif distribution=='Bernoulli':\n",
    "        for i in range(correct_iter):\n",
    "            print('correct_iter '+str(i+1))\n",
    "            simu_param_peak=Bernoulli_pm_correction(simu_param_peak,param_pm)\n",
    "            simu_param_peak=Bernoulli_lib_correction(simu_param_peak,param_lib)\n",
    "        atac_counts=np.random.binomial(1,p=simu_param_peak,size=simu_param_peak.shape)\n",
    "    \n",
    "    return atac_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "527fd9e9-7657-4320-81a4-9ae0b95051d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def kl_div(peak_count,peak_count_simu):\n",
    "    # -------- K-L散度\n",
    "    peak_count_combine=np.concatenate((peak_count,peak_count_simu))\n",
    "    value=np.sort(np.unique(peak_count_combine))\n",
    "    value_count_ori,value_count_simu=[],[]\n",
    "    for value_ in value:\n",
    "        value_count_ori.append(len(np.where(peak_count==value_)[0]))\n",
    "        value_count_simu.append(len(np.where(peak_count_simu==value_)[0]))\n",
    "\n",
    "    value_count_ori=np.array(value_count_ori)\n",
    "    value_count_ori=value_count_ori/sum(value_count_ori)\n",
    "    value_count_simu=np.array(value_count_simu)\n",
    "    value_count_simu=value_count_simu/sum(value_count_simu)\n",
    "    \n",
    "    epsilon = 0.00001\n",
    "    value_count_ori+=epsilon\n",
    "    value_count_simu+=epsilon\n",
    "    \n",
    "    # print('KL divergence:',sum(rel_entr(value_count_ori, value_count_simu)))\n",
    "    return sum(rel_entr(value_count_ori, value_count_simu))\n",
    "    \n",
    "def zero_logser(peak_count):\n",
    "    peak_count_new=np.delete(peak_count,np.where(peak_count == 0))\n",
    "    zero_prob_=len(np.where(peak_count == 0)[0])/len(peak_count)\n",
    "    def solve_function(unsolved_value):\n",
    "        p=unsolved_value[0]\n",
    "        return [\n",
    "            -1*p/(np.log(1-p)*(1-p))-np.mean(peak_count_new)\n",
    "        ]\n",
    "\n",
    "    solved=fsolve(solve_function,[0.995],maxfev=2000)\n",
    "    p=solved[0]\n",
    "    # print(-1*p/(np.log(1-p)*(1-p)),np.mean(peak_count_new))\n",
    "    peak_count_simu=logser.rvs(p,size=len(peak_count))*\\\n",
    "        stats.bernoulli.rvs(p = 1-zero_prob_, size = len(peak_count)) \n",
    "    \n",
    "    return peak_count_simu\n",
    "\n",
    "def one_logser(peak_count):\n",
    "    zero_prob_=len(np.where(peak_count == 0)[0])/len(peak_count)\n",
    "    one_prob=len(np.where(peak_count == 1)[0])/len(peak_count)\n",
    "    peak_count_new=np.delete(peak_count,np.where(peak_count == 0))\n",
    "    peak_count_new=np.delete(peak_count_new,np.where(peak_count_new == 1))-1\n",
    "    # 固定0、1的概率\n",
    "    idx_all=range(len(peak_count))\n",
    "    idx_zero=np.random.choice(idx_all,replace=False,size=int(len(peak_count)*(zero_prob_)))\n",
    "    idx_one=np.random.choice(np.delete(idx_all,idx_zero),replace=False,size=int(len(peak_count)*(one_prob)))\n",
    "\n",
    "    def solve_function(unsolved_value):\n",
    "        p=unsolved_value[0]\n",
    "        return [\n",
    "            -1*p/(np.log(1-p)*(1-p))-np.mean(peak_count_new)\n",
    "        ]\n",
    "\n",
    "    solved=fsolve(solve_function,[0.995],maxfev=2000)\n",
    "    p=solved[0]\n",
    "    # print(-1*p/(np.log(1-p)*(1-p)),np.mean(peak_count_new))\n",
    "\n",
    "    peak_count_simu=logser.rvs(p,size=len(peak_count))+1\n",
    "    peak_count_simu[idx_zero]=0\n",
    "    peak_count_simu[idx_one]=1\n",
    "    \n",
    "    return peak_count_simu\n",
    "\n",
    "def ZINB(peak_count):\n",
    "    model_zinb = ZeroInflatedNegativeBinomialP(peak_count, np.ones_like(peak_count), p=1)\n",
    "    res_zinb = model_zinb.fit(method='bfgs', maxiter=5000, maxfun=5000)\n",
    "    mu = np.exp(res_zinb.params[1])\n",
    "    alpha = res_zinb.params[2]\n",
    "    pi = expit(res_zinb.params[0])\n",
    "\n",
    "    p=1/(1+alpha)\n",
    "    n=mu*p/(1-p)\n",
    "\n",
    "    peak_count_simu=(nbinom.rvs(n,p,size=len(peak_count)))*\\\n",
    "        stats.bernoulli.rvs(p = 1-pi, size = len(peak_count))\n",
    "    \n",
    "    return peak_count_simu\n",
    "\n",
    "def zero_NB(peak_count):\n",
    "    zero_prob=len(np.where(peak_count == 0)[0])/len(peak_count)\n",
    "    peak_count_new=np.delete(peak_count,np.where(peak_count == 0))\n",
    "    res=sm.NegativeBinomial(peak_count_new-1, np.ones_like(peak_count_new)).fit(start_params=[1,1])\n",
    "    mu=np.exp(res.params[0])\n",
    "    p=1/(1+mu*res.params[1])\n",
    "    n=mu*p/(1-p)\n",
    "\n",
    "    peak_count_simu=(nbinom.rvs(n,p,size=len(peak_count))+1)*\\\n",
    "        stats.bernoulli.rvs(p = 1-zero_prob, size = len(peak_count))\n",
    "    \n",
    "    return peak_count_simu\n",
    "    \n",
    "def NB(peak_count):\n",
    "    res=sm.NegativeBinomial(peak_count, np.ones_like(peak_count)).fit(start_params=[1,1])\n",
    "    mu=np.exp(res.params[0])\n",
    "    p=1/(1+mu*res.params[1])\n",
    "    n=mu*p/(1-p)\n",
    "\n",
    "    peak_count_simu=nbinom.rvs(n,p,size=len(peak_count))\n",
    "    \n",
    "    return peak_count_simu\n",
    "\n",
    "def ZIP(peak_count):\n",
    "    zip_model = ZeroInflatedPoisson(endog = peak_count, exog= np.ones_like(peak_count)) \n",
    "    zip_res = zip_model.fit()\n",
    "    mu=zip_res.params[1]\n",
    "    pi = expit(zip_res.params[0])\n",
    "    peak_count_simu = stats.bernoulli.rvs(p = 1-pi, size = len(peak_count))*\\\n",
    "            stats.poisson.rvs(mu = mu, size = len(peak_count))\n",
    "    \n",
    "    return peak_count_simu\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa905276-1b7e-4681-b7b0-4fe8823a8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Celltype_Counts(adata_part,two_embeds,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same,correct_iter=10,lib_simu='real',n_cell_total=None,\n",
    "                       distribution='Poisson',activation='sigmod'\n",
    "                       ,bw_pm=1e-4,bw_lib=0.05,bw_nozero=0.05,rand_seed=0):# 如果lib_simu为‘estimate’则需要提供对应的n_cell_total\n",
    "    \n",
    "    # np.random.seed(rand_seed)\n",
    "    # 计算真实参数\n",
    "    peak_mean=np.log(cal_pm(adata_part)+1)\n",
    "    lib_size=np.log(cal_lib(adata_part)+1)\n",
    "    nozero=np.log(cal_nozero(adata_part)+1)\n",
    "    peak_count=cal_peak_count(adata_part)\n",
    "    \n",
    "    if distribution=='Bernoulli' and np.max(np.exp(peak_mean)-1)>1:\n",
    "        raise ValueError('you data may not be Bernoulli distribution!')\n",
    "    \n",
    "    n_peak         =len(peak_mean)\n",
    "    n_cell_total   =len(lib_size) #总共的细胞数目\n",
    "    if lib_simu=='real':\n",
    "        # param_lib=lib_size\n",
    "        param_lib=np.sort(np.random.choice(lib_size,size=n_cell_total),axis=0).ravel()\n",
    "        param_nozero=np.sort(np.random.choice(nozero,size=n_cell_total),axis=0).ravel()\n",
    "    elif lib_simu=='estimate':\n",
    "        # kde_lib = KernelDensity(kernel='gaussian', bandwidth=bw_lib).fit(lib_size.reshape(-1,1))\n",
    "        # param_lib=kde_lib.sample(n_cell_total,random_state=rand_seed)\n",
    "        # param_lib=np.sort(param_lib)\n",
    "        \n",
    "        estimation_dis='one_logser' # 'NB'/'one_logser'/'gamma'/'zero_logser'\n",
    "        \n",
    "        print('the estimation method is ',estimation_dis)\n",
    "        \n",
    "        if estimation_dis=='gamma':\n",
    "            peak_mean_real = np.exp(peak_mean)-1\n",
    "            peak_mean_sqrt = np.sqrt(peak_mean_real)\n",
    "\n",
    "            fit_alpha, fit_loc, fit_beta = stats.gamma.fit(peak_mean_sqrt,floc=np.min(peak_mean_sqrt)-0.001)\n",
    "            peak_mean_sqrt_sample = stats.gamma.rvs(a=fit_alpha, loc=fit_loc, scale=fit_beta, size=n_peak, random_state=rand_seed)\n",
    "            param_pm = np.sort(peak_mean_sqrt_sample)\n",
    "            param_pm = np.log(param_pm**2+1)\n",
    "        elif estimation_dis=='zero_logser':\n",
    "            peak_count_simu=zero_logser(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "        elif estimation_dis=='one_logser':\n",
    "            peak_count_simu=one_logser(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "        elif estimation_dis=='zero_NB':\n",
    "            peak_count_simu=zero_NB(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "        elif estimation_dis=='NB':\n",
    "            peak_count_simu=NB(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "        elif estimation_dis=='ZIP':\n",
    "            peak_count_simu=ZIP(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "            \n",
    "        elif estimation_dis=='ZINB':\n",
    "            peak_count_simu=ZINB(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('wrong estimation distribution!')\n",
    "            \n",
    "        \n",
    "        \n",
    "        lib_size_real = np.exp(lib_size)-1\n",
    "        lib_size_log = np.log(lib_size_real)\n",
    "        \n",
    "        #n,random_state = 2,2022\n",
    "        gmm_lz = GMM(2, random_state=rand_seed)\n",
    "        gmm_lz.fit(lib_size_log.reshape(-1,1))\n",
    "        # [sample[0] for sample in gmm.sample(1000)]\n",
    "        lib_size_log_sample = gmm_lz.sample(n_cell_total)[0].reshape(-1)\n",
    "        param_lib = np.sort(lib_size_log_sample)\n",
    "        \n",
    "        non_zero_real = np.exp(nozero)-1\n",
    "        non_zero_log = np.log(non_zero_real)\n",
    "        gmm_nz = GMM(2, random_state=rand_seed)\n",
    "        gmm_nz.fit(non_zero_log.reshape(-1,1))\n",
    "        # [sample[0] for sample in gmm.sample(1000)]\n",
    "        non_zero_log_sample = gmm_nz.sample(n_cell_total)[0].reshape(-1)\n",
    "        param_nozero = np.log(np.exp(np.sort(non_zero_log_sample))+1)\n",
    "\n",
    "    param_pm=param_pm[peak_mean.argsort().argsort()]\n",
    "    # param_pm=np.sort(peak_mean)\n",
    "    # origin_peak=np.arange(len(peak_mean))[peak_mean.argsort()]#记录实际peak的位置，保证最后输出的与输入peak含义一致\n",
    "\n",
    "    # 生成effect和embedding\n",
    "    peak_effect,lib_size_effect=Get_Effect(n_peak,n_cell_total,\n",
    "                    len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd)\n",
    "\n",
    "    # if simu_type=='single':\n",
    "    embeds_param={}\n",
    "    embeds_param['peak'],meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "    embeds_param['lib_size'],meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "\n",
    "\n",
    "    # 从模拟矩阵的参数顺序对应到采样的真实参数\n",
    "    X_peak=np.dot(peak_effect,embeds_param['peak'].values)# peak*cell\n",
    "    X_peak=Activation(X_peak,method=activation)\n",
    "    # rank=np.arange(len(X_peak))[np.mean(X_peak,axis=1).argsort().argsort()]\n",
    "    # param_pm=param_pm[rank]\n",
    "    # origin_peak=origin_peak[rank]\n",
    "\n",
    "    if two_embeds:\n",
    "        X_lib=np.dot(lib_size_effect,embeds_param['lib_size'].values).ravel()\n",
    "    else:\n",
    "        X_lib=np.dot(lib_size_effect,embeds_param['peak'].values).ravel()\n",
    "    rank = np.arange(len(X_lib))[X_lib.argsort().argsort()]\n",
    "    param_lib=param_lib[rank]\n",
    "    param_nozero=param_nozero[rank]\n",
    "    \n",
    "\n",
    "    # 对参数进行修正\n",
    "    # X_peak维度是peak*cell\n",
    "    simu_param_peak=X_peak\n",
    "    if distribution=='Poisson':\n",
    "        for i in range(correct_iter):\n",
    "            print('correct_iter '+str(i+1))\n",
    "            simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=1).reshape(-1,1)+1e-8)*((np.exp(param_pm)-1).reshape(-1,1))*simu_param_peak.shape[1]\n",
    "            simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=0).reshape(1,-1)+1e-8)*((np.exp(param_lib)-1).reshape(1,-1))   # 分母加一个很小的数防止nan\n",
    "            \n",
    "        simu_param_lib=np.exp(param_lib)-1\n",
    "        simu_param_nozero=np.exp(param_nozero)-1\n",
    "        simu_param_pm=np.exp(param_pm)-1\n",
    "        #--------使用poisson分布生成ATAC\n",
    "        lambdas=simu_param_peak\n",
    "        # lambdas=lambdas[origin_peak.argsort(),:] #保证peak与输入peak一致\n",
    " \n",
    "        atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "    elif distribution=='Bernoulli':\n",
    "        for i in range(correct_iter):\n",
    "            print('correct_iter '+str(i+1))\n",
    "            simu_param_peak=Bernoulli_pm_correction(simu_param_peak,param_pm)\n",
    "            simu_param_peak=Bernoulli_lib_correction(simu_param_peak,param_lib)\n",
    "        atac_counts=np.random.binomial(1,p=simu_param_peak,size=simu_param_peak.shape)\n",
    "        \n",
    "        lambdas,simu_param_nozero,simu_param_lib,simu_param_pm=None,None,None,None\n",
    "    \n",
    "    return atac_counts,embeds_param['peak'].values,embeds_param['lib_size'].values,lambdas,simu_param_nozero,simu_param_lib,simu_param_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6cb8dd-1678-4fd2-8949-c5bf603b1693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
