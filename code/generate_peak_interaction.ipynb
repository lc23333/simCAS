{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af79b52-9093-43f7-a458-02cd1fa129eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a0f9bc-5cd5-4472-8d96-98bf36a9c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import os\n",
    "from scipy import sparse\n",
    "import scipy.io as sio\n",
    "import scanpy as sc\n",
    "from Bio import Phylo\n",
    "from io import StringIO\n",
    "import logging\n",
    "from scipy.optimize import fsolve\n",
    "import random\n",
    "import threading\n",
    "import scipy.stats as stats\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from scipy.stats import logser\n",
    "\n",
    "from scipy.special import rel_entr\n",
    "from statsmodels.discrete.count_model import (ZeroInflatedNegativeBinomialP, ZeroInflatedPoisson,\n",
    "                                              ZeroInflatedGeneralizedPoisson)\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import nbinom\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be14b6-1389-47c8-9a8d-0fe09289cdfa",
   "metadata": {},
   "source": [
    "# count数据 - Buenrostro2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8a58b-da1c-41ca-a784-b5a412c689e4",
   "metadata": {},
   "source": [
    "## normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2397be90-4b54-4d1e-aebd-0d2d871210ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取参数\n",
    "prefix_='Buenrostro_2018'\n",
    "# prefix_='Forebrain'\n",
    "# prefix_='MCA/Cerebellum'\n",
    "resultdir='/data1/lichen/code/second/scATAC_integration/data/scATACdata_total/process/{0}/'.format(prefix_)\n",
    "peak_mean=pd.read_csv(resultdir+'peak_mean_log.csv',index_col=0)\n",
    "lib_size=pd.read_csv(resultdir+'library_size_log.csv',index_col=0)\n",
    "nozero=pd.read_csv(resultdir+'nozero_log.csv',index_col=0)\n",
    "\n",
    "peak_mean=np.array(peak_mean['peak mean'])\n",
    "lib_size=np.array(lib_size['library size'])\n",
    "nozero=np.array(nozero['nozero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "bf218081-674f-42fe-8ca8-00e792994fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数，后续所有函数中的参数都在下面给出定义\n",
    "\n",
    "n_peak         =len(peak_mean) # peak数目\n",
    "n_cell_total   =1500 #总共的细胞数目\n",
    "rand_seed      =2022 #随机种子\n",
    "zero_prob      =0.5  #对于peak_effect的置零个数\n",
    "zero_set       ='all'#'by_row'指的是对于每一个peak的effect vector进行置零；'all'指的是随机在所有的index中选择进行置零\n",
    "effect_mean    =0 #生成effect vector的均值\n",
    "effect_sd      =1 #生成effect vector的方差\n",
    "\n",
    "min_popsize    =300 #离散模式下设定的细胞群的最小数目\n",
    "min_pop        ='A' #离散模式下设定最小细胞群的名称，注意需要与下面的tree_text一致\n",
    "tree_text      =[\"((A:0.5,B:0.5):1,(C:0.5,D:0.5):1);\", #注：前三个用来仿真离散模式，只标叶子结点名称就行；后两个为连续模式的仿真树，与标准newick形式略有不同\n",
    "                 \"((A:0.4,B:0.4,C:0.4):1,(D:1,E:1):1);\",\n",
    "                \"(((A:0.2,B:0.2):0.2,C:0.4):0.5,(D:1,E:1):1);\",\n",
    "                \"(((A:1,B:1)C:1,(D:1,E:1)F:1)R)\",\n",
    "                '((((A:1, B:1)C:0.5,D:1)E:0.5, F:2)R)',\n",
    "                '((((A:1, B:1)C:0.5,(D:1, E:1,F:1)G:1)H:0.5, I:2)R)']\n",
    "pops_name      =[['A','B','C','D'],\n",
    "                ['A','B','C','D','E'],\n",
    "                ['A','B','C','D','E']]  # 输入不同节点的名字，离散模式只需要输入叶子节点的名称就行，注意这里需要与tree_text的前三个顺序保持一致\n",
    "pops_size      =None # 设置不同cluster的细胞数目，None则直接取平均\n",
    "\n",
    "\n",
    "embed_mean_same=1 # 对embedding非差异特征采样的均值\n",
    "embed_sd_same  =0.5 # 对embedding的非差异特征采样的方差\n",
    "embed_mean_diff=1 # 对embedding差异特征采样的均值\n",
    "embed_sd_diff  =0.5 # 对embedding的差异特征采样的方差\n",
    "\n",
    "len_cell_embed =12   #仿真细胞的低维特征的特征个数\n",
    "n_embed_diff   =8 # 使得cell embedding不同的特征维度数目\n",
    "n_embed_same   =len_cell_embed-n_embed_diff\n",
    "\n",
    "simu_type      ='continuous' # continuous/discrete/single/cell_type\n",
    "correct_iter   =2 # 使用参数进行修正的迭代次数\n",
    "activation     ='exp_linear' #对参数矩阵矫正的方式，在连续和离散的条件下使用'exp'，在仿真celltype的时候应该使用'sigmod'\n",
    "\n",
    "two_embeds     =True  # true表明peak mean和library size通过两个不同的矩阵排序对应得到；False 表明通过一个矩阵的值排序对应得到\n",
    "\n",
    "adata_dir      =resultdir+'adata_forsimulation.h5ad' # 为了进行cell_type simulation\n",
    "lib_simu       ='estimate' # 在仿真cell_type时用的参数，’real‘表示直接使用真实的library_size参数，‘estimate’表示从估计的分布中采样\n",
    "distribution   ='Poisson' # 数据的分布，如果二值化就是’Bernoulli‘，count就是‘Poisson’\n",
    "\n",
    "bw_pm          =1e-4 #分别为对peak mean、library_size、nozero的核密度估计的窗宽；注：bw_pm若取的过大可能会导致采样的peak mean小于0而报错\n",
    "bw_lib         =0.05\n",
    "bw_nozero      =0.05\n",
    "\n",
    "real_param     =False # 是否使用真实的参数，True则为直接使用真实参数，False\n",
    "\n",
    "log            =None\n",
    "\n",
    "fix_seed(rand_seed)\n",
    "\n",
    "k_dict,pi_dict={},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "663e895e-adc3-4010-88a6-fcc1e54babde",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********start generate effect vector...**********\n",
      "**********generate effect finished!**********\n",
      "**********start generate cell embedding...**********\n",
      "simulation type is continuous\n",
      "**********generate cell embedding finished**********\n",
      "**********start generate counts...**********\n",
      "correct_iter 1\n",
      "correct_iter 2\n",
      "**********start ZIP correction...**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lichen/anaconda3/envs/pytorch2/lib/python3.8/site-packages/scipy/optimize/_minpack_py.py:175: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "1096\n",
      "1888.4505889573293 3390.023139254722\n",
      "=================================\n",
      "1266\n",
      "1549.8453324554303 3053.037211862144\n",
      "=================================\n",
      "1315\n",
      "1311.3524786019236 2509.9004389105935\n",
      "=================================\n",
      "1317\n",
      "1239.581867330175 2297.8816965792307\n",
      "=================================\n",
      "1320\n",
      "1482.8777681605495 2743.853006904476\n",
      "=================================\n",
      "1332\n",
      "1270.8918083005153 2423.4279439299303\n",
      "=================================\n",
      "1334\n",
      "1514.9700353217834 2862.7275983856903\n",
      "=================================\n",
      "1339\n",
      "1503.839958212363 2953.2417618971003\n",
      "=================================\n",
      "1340\n",
      "1123.5522332367718 2108.542651152407\n",
      "=================================\n",
      "1345\n",
      "1068.3350427240207 2142.080476996943\n",
      "=================================\n",
      "1347\n",
      "1465.9166139669862 2777.129120749535\n",
      "=================================\n",
      "1348\n",
      "1239.3136098505724 2297.8749568629364\n",
      "=================================\n",
      "1353\n",
      "1693.2456473256882 3270.115717193023\n",
      "=================================\n",
      "1354\n",
      "1401.5104831973185 2648.795459892721\n",
      "=================================\n",
      "1368\n",
      "1192.2769792170184 2199.797578770765\n",
      "=================================\n",
      "1369\n",
      "1415.7823553108246 2535.440847653431\n",
      "=================================\n",
      "1373\n",
      "1346.4924598510656 2502.427013644658\n",
      "=================================\n",
      "1374\n",
      "1515.2533232542316 2900.666513462077\n",
      "=================================\n",
      "1375\n",
      "1171.3638189557087 2196.8309444276906\n",
      "=================================\n",
      "1377\n",
      "1128.0614656545238 2129.0754806462514\n",
      "=================================\n",
      "1378\n",
      "1117.7996127953465 2125.8528900918764\n",
      "=================================\n",
      "1379\n",
      "1105.1848268689319 2001.817863408548\n",
      "=================================\n",
      "1380\n",
      "1115.7136117802313 2061.246595788463\n",
      "=================================\n",
      "1381\n",
      "1126.0770669298915 2177.2531132910185\n",
      "=================================\n",
      "1383\n",
      "1379.2734035998178 2471.678624033552\n",
      "=================================\n",
      "1384\n",
      "1094.2721014596348 1997.2582877413674\n",
      "=================================\n",
      "1385\n",
      "1319.6963316469398 2439.169496928679\n",
      "=================================\n",
      "1386\n",
      "1575.0803090989516 2910.435611513656\n",
      "=================================\n",
      "1389\n",
      "1402.6592566826255 2718.4333174484786\n",
      "=================================\n",
      "1392\n",
      "1106.447968003104 1972.215887426075\n",
      "=================================\n",
      "1393\n",
      "1516.1617049077654 2824.847575262297\n",
      "=================================\n",
      "1395\n",
      "1390.6018745796637 2497.1751969165375\n",
      "=================================\n",
      "1397\n",
      "1492.1424349532952 2809.2752584289783\n",
      "=================================\n",
      "1399\n",
      "1504.0150645197762 2865.6254903399363\n",
      "=================================\n",
      "1400\n",
      "1393.621677747075 2685.852684681904\n",
      "=================================\n",
      "1402\n",
      "1473.8354974278554 2852.788713623046\n",
      "=================================\n",
      "1403\n",
      "1512.0431001656382 2730.324175927572\n",
      "=================================\n",
      "1404\n",
      "1418.8565127772065 2725.1520351666272\n",
      "=================================\n",
      "1405\n",
      "1208.7475103587908 2263.4900707951583\n",
      "=================================\n",
      "1406\n",
      "1350.8318228619016 2583.034499855589\n",
      "=================================\n",
      "1407\n",
      "1205.5851822189 2261.918068125569\n",
      "=================================\n",
      "1408\n",
      "1299.3930157710868 2316.9427370478966\n",
      "=================================\n",
      "1418\n",
      "1259.2255236461467 2332.61292476347\n",
      "=================================\n",
      "1421\n",
      "1527.5932437485956 2960.673980280669\n",
      "**********ZIP correction finished!**********\n",
      "**********generate counts finshed!**********\n"
     ]
    }
   ],
   "source": [
    "# 生成effect和embedding\n",
    "print(\"**********start generate effect vector...**********\")\n",
    "peak_effect,lib_size_effect=Get_Effect(n_peak,n_cell_total,\n",
    "                len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd)\n",
    "print(\"**********generate effect finished!**********\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"**********start generate cell embedding...**********\")\n",
    "print(\"simulation type is {0}\".format(simu_type))\n",
    "if simu_type=='discrete':\n",
    "    # 重复两次获得两个矩阵，后续使用参数two_embeds决定是用两个矩阵还是用一个\n",
    "    embeds_peak,meta=Get_Discrete_Embedding(pops_name[0],min_popsize,tree_text[0],\n",
    "                 n_cell_total,pops_size,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed)\n",
    "    embeds_lib,meta=Get_Discrete_Embedding(pops_name[0],min_popsize,tree_text[0],\n",
    "                 n_cell_total,pops_size,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed+1)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished**********\")\n",
    "    # 获得count\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero,\n",
    "                                real_param)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='continuous':\n",
    "    embeds_param={}\n",
    "    embeds_peak,meta=Get_Continuous_Embedding(tree_text[3],n_cell_total,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed)\n",
    "    embeds_lib,meta=Get_Continuous_Embedding(tree_text[3],n_cell_total,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed+1)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished**********\")\n",
    "    \n",
    "    print(\"**********start generate counts...**********\")\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero,\n",
    "                               real_param)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='single':\n",
    "    embeds_param={}\n",
    "    embeds_peak,meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "    embeds_lib,meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished!**********\")\n",
    "    \n",
    "    \n",
    "    print(\"**********start generate counts...**********\")\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='cell_type':\n",
    "    adata=sc.read_h5ad(adata_dir)\n",
    "    counts_list,celltype_list,embed_peak_list,embed_lib_list=[],[],[],[]\n",
    "    lambdas_list,simu_param_nozero_list,simu_param_lib_list,simu_param_pm_list=[],[],[],[]#新加的list用来重新对lambdas进行spasity的修正\n",
    "    celltypes=np.unique(adata.obs.celltype)\n",
    "    for i in range(len(celltypes)):\n",
    "    # 可以分为直接从真实数据中进行采样或是从核密度估计中采样特定细胞数目，先做直接从真实数据中采样的结果\n",
    "        # print(celltypes[i])\n",
    "        print(\"simulating cell type: {}...\".format(celltypes[i]))\n",
    "        adata_part=adata[adata.obs.celltype==celltypes[i],:]\n",
    "\n",
    "        # 对每个celltype单独进行仿真\n",
    "        counts,embed_peak,embed_lib,lambdas,simu_param_nozero,simu_param_lib,simu_param_pm=Get_Celltype_Counts(adata_part,two_embeds,\n",
    "                                            embed_mean_same,embed_sd_same,\n",
    "                     n_embed_diff,n_embed_same,correct_iter,lib_simu=lib_simu,n_cell_total=None,\n",
    "                                        distribution=distribution,activation=activation,\n",
    "                    bw_pm=bw_pm,bw_lib=bw_lib,bw_nozero=bw_nozero,rand_seed=rand_seed) # peak*cell\n",
    "\n",
    "        counts_list.append(counts)\n",
    "        embed_peak_list.append(embed_peak)\n",
    "        embed_lib_list.append(embed_lib)\n",
    "        celltype_list.append([celltypes[i]]*counts.shape[1])\n",
    "        lambdas_list.append(lambdas)\n",
    "        simu_param_nozero_list.append(simu_param_nozero)\n",
    "        simu_param_lib_list.append(simu_param_lib)\n",
    "        simu_param_pm_list.append(simu_param_pm)\n",
    "        \n",
    "    if distribution=='Poisson':\n",
    "        # atac_counts=np.hstack(counts_list)\n",
    "        meta=np.hstack(celltype_list)\n",
    "        embeds_peak=np.hstack(embed_peak_list)\n",
    "        embeds_lib=np.hstack(embed_lib_list)\n",
    "        #对整体lambdas进行sparsity修正\n",
    "        lambdas=np.hstack(lambdas_list)\n",
    "        simu_param_nozero=np.hstack(simu_param_nozero_list)\n",
    "        simu_param_lib=np.hstack(simu_param_lib_list)\n",
    "        simu_param_pm=peak_mean\n",
    "\n",
    "        lambdas_sum=np.sum(lambdas,axis=0)\n",
    "\n",
    "#         print(\"**********start ZIP correction...**********\")\n",
    "#         k_list,pi_list=[],[]\n",
    "#         # 求解每个cell中lambda扩大的倍数和置零的比例\n",
    "#         for i in range(n_cell_total):\n",
    "#             iter_=i\n",
    "#             # print(i)\n",
    "#             def solve_function(unsolved_value):\n",
    "#                 k,pi=unsolved_value[0],unsolved_value[1]\n",
    "#                 return [\n",
    "#                     k*(1-pi)-simu_param_lib[iter_]/(lambdas_sum[iter_]),\n",
    "#                     n_peak*pi+(1-pi)*np.sum(np.exp(-lambdas[:,iter_]*k))-(n_peak-simu_param_nozero[iter_])\n",
    "#                 ]\n",
    "\n",
    "#             solved=fsolve(solve_function,[3,0.5],maxfev=2000)\n",
    "#             k,pi=solved[0],solved[1]\n",
    "#             simu1=k*(1-pi)*(lambdas_sum[iter_])\n",
    "#             real1=simu_param_lib[iter_]\n",
    "#             if abs(simu1-real1)/real1>0.1:\n",
    "#                 # print(i)\n",
    "#                 # print(simu1,real1)\n",
    "#                 # print('=================================')\n",
    "#                 solved=fsolve(solve_function,[20,0.5],maxfev=2000)\n",
    "#             k_list.append(solved[0])\n",
    "#             pi_list.append(solved[1])\n",
    "#         # 对每个cell的lambda置零并扩大相应倍数\n",
    "#         for i in range(n_cell_total):\n",
    "#             if k_list[i]==3 or k_list[i]==20 or pi_list[i]<0:\n",
    "#                 continue\n",
    "#             a=lambdas[:,i]*k_list[i]\n",
    "#             # b=atac_counts[:,i]\n",
    "#             a[np.random.choice(n_peak,replace=False,size=int(pi_list[i]*n_peak))]=0\n",
    "#             lambdas[:,i]=a\n",
    "#         print(\"**********ZIP correction finished!**********\")\n",
    "            \n",
    "        print(\"**********start ZIP correction...**********\")\n",
    "        batch_size = 1000 # 并行数目，全局字典\n",
    "        global k_dict,pi_dict\n",
    "        for i in range(0,n_cell_total,batch_size):\n",
    "            if i+batch_size<=n_cell_total:\n",
    "                my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, i+batch_size)]\n",
    "            else:\n",
    "                my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, n_cell_total)]\n",
    "            for thread_ in my_thread:\n",
    "                thread_.start()\n",
    "            for thread_ in my_thread:\n",
    "                thread_.join()\n",
    "        # 对每个cell的lambda置零并扩大相应倍数\n",
    "        for i in range(n_cell_total):\n",
    "            if k_dict[i]==3 or k_dict[i]==20 or pi_dict[i]<0 or k_dict[i]<0:\n",
    "                continue\n",
    "            a=lambdas[:,i]*k_dict[i]\n",
    "            # b=atac_counts[:,i]\n",
    "            a[np.random.choice(n_peak,replace=False,size=int(pi_dict[i]*n_peak))]=0\n",
    "            lambdas[:,i]=a\n",
    "            \n",
    "        print(\"**********ZIP correction finished!**********\")\n",
    "\n",
    "        # # spasity矫正完之后再来一轮peak mean和library size的矫正，保证都符合实际\n",
    "        # lambdas_copy=lambdas.copy()\n",
    "        # lambdas_copy=lambdas_copy/(np.sum(lambdas_copy,axis=1).reshape(-1,1)+1e-8)*(simu_param_pm.reshape(-1,1))*lambdas_copy.shape[1]\n",
    "        # lambdas_copy=lambdas_copy/(np.sum(lambdas_copy,axis=0).reshape(1,-1)+1e-8)*(simu_param_lib.reshape(1,-1))\n",
    "\n",
    "        atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "        \n",
    "    elif distribution=='Bernoulli':\n",
    "        atac_counts=np.hstack(counts_list)\n",
    "        meta=np.hstack(celltype_list)\n",
    "        embeds_peak=np.hstack(embed_peak_list)\n",
    "        embeds_lib=np.hstack(embed_lib_list)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('wrong distribution input!')\n",
    "    \n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "\n",
    "else:\n",
    "    raise ValueError('wrong simulation type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "81725954-f682-4297-aa12-418145ee4240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuous_embed12_diff8_cell1500_EmbedSd0.5_EffectSd1_prob0.5_CorrectIter2_TwoEmbeddTrue_activation_exp_linear_ContinuousTest4\n"
     ]
    }
   ],
   "source": [
    "# -----------------保存数据\n",
    "\n",
    "# 创建对应的文件夹\n",
    "resultdir=\"/data1/lichen/code/second/scATAC_integration/code/simulation/data/{}/\".format(prefix_)\n",
    "\n",
    "prefix='_'.join([simu_type,'embed'+str(len_cell_embed),\n",
    "                'diff'+str(n_embed_diff),'cell'+str(n_cell_total),\n",
    "                'EmbedSd'+str(embed_sd_diff),'EffectSd'+str(effect_sd),'prob'+str(zero_prob),\n",
    "                'CorrectIter'+str(correct_iter),'TwoEmbedd'+str(two_embeds),\n",
    "                'activation_'+activation+'_ContinuousTest4'])\n",
    "os.makedirs(os.path.join(resultdir,prefix),exist_ok=True)\n",
    "\n",
    "print(prefix)\n",
    "\n",
    "# save mtx\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix.mtx\"),sparse.csr_matrix(atac_counts.T))\n",
    "\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix_embed_pm.mtx\"),sparse.csr_matrix(embeds_peak.T))\n",
    "sio.mmwrite(os.path.join(resultdir,prefix,\"matrix_embed_lib.mtx\"),sparse.csr_matrix(embeds_lib.T))\n",
    "\n",
    "df=pd.DataFrame({'pop':meta})\n",
    "df.to_csv(os.path.join(resultdir,prefix,\"meta.tsv\"),sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad5778-e8ca-4318-8c83-46a6b6349fec",
   "metadata": {},
   "source": [
    "## 生成chromatin hub的peak effect然后生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c0d6934-c912-4d25-a321-bf661a99b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取参数\n",
    "prefix_='Buenrostro_2018'\n",
    "# prefix_='Forebrain'\n",
    "# prefix_='MCA/Cerebellum'\n",
    "resultdir='/data1/lichen/code/second/scATAC_integration/data/scATACdata_total/process/{0}/'.format(prefix_)\n",
    "peak_mean=pd.read_csv(resultdir+'peak_mean_log.csv',index_col=0)\n",
    "lib_size=pd.read_csv(resultdir+'library_size_log.csv',index_col=0)\n",
    "nozero=pd.read_csv(resultdir+'nozero_log.csv',index_col=0)\n",
    "\n",
    "peak_mean=np.array(peak_mean['peak mean'])\n",
    "lib_size=np.array(lib_size['library size'])\n",
    "nozero=np.array(nozero['nozero'])\n",
    "\n",
    "save_dir='/data1/lichen/code/second/scATAC_integration/code/simulation/data/figures/chromatin_hub_inference/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0be7bf54-a350-4690-b141-ffe767953abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1931 × 169221\n",
       "    obs: 'label', 'nb_features', 'log_nb_features', 'celltype'\n",
       "    var: 'n_cells', 'commonness'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix='Buenrostro_2018'\n",
    "resultdir='/data1/lichen/code/second/scATAC_integration/data/scATACdata_total/process/{0}/'.format(prefix)\n",
    "adata=sc.read(resultdir+'adata_forsimulation.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a723cf4-f082-455b-9a83-bfdf68c5f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数，后续所有函数中的参数都在下面给出定义\n",
    "\n",
    "n_peak         =len(peak_mean) # peak数目\n",
    "n_cell_total   =1500 #总共的细胞数目\n",
    "rand_seed      =2022 #随机种子\n",
    "zero_prob      =0  #对于peak_effect的置零个数\n",
    "zero_set       ='all'#'by_row'指的是对于每一个peak的effect vector进行置零；'all'指的是随机在所有的index中选择进行置零\n",
    "effect_mean    =0 #生成effect vector的均值\n",
    "effect_sd      =1 #生成effect vector的方差\n",
    "\n",
    "min_popsize    =300 #离散模式下设定的细胞群的最小数目\n",
    "min_pop        ='A' #离散模式下设定最小细胞群的名称，注意需要与下面的tree_text一致\n",
    "tree_text      =[\"((A:0.5,B:0.5):1,C:1);\",\n",
    "                \"((A:0.5,B:0.5):1,(C:0.5,D:0.5):1);\", #注：前三个用来仿真离散模式，只标叶子结点名称就行；后两个为连续模式的仿真树，与标准newick形式略有不同\n",
    "                 \"((A:0.4,B:0.4,C:0.4):1,(D:1,E:1):1);\",\n",
    "                \"(((A:0.2,B:0.2):0.2,C:0.4):0.5,(D:1,E:1):1);\",\n",
    "                \"(((A:1,B:1)C:1,(D:1,E:1)F:1)R)\",\n",
    "                '((((A:1, B:1)C:0.5,D:1)E:0.5, F:2)R)',\n",
    "                '((((A:1, B:1)C:0.5,(D:1, E:1,F:1)G:1)H:0.5, I:2)R)']\n",
    "pops_name      =[['A','B','C'],\n",
    "                ['A','B','C','D'],\n",
    "                ['A','B','C','D','E'],\n",
    "                ['A','B','C','D','E']]  # 输入不同节点的名字，离散模式只需要输入叶子节点的名称就行，注意这里需要与tree_text的前三个顺序保持一致\n",
    "pops_size      =None # 设置不同cluster的细胞数目，None则直接取平均\n",
    "\n",
    "\n",
    "embed_mean_same=1 # 对embedding非差异特征采样的均值\n",
    "embed_sd_same  =0.5 # 对embedding的非差异特征采样的方差\n",
    "embed_mean_diff=1 # 对embedding差异特征采样的均值\n",
    "embed_sd_diff  =0.5 # 对embedding的差异特征采样的方差\n",
    "\n",
    "len_cell_embed =12   #仿真细胞的低维特征的特征个数\n",
    "n_embed_diff   =10 # 使得cell embedding不同的特征维度数目\n",
    "n_embed_same   =len_cell_embed-n_embed_diff\n",
    "\n",
    "simu_type      ='continuous' # continuous/discrete/single/cell_type\n",
    "correct_iter   =2 # 使用参数进行修正的迭代次数\n",
    "activation     ='exp_linear' #对参数矩阵矫正的方式，在连续和离散的条件下使用'exp'，在仿真celltype的时候应该使用'sigmod'\n",
    "\n",
    "two_embeds     =True  # true表明peak mean和library size通过两个不同的矩阵排序对应得到；False 表明通过一个矩阵的值排序对应得到\n",
    "\n",
    "adata_dir      =resultdir+'adata_forsimulation.h5ad' # 为了进行cell_type simulation\n",
    "lib_simu       ='estimate' # 在仿真cell_type时用的参数，’real‘表示直接使用真实的library_size参数，‘estimate’表示从估计的分布中采样\n",
    "distribution   ='Poisson' # 数据的分布，如果二值化就是’Bernoulli‘，count就是‘Poisson’\n",
    "\n",
    "bw_pm          =1e-4 #分别为对peak mean、library_size、nozero的核密度估计的窗宽；注：bw_pm若取的过大可能会导致采样的peak mean小于0而报错\n",
    "bw_lib         =0.05\n",
    "bw_nozero      =0.05\n",
    "\n",
    "real_param     =False # 是否使用真实的参数，True则为直接使用真实参数，False\n",
    "\n",
    "log            =None\n",
    "\n",
    "fix_seed(rand_seed)\n",
    "\n",
    "k_dict,pi_dict={},{}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f59a7e6c-b7ea-46c4-8cb2-a110eef72f96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********start generate effect vector...**********\n",
      "343\n",
      "344\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "352\n",
      "353\n",
      "356\n",
      "363\n",
      "365\n",
      "368\n",
      "371\n",
      "374\n",
      "375\n",
      "377\n",
      "378\n",
      "379\n",
      "382\n",
      "383\n",
      "385\n",
      "387\n",
      "389\n",
      "390\n",
      "394\n",
      "395\n",
      "397\n",
      "399\n",
      "402\n",
      "404\n",
      "405\n",
      "407\n",
      "410\n",
      "412\n",
      "413\n",
      "415\n",
      "416\n",
      "417\n",
      "419\n",
      "678\n",
      "683\n",
      "685\n",
      "686\n",
      "687\n",
      "690\n",
      "691\n",
      "693\n",
      "695\n",
      "696\n",
      "700\n",
      "707\n",
      "708\n",
      "709\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "725\n",
      "726\n",
      "729\n",
      "730\n",
      "733\n",
      "735\n",
      "739\n",
      "743\n",
      "745\n",
      "748\n",
      "750\n",
      "751\n",
      "752\n",
      "756\n",
      "757\n",
      "758\n",
      "760\n",
      "763\n",
      "764\n",
      "768\n",
      "824\n",
      "826\n",
      "827\n",
      "829\n",
      "831\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "840\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "863\n",
      "864\n",
      "866\n",
      "867\n",
      "868\n",
      "871\n",
      "873\n",
      "876\n",
      "877\n",
      "878\n",
      "880\n",
      "881\n",
      "883\n",
      "888\n",
      "897\n",
      "898\n",
      "901\n",
      "903\n",
      "904\n",
      "905\n",
      "908\n",
      "910\n",
      "1444\n",
      "1446\n",
      "1448\n",
      "1449\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1459\n",
      "1463\n",
      "1465\n",
      "1467\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1476\n",
      "1479\n",
      "1482\n",
      "1484\n",
      "1485\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1503\n",
      "1504\n",
      "1506\n",
      "1508\n",
      "4171\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4177\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4185\n",
      "4186\n",
      "4188\n",
      "4192\n",
      "4193\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4481\n",
      "4483\n",
      "4484\n",
      "4487\n",
      "4490\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4497\n",
      "4499\n",
      "4503\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4513\n",
      "4514\n",
      "4519\n",
      "4520\n",
      "4523\n",
      "4525\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4534\n",
      "4537\n",
      "4538\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4549\n",
      "4552\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4778\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4789\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4800\n",
      "4801\n",
      "4802\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4811\n",
      "4812\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4818\n",
      "4819\n",
      "4821\n",
      "4822\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4829\n",
      "5713\n",
      "5714\n",
      "5715\n",
      "5718\n",
      "5719\n",
      "5720\n",
      "5721\n",
      "5722\n",
      "5723\n",
      "5724\n",
      "5726\n",
      "5727\n",
      "5729\n",
      "5731\n",
      "5732\n",
      "5734\n",
      "5735\n",
      "5736\n",
      "5738\n",
      "5739\n",
      "5741\n",
      "5742\n",
      "5744\n",
      "5745\n",
      "5746\n",
      "5748\n",
      "5749\n",
      "5750\n",
      "5753\n",
      "5754\n",
      "5755\n",
      "5756\n",
      "5757\n",
      "5758\n",
      "5759\n",
      "5760\n",
      "5761\n",
      "5762\n",
      "5766\n",
      "5767\n",
      "5905\n",
      "5908\n",
      "5909\n",
      "5911\n",
      "5915\n",
      "5919\n",
      "5921\n",
      "5926\n",
      "5927\n",
      "5928\n",
      "5929\n",
      "5930\n",
      "5932\n",
      "5933\n",
      "5936\n",
      "5937\n",
      "5941\n",
      "5942\n",
      "5946\n",
      "5947\n",
      "5949\n",
      "5951\n",
      "5952\n",
      "5954\n",
      "5956\n",
      "5958\n",
      "5959\n",
      "5960\n",
      "5965\n",
      "5966\n",
      "5967\n",
      "5968\n",
      "5972\n",
      "5973\n",
      "5974\n",
      "5975\n",
      "5976\n",
      "5977\n",
      "5979\n",
      "5982\n",
      "6258\n",
      "6259\n",
      "6260\n",
      "6261\n",
      "6263\n",
      "6266\n",
      "6267\n",
      "6268\n",
      "6269\n",
      "6270\n",
      "6271\n",
      "6272\n",
      "6274\n",
      "6275\n",
      "6277\n",
      "6278\n",
      "6279\n",
      "6280\n",
      "6283\n",
      "6285\n",
      "6286\n",
      "6287\n",
      "6289\n",
      "6290\n",
      "6291\n",
      "6292\n",
      "6296\n",
      "6297\n",
      "6298\n",
      "6299\n",
      "6300\n",
      "6301\n",
      "6302\n",
      "6303\n",
      "6305\n",
      "6307\n",
      "6308\n",
      "6312\n",
      "6313\n",
      "6315\n",
      "7190\n",
      "7191\n",
      "7192\n",
      "7194\n",
      "7197\n",
      "7200\n",
      "7201\n",
      "7202\n",
      "7204\n",
      "7205\n",
      "7207\n",
      "7208\n",
      "7210\n",
      "7212\n",
      "7213\n",
      "7214\n",
      "7216\n",
      "7217\n",
      "7218\n",
      "7220\n",
      "7221\n",
      "7222\n",
      "7224\n",
      "7225\n",
      "7226\n",
      "7227\n",
      "7228\n",
      "7229\n",
      "7231\n",
      "7232\n",
      "7234\n",
      "7235\n",
      "7237\n",
      "7238\n",
      "7239\n",
      "7240\n",
      "7242\n",
      "7243\n",
      "7244\n",
      "7245\n",
      "7973\n",
      "7975\n",
      "7977\n",
      "7978\n",
      "7982\n",
      "7984\n",
      "7986\n",
      "7988\n",
      "7990\n",
      "7991\n",
      "7992\n",
      "7996\n",
      "7997\n",
      "7998\n",
      "7999\n",
      "8000\n",
      "8002\n",
      "8004\n",
      "8005\n",
      "8006\n",
      "8007\n",
      "8008\n",
      "8009\n",
      "8011\n",
      "8016\n",
      "8017\n",
      "8018\n",
      "8019\n",
      "8020\n",
      "8021\n",
      "8022\n",
      "8023\n",
      "8024\n",
      "8026\n",
      "8027\n",
      "8029\n",
      "8030\n",
      "8032\n",
      "8033\n",
      "8035\n",
      "13548\n",
      "13552\n",
      "13553\n",
      "13554\n",
      "13556\n",
      "13557\n",
      "13558\n",
      "13562\n",
      "13564\n",
      "13565\n",
      "13566\n",
      "13567\n",
      "13568\n",
      "13569\n",
      "13570\n",
      "13571\n",
      "13572\n",
      "13573\n",
      "13574\n",
      "13575\n",
      "13576\n",
      "13577\n",
      "13579\n",
      "13580\n",
      "13581\n",
      "13583\n",
      "13584\n",
      "13585\n",
      "13586\n",
      "13587\n",
      "13588\n",
      "13589\n",
      "13590\n",
      "13591\n",
      "13592\n",
      "13593\n",
      "13594\n",
      "13596\n",
      "13597\n",
      "13598\n",
      "15432\n",
      "15433\n",
      "15434\n",
      "15436\n",
      "15438\n",
      "15439\n",
      "15440\n",
      "15441\n",
      "15443\n",
      "15444\n",
      "15445\n",
      "15446\n",
      "15447\n",
      "15448\n",
      "15451\n",
      "15452\n",
      "15454\n",
      "15455\n",
      "15456\n",
      "15458\n",
      "15459\n",
      "15460\n",
      "15461\n",
      "15462\n",
      "15463\n",
      "15464\n",
      "15467\n",
      "15469\n",
      "15470\n",
      "15471\n",
      "15472\n",
      "15473\n",
      "15477\n",
      "15478\n",
      "15479\n",
      "15480\n",
      "15481\n",
      "15482\n",
      "15483\n",
      "15486\n",
      "**********generate effect finished!**********\n"
     ]
    }
   ],
   "source": [
    "# 生成effect和embedding\n",
    "import pickle\n",
    "with open(save_dir+'hub_peak_dict.pkl', 'rb') as f:\n",
    "    hub_peak_dict = pickle.load(f)\n",
    "with open(save_dir+'gene_feature_dict.pkl', 'rb') as f:\n",
    "    gene_feature_dict = pickle.load(f)\n",
    "\n",
    "peaks=adata.var_names\n",
    "chr_hub_sd=0.01\n",
    "\n",
    "# 生成hub_vector_dict,peak_hub_dict\n",
    "hub_vector_dict={}\n",
    "peak_hub_dict={}\n",
    "for (key,value) in hub_peak_dict.items():\n",
    "    hub_vector_dict[key]=np.random.normal(effect_mean,effect_sd,(len_cell_embed,))\n",
    "    for peak_ in value:\n",
    "        # print(peak_)\n",
    "        peak_hub_dict[peak_]=key\n",
    "    \n",
    "print(\"**********start generate effect vector...**********\")\n",
    "# peak_effect,lib_size_effect=Get_Effect(n_peak,n_cell_total,\n",
    "#                 len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd)\n",
    "\n",
    "peak_effect=np.random.normal(effect_mean,effect_sd,(n_peak,len_cell_embed))\n",
    "for (i,peak) in enumerate(peaks):\n",
    "    if peak in peak_hub_dict:\n",
    "        # peak_effect[i]=np.random.normal(hub_vector_dict[peak_hub_dict[peak]],chr_hub_sd,(len_cell_embed,))\n",
    "        peak_effect[i]=np.random.normal(hub_vector_dict['hub_3_KAZN'],chr_hub_sd,(len_cell_embed,))\n",
    "        print(i)\n",
    "# 生成lib effect并置零\n",
    "\n",
    "lib_size_effect=np.random.normal(effect_mean,effect_sd,(1,len_cell_embed))\n",
    "\n",
    "# # 対生成的effect vevtor进行置零\n",
    "# if zero_set=='by_row':\n",
    "#     # 对于每个peak的effect进行相同概率的置零\n",
    "#     def set_zero(a,zero_prob=0.5):\n",
    "#         a[np.random.choice(len(a),replace=False,size=int(len(a)*zero_prob))]=0\n",
    "#         return a\n",
    "#     peak_effect=np.apply_along_axis(set_zero,1,peak_effect,zero_prob=zero_prob)\n",
    "\n",
    "# if zero_set=='all':\n",
    "#     # 对于所有index选择进行置零\n",
    "#     indices = np.random.choice(peak_effect.shape[1]*peak_effect.shape[0], replace=False, size=int(peak_effect.shape[1]*peak_effect.shape[0]*zero_prob))\n",
    "#     peak_effect[np.unravel_index(indices, peak_effect.shape)] = 0 \n",
    "\n",
    "print(\"**********generate effect finished!**********\")\n",
    "\n",
    "# hub_vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0c769cd-1c47-4c80-9901-e1ac0e95828e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chr1_2985257_2985456': 'hub_0_PRDM16',\n",
       " 'chr1_2985626_2985990': 'hub_0_PRDM16',\n",
       " 'chr1_2988765_2988999': 'hub_0_PRDM16',\n",
       " 'chr1_2994910_2995992': 'hub_0_PRDM16',\n",
       " 'chr1_3003891_3004190': 'hub_0_PRDM16',\n",
       " 'chr1_3010271_3010997': 'hub_0_PRDM16',\n",
       " 'chr1_3013045_3013292': 'hub_0_PRDM16',\n",
       " 'chr1_3019763_3019913': 'hub_0_PRDM16',\n",
       " 'chr1_3033876_3034026': 'hub_0_PRDM16',\n",
       " 'chr1_3049308_3049560': 'hub_0_PRDM16',\n",
       " 'chr1_3087044_3088038': 'hub_0_PRDM16',\n",
       " 'chr1_3090886_3091775': 'hub_0_PRDM16',\n",
       " 'chr1_3099903_3100376': 'hub_0_PRDM16',\n",
       " 'chr1_3119825_3120324': 'hub_0_PRDM16',\n",
       " 'chr1_3138555_3139322': 'hub_0_PRDM16',\n",
       " 'chr1_3163985_3164421': 'hub_0_PRDM16',\n",
       " 'chr1_3168667_3168853': 'hub_0_PRDM16',\n",
       " 'chr1_3188289_3188673': 'hub_0_PRDM16',\n",
       " 'chr1_3200467_3201000': 'hub_0_PRDM16',\n",
       " 'chr1_3229487_3230108': 'hub_0_PRDM16',\n",
       " 'chr1_3233040_3233717': 'hub_0_PRDM16',\n",
       " 'chr1_3237186_3237581': 'hub_0_PRDM16',\n",
       " 'chr1_3244614_3245205': 'hub_0_PRDM16',\n",
       " 'chr1_3254742_3255230': 'hub_0_PRDM16',\n",
       " 'chr1_3255857_3256052': 'hub_0_PRDM16',\n",
       " 'chr1_3274110_3274480': 'hub_0_PRDM16',\n",
       " 'chr1_3281272_3281520': 'hub_0_PRDM16',\n",
       " 'chr1_3284188_3284426': 'hub_0_PRDM16',\n",
       " 'chr1_3291907_3292376': 'hub_0_PRDM16',\n",
       " 'chr1_3318967_3319157': 'hub_0_PRDM16',\n",
       " 'chr1_3326667_3327009': 'hub_0_PRDM16',\n",
       " 'chr1_3338341_3338756': 'hub_0_PRDM16',\n",
       " 'chr1_3341172_3341610': 'hub_0_PRDM16',\n",
       " 'chr1_3347805_3348286': 'hub_0_PRDM16',\n",
       " 'chr1_3370792_3371336': 'hub_0_PRDM16',\n",
       " 'chr1_3371447_3371685': 'hub_0_PRDM16',\n",
       " 'chr1_3382508_3382922': 'hub_0_PRDM16',\n",
       " 'chr1_3387733_3388124': 'hub_0_PRDM16',\n",
       " 'chr1_3393891_3394091': 'hub_0_PRDM16',\n",
       " 'chr1_3400109_3400762': 'hub_0_PRDM16',\n",
       " 'chr1_6794961_6795412': 'hub_1_CAMTA1',\n",
       " 'chr1_6802001_6802452': 'hub_1_CAMTA1',\n",
       " 'chr1_6836962_6837467': 'hub_1_CAMTA1',\n",
       " 'chr1_6844344_6845664': 'hub_1_CAMTA1',\n",
       " 'chr1_6854324_6854479': 'hub_1_CAMTA1',\n",
       " 'chr1_6865223_6865421': 'hub_1_CAMTA1',\n",
       " 'chr1_6878834_6879180': 'hub_1_CAMTA1',\n",
       " 'chr1_6950000_6950446': 'hub_1_CAMTA1',\n",
       " 'chr1_6961567_6961913': 'hub_1_CAMTA1',\n",
       " 'chr1_6966903_6967124': 'hub_1_CAMTA1',\n",
       " 'chr1_7022159_7023834': 'hub_1_CAMTA1',\n",
       " 'chr1_7130265_7131310': 'hub_1_CAMTA1',\n",
       " 'chr1_7135335_7135729': 'hub_1_CAMTA1',\n",
       " 'chr1_7168307_7168724': 'hub_1_CAMTA1',\n",
       " 'chr1_7210342_7210804': 'hub_1_CAMTA1',\n",
       " 'chr1_7231682_7232579': 'hub_1_CAMTA1',\n",
       " 'chr1_7236898_7237216': 'hub_1_CAMTA1',\n",
       " 'chr1_7254714_7254902': 'hub_1_CAMTA1',\n",
       " 'chr1_7256096_7256578': 'hub_1_CAMTA1',\n",
       " 'chr1_7267841_7268349': 'hub_1_CAMTA1',\n",
       " 'chr1_7322641_7322912': 'hub_1_CAMTA1',\n",
       " 'chr1_7394717_7395187': 'hub_1_CAMTA1',\n",
       " 'chr1_7568156_7568379': 'hub_1_CAMTA1',\n",
       " 'chr1_7576497_7576906': 'hub_1_CAMTA1',\n",
       " 'chr1_7609252_7610124': 'hub_1_CAMTA1',\n",
       " 'chr1_7621558_7621801': 'hub_1_CAMTA1',\n",
       " 'chr1_7676495_7676645': 'hub_1_CAMTA1',\n",
       " 'chr1_7692160_7692914': 'hub_1_CAMTA1',\n",
       " 'chr1_7707022_7707601': 'hub_1_CAMTA1',\n",
       " 'chr1_7719497_7719862': 'hub_1_CAMTA1',\n",
       " 'chr1_7729408_7730078': 'hub_1_CAMTA1',\n",
       " 'chr1_7739922_7741277': 'hub_1_CAMTA1',\n",
       " 'chr1_7751648_7752102': 'hub_1_CAMTA1',\n",
       " 'chr1_7786653_7787021': 'hub_1_CAMTA1',\n",
       " 'chr1_7789016_7789305': 'hub_1_CAMTA1',\n",
       " 'chr1_7793538_7794080': 'hub_1_CAMTA1',\n",
       " 'chr1_7813325_7813610': 'hub_1_CAMTA1',\n",
       " 'chr1_7830710_7831956': 'hub_1_CAMTA1',\n",
       " 'chr1_7841585_7841895': 'hub_1_CAMTA1',\n",
       " 'chr1_7868262_7868619': 'hub_1_CAMTA1',\n",
       " 'chr1_8374443_8374640': 'hub_2_RERE',\n",
       " 'chr1_8377929_8378795': 'hub_2_RERE',\n",
       " 'chr1_8389466_8389631': 'hub_2_RERE',\n",
       " 'chr1_8411607_8412063': 'hub_2_RERE',\n",
       " 'chr1_8422634_8422910': 'hub_2_RERE',\n",
       " 'chr1_8455419_8456072': 'hub_2_RERE',\n",
       " 'chr1_8468917_8469745': 'hub_2_RERE',\n",
       " 'chr1_8472292_8472624': 'hub_2_RERE',\n",
       " 'chr1_8477505_8477851': 'hub_2_RERE',\n",
       " 'chr1_8487570_8488133': 'hub_2_RERE',\n",
       " 'chr1_8517199_8517392': 'hub_2_RERE',\n",
       " 'chr1_8517908_8518328': 'hub_2_RERE',\n",
       " 'chr1_8518519_8518816': 'hub_2_RERE',\n",
       " 'chr1_8523041_8523498': 'hub_2_RERE',\n",
       " 'chr1_8525162_8525578': 'hub_2_RERE',\n",
       " 'chr1_8525847_8526063': 'hub_2_RERE',\n",
       " 'chr1_8532736_8532970': 'hub_2_RERE',\n",
       " 'chr1_8536132_8536598': 'hub_2_RERE',\n",
       " 'chr1_8621138_8621552': 'hub_2_RERE',\n",
       " 'chr1_8622367_8622597': 'hub_2_RERE',\n",
       " 'chr1_8630536_8630744': 'hub_2_RERE',\n",
       " 'chr1_8631190_8631491': 'hub_2_RERE',\n",
       " 'chr1_8631603_8631910': 'hub_2_RERE',\n",
       " 'chr1_8664313_8664573': 'hub_2_RERE',\n",
       " 'chr1_8681796_8682297': 'hub_2_RERE',\n",
       " 'chr1_8686419_8686894': 'hub_2_RERE',\n",
       " 'chr1_8689583_8690308': 'hub_2_RERE',\n",
       " 'chr1_8694165_8694634': 'hub_2_RERE',\n",
       " 'chr1_8707448_8708161': 'hub_2_RERE',\n",
       " 'chr1_8712199_8712349': 'hub_2_RERE',\n",
       " 'chr1_8724087_8724475': 'hub_2_RERE',\n",
       " 'chr1_8738762_8738912': 'hub_2_RERE',\n",
       " 'chr1_8797054_8797204': 'hub_2_RERE',\n",
       " 'chr1_8806499_8807087': 'hub_2_RERE',\n",
       " 'chr1_8856889_8857257': 'hub_2_RERE',\n",
       " 'chr1_8877572_8878334': 'hub_2_RERE',\n",
       " 'chr1_8889402_8890263': 'hub_2_RERE',\n",
       " 'chr1_8891454_8891797': 'hub_2_RERE',\n",
       " 'chr1_8917954_8918960': 'hub_2_RERE',\n",
       " 'chr1_8922585_8922998': 'hub_2_RERE',\n",
       " 'chr1_14924645_14924970': 'hub_3_KAZN',\n",
       " 'chr1_14925818_14925974': 'hub_3_KAZN',\n",
       " 'chr1_14934883_14935123': 'hub_3_KAZN',\n",
       " 'chr1_14938174_14938425': 'hub_3_KAZN',\n",
       " 'chr1_14987326_14987957': 'hub_3_KAZN',\n",
       " 'chr1_15000643_15000984': 'hub_3_KAZN',\n",
       " 'chr1_15032977_15033764': 'hub_3_KAZN',\n",
       " 'chr1_15051327_15051477': 'hub_3_KAZN',\n",
       " 'chr1_15058462_15058685': 'hub_3_KAZN',\n",
       " 'chr1_15062293_15062711': 'hub_3_KAZN',\n",
       " 'chr1_15064103_15064287': 'hub_3_KAZN',\n",
       " 'chr1_15068762_15068989': 'hub_3_KAZN',\n",
       " 'chr1_15104310_15105513': 'hub_3_KAZN',\n",
       " 'chr1_15109306_15109596': 'hub_3_KAZN',\n",
       " 'chr1_15111143_15111662': 'hub_3_KAZN',\n",
       " 'chr1_15131049_15131444': 'hub_3_KAZN',\n",
       " 'chr1_15137010_15138023': 'hub_3_KAZN',\n",
       " 'chr1_15143745_15144066': 'hub_3_KAZN',\n",
       " 'chr1_15150807_15151081': 'hub_3_KAZN',\n",
       " 'chr1_15188667_15188925': 'hub_3_KAZN',\n",
       " 'chr1_15224138_15224313': 'hub_3_KAZN',\n",
       " 'chr1_15270989_15271578': 'hub_3_KAZN',\n",
       " 'chr1_15296762_15296939': 'hub_3_KAZN',\n",
       " 'chr1_15307302_15307639': 'hub_3_KAZN',\n",
       " 'chr1_15364631_15364933': 'hub_3_KAZN',\n",
       " 'chr1_15373027_15373343': 'hub_3_KAZN',\n",
       " 'chr1_15386380_15386652': 'hub_3_KAZN',\n",
       " 'chr1_15402292_15402826': 'hub_3_KAZN',\n",
       " 'chr1_15410112_15410475': 'hub_3_KAZN',\n",
       " 'chr1_15415940_15416582': 'hub_3_KAZN',\n",
       " 'chr1_15416792_15417332': 'hub_3_KAZN',\n",
       " 'chr1_15426525_15427381': 'hub_3_KAZN',\n",
       " 'chr1_15427414_15427642': 'hub_3_KAZN',\n",
       " 'chr1_15433064_15433781': 'hub_3_KAZN',\n",
       " 'chr1_15438636_15439195': 'hub_3_KAZN',\n",
       " 'chr1_15445778_15445992': 'hub_3_KAZN',\n",
       " 'chr1_15462192_15462455': 'hub_3_KAZN',\n",
       " 'chr1_15464360_15464648': 'hub_3_KAZN',\n",
       " 'chr1_15473234_15473728': 'hub_3_KAZN',\n",
       " 'chr1_15479987_15480837': 'hub_3_KAZN',\n",
       " 'chr1_39499879_39500258': 'hub_4_MACF1',\n",
       " 'chr1_39529277_39529762': 'hub_4_MACF1',\n",
       " 'chr1_39530435_39530740': 'hub_4_MACF1',\n",
       " 'chr1_39546623_39547781': 'hub_4_MACF1',\n",
       " 'chr1_39552716_39553224': 'hub_4_MACF1',\n",
       " 'chr1_39564264_39564609': 'hub_4_MACF1',\n",
       " 'chr1_39568085_39569225': 'hub_4_MACF1',\n",
       " 'chr1_39570859_39571212': 'hub_4_MACF1',\n",
       " 'chr1_39571568_39571897': 'hub_4_MACF1',\n",
       " 'chr1_39595783_39596334': 'hub_4_MACF1',\n",
       " 'chr1_39600607_39601072': 'hub_4_MACF1',\n",
       " 'chr1_39620272_39620712': 'hub_4_MACF1',\n",
       " 'chr1_39657334_39657567': 'hub_4_MACF1',\n",
       " 'chr1_39657702_39658190': 'hub_4_MACF1',\n",
       " 'chr1_39661984_39662233': 'hub_4_MACF1',\n",
       " 'chr1_39662550_39662934': 'hub_4_MACF1',\n",
       " 'chr1_39669987_39671320': 'hub_4_MACF1',\n",
       " 'chr1_39680489_39681779': 'hub_4_MACF1',\n",
       " 'chr1_39687555_39687995': 'hub_4_MACF1',\n",
       " 'chr1_39696343_39697008': 'hub_4_MACF1',\n",
       " 'chr1_39708566_39708716': 'hub_4_MACF1',\n",
       " 'chr1_39724181_39724815': 'hub_4_MACF1',\n",
       " 'chr1_39727497_39727721': 'hub_4_MACF1',\n",
       " 'chr1_39731998_39732221': 'hub_4_MACF1',\n",
       " 'chr1_39733728_39734204': 'hub_4_MACF1',\n",
       " 'chr1_39748279_39748618': 'hub_4_MACF1',\n",
       " 'chr1_39783579_39784355': 'hub_4_MACF1',\n",
       " 'chr1_39808813_39809349': 'hub_4_MACF1',\n",
       " 'chr1_39820284_39820598': 'hub_4_MACF1',\n",
       " 'chr1_39826427_39826952': 'hub_4_MACF1',\n",
       " 'chr1_39864029_39864770': 'hub_4_MACF1',\n",
       " 'chr1_39874127_39875262': 'hub_4_MACF1',\n",
       " 'chr1_39876600_39876868': 'hub_4_MACF1',\n",
       " 'chr1_39891944_39892391': 'hub_4_MACF1',\n",
       " 'chr1_39956469_39957774': 'hub_4_MACF1',\n",
       " 'chr1_39963322_39963672': 'hub_4_MACF1',\n",
       " 'chr1_39966691_39967099': 'hub_4_MACF1',\n",
       " 'chr1_39980479_39980704': 'hub_4_MACF1',\n",
       " 'chr1_39981027_39981603': 'hub_4_MACF1',\n",
       " 'chr1_39991305_39992026': 'hub_4_MACF1',\n",
       " 'chr1_41952317_41952568': 'hub_5_HIVEP3',\n",
       " 'chr1_41958783_41959066': 'hub_5_HIVEP3',\n",
       " 'chr1_41961716_41962378': 'hub_5_HIVEP3',\n",
       " 'chr1_41981678_41982623': 'hub_5_HIVEP3',\n",
       " 'chr1_41987657_41987840': 'hub_5_HIVEP3',\n",
       " 'chr1_41991556_41991792': 'hub_5_HIVEP3',\n",
       " 'chr1_42006557_42006812': 'hub_5_HIVEP3',\n",
       " 'chr1_42009291_42009563': 'hub_5_HIVEP3',\n",
       " 'chr1_42013791_42014190': 'hub_5_HIVEP3',\n",
       " 'chr1_42039442_42040286': 'hub_5_HIVEP3',\n",
       " 'chr1_42064383_42064533': 'hub_5_HIVEP3',\n",
       " 'chr1_42139072_42139410': 'hub_5_HIVEP3',\n",
       " 'chr1_42175055_42175355': 'hub_5_HIVEP3',\n",
       " 'chr1_42180623_42180947': 'hub_5_HIVEP3',\n",
       " 'chr1_42181198_42181699': 'hub_5_HIVEP3',\n",
       " 'chr1_42194142_42194812': 'hub_5_HIVEP3',\n",
       " 'chr1_42200416_42200661': 'hub_5_HIVEP3',\n",
       " 'chr1_42217550_42218076': 'hub_5_HIVEP3',\n",
       " 'chr1_42230361_42230599': 'hub_5_HIVEP3',\n",
       " 'chr1_42234167_42234731': 'hub_5_HIVEP3',\n",
       " 'chr1_42237399_42237601': 'hub_5_HIVEP3',\n",
       " 'chr1_42256185_42256415': 'hub_5_HIVEP3',\n",
       " 'chr1_42257884_42258372': 'hub_5_HIVEP3',\n",
       " 'chr1_42268415_42269131': 'hub_5_HIVEP3',\n",
       " 'chr1_42275454_42275681': 'hub_5_HIVEP3',\n",
       " 'chr1_42277117_42277408': 'hub_5_HIVEP3',\n",
       " 'chr1_42309008_42309436': 'hub_5_HIVEP3',\n",
       " 'chr1_42317192_42317705': 'hub_5_HIVEP3',\n",
       " 'chr1_42335710_42336262': 'hub_5_HIVEP3',\n",
       " 'chr1_42340025_42340479': 'hub_5_HIVEP3',\n",
       " 'chr1_42342595_42342957': 'hub_5_HIVEP3',\n",
       " 'chr1_42345187_42345379': 'hub_5_HIVEP3',\n",
       " 'chr1_42356623_42357559': 'hub_5_HIVEP3',\n",
       " 'chr1_42363796_42364256': 'hub_5_HIVEP3',\n",
       " 'chr1_42367631_42368143': 'hub_5_HIVEP3',\n",
       " 'chr1_42372629_42373038': 'hub_5_HIVEP3',\n",
       " 'chr1_42392605_42392816': 'hub_5_HIVEP3',\n",
       " 'chr1_42407049_42407414': 'hub_5_HIVEP3',\n",
       " 'chr1_42421066_42421588': 'hub_5_HIVEP3',\n",
       " 'chr1_42442865_42443596': 'hub_5_HIVEP3',\n",
       " 'chr1_44827257_44827468': 'hub_6_RNF220',\n",
       " 'chr1_44831628_44832228': 'hub_6_RNF220',\n",
       " 'chr1_44843731_44844295': 'hub_6_RNF220',\n",
       " 'chr1_44870562_44871215': 'hub_6_RNF220',\n",
       " 'chr1_44871396_44872086': 'hub_6_RNF220',\n",
       " 'chr1_44872472_44872796': 'hub_6_RNF220',\n",
       " 'chr1_44882890_44884036': 'hub_6_RNF220',\n",
       " 'chr1_44887083_44887271': 'hub_6_RNF220',\n",
       " 'chr1_44887910_44888123': 'hub_6_RNF220',\n",
       " 'chr1_44890253_44891090': 'hub_6_RNF220',\n",
       " 'chr1_44899421_44899607': 'hub_6_RNF220',\n",
       " 'chr1_44925830_44925980': 'hub_6_RNF220',\n",
       " 'chr1_44925996_44926627': 'hub_6_RNF220',\n",
       " 'chr1_44950101_44950410': 'hub_6_RNF220',\n",
       " 'chr1_44951202_44952037': 'hub_6_RNF220',\n",
       " 'chr1_44965009_44965301': 'hub_6_RNF220',\n",
       " 'chr1_44973223_44973886': 'hub_6_RNF220',\n",
       " 'chr1_44981078_44981590': 'hub_6_RNF220',\n",
       " 'chr1_44984899_44985508': 'hub_6_RNF220',\n",
       " 'chr1_44989871_44991074': 'hub_6_RNF220',\n",
       " 'chr1_44997486_44998336': 'hub_6_RNF220',\n",
       " 'chr1_45008576_45009434': 'hub_6_RNF220',\n",
       " 'chr1_45015132_45015460': 'hub_6_RNF220',\n",
       " 'chr1_45018865_45019650': 'hub_6_RNF220',\n",
       " 'chr1_45023415_45023686': 'hub_6_RNF220',\n",
       " 'chr1_45029702_45030075': 'hub_6_RNF220',\n",
       " 'chr1_45037627_45038058': 'hub_6_RNF220',\n",
       " 'chr1_45041088_45041433': 'hub_6_RNF220',\n",
       " 'chr1_45058223_45058707': 'hub_6_RNF220',\n",
       " 'chr1_45060661_45061299': 'hub_6_RNF220',\n",
       " 'chr1_45066404_45066704': 'hub_6_RNF220',\n",
       " 'chr1_45097225_45098698': 'hub_6_RNF220',\n",
       " 'chr1_45103610_45104349': 'hub_6_RNF220',\n",
       " 'chr1_45121288_45121532': 'hub_6_RNF220',\n",
       " 'chr1_45127364_45128136': 'hub_6_RNF220',\n",
       " 'chr1_45139708_45140489': 'hub_6_RNF220',\n",
       " 'chr1_45142201_45142493': 'hub_6_RNF220',\n",
       " 'chr1_45150115_45150680': 'hub_6_RNF220',\n",
       " 'chr1_45158867_45159152': 'hub_6_RNF220',\n",
       " 'chr1_45163834_45164453': 'hub_6_RNF220',\n",
       " 'chr1_57439689_57439839': 'hub_7_DAB1',\n",
       " 'chr1_57440045_57440305': 'hub_7_DAB1',\n",
       " 'chr1_57538457_57538722': 'hub_7_DAB1',\n",
       " 'chr1_57684271_57684773': 'hub_7_DAB1',\n",
       " 'chr1_57701354_57701559': 'hub_7_DAB1',\n",
       " 'chr1_57701612_57702556': 'hub_7_DAB1',\n",
       " 'chr1_57773017_57773472': 'hub_7_DAB1',\n",
       " 'chr1_57887928_57888354': 'hub_7_DAB1',\n",
       " 'chr1_57888385_57888561': 'hub_7_DAB1',\n",
       " 'chr1_57889547_57890087': 'hub_7_DAB1',\n",
       " 'chr1_57984161_57984704': 'hub_7_DAB1',\n",
       " 'chr1_58012644_58012869': 'hub_7_DAB1',\n",
       " 'chr1_58173460_58173770': 'hub_7_DAB1',\n",
       " 'chr1_58202755_58202940': 'hub_7_DAB1',\n",
       " 'chr1_58227383_58227645': 'hub_7_DAB1',\n",
       " 'chr1_58423796_58423946': 'hub_7_DAB1',\n",
       " 'chr1_58452021_58452676': 'hub_7_DAB1',\n",
       " 'chr1_58465767_58466258': 'hub_7_DAB1',\n",
       " 'chr1_58478242_58478573': 'hub_7_DAB1',\n",
       " 'chr1_58513006_58513403': 'hub_7_DAB1',\n",
       " 'chr1_58570584_58571354': 'hub_7_DAB1',\n",
       " 'chr1_58577257_58577540': 'hub_7_DAB1',\n",
       " 'chr1_58614920_58615384': 'hub_7_DAB1',\n",
       " 'chr1_58620935_58621318': 'hub_7_DAB1',\n",
       " 'chr1_58621463_58622107': 'hub_7_DAB1',\n",
       " 'chr1_58630737_58631124': 'hub_7_DAB1',\n",
       " 'chr1_58715828_58716306': 'hub_7_DAB1',\n",
       " 'chr1_58717065_58717591': 'hub_7_DAB1',\n",
       " 'chr1_58877252_58877464': 'hub_7_DAB1',\n",
       " 'chr1_58902593_58902960': 'hub_7_DAB1',\n",
       " 'chr1_58952768_58952998': 'hub_7_DAB1',\n",
       " 'chr1_58963250_58963454': 'hub_7_DAB1',\n",
       " 'chr1_58981571_58981822': 'hub_7_DAB1',\n",
       " 'chr1_59011699_59012714': 'hub_7_DAB1',\n",
       " 'chr1_59042243_59043019': 'hub_7_DAB1',\n",
       " 'chr1_59043198_59043703': 'hub_7_DAB1',\n",
       " 'chr1_59043767_59044026': 'hub_7_DAB1',\n",
       " 'chr1_59044158_59044362': 'hub_7_DAB1',\n",
       " 'chr1_59050553_59051576': 'hub_7_DAB1',\n",
       " 'chr1_59062231_59062678': 'hub_7_DAB1',\n",
       " 'chr1_61332642_61332917': 'hub_8_NFIA',\n",
       " 'chr1_61370235_61370607': 'hub_8_NFIA',\n",
       " 'chr1_61377596_61378146': 'hub_8_NFIA',\n",
       " 'chr1_61388136_61388475': 'hub_8_NFIA',\n",
       " 'chr1_61408802_61409205': 'hub_8_NFIA',\n",
       " 'chr1_61435733_61436007': 'hub_8_NFIA',\n",
       " 'chr1_61444655_61445088': 'hub_8_NFIA',\n",
       " 'chr1_61508326_61509385': 'hub_8_NFIA',\n",
       " 'chr1_61514576_61514870': 'hub_8_NFIA',\n",
       " 'chr1_61515562_61516424': 'hub_8_NFIA',\n",
       " 'chr1_61519515_61519826': 'hub_8_NFIA',\n",
       " 'chr1_61522517_61523113': 'hub_8_NFIA',\n",
       " 'chr1_61537397_61537769': 'hub_8_NFIA',\n",
       " 'chr1_61542376_61543144': 'hub_8_NFIA',\n",
       " 'chr1_61547894_61548374': 'hub_8_NFIA',\n",
       " 'chr1_61549035_61549522': 'hub_8_NFIA',\n",
       " 'chr1_61569890_61570422': 'hub_8_NFIA',\n",
       " 'chr1_61581889_61582442': 'hub_8_NFIA',\n",
       " 'chr1_61606433_61606882': 'hub_8_NFIA',\n",
       " 'chr1_61607356_61607576': 'hub_8_NFIA',\n",
       " 'chr1_61626278_61626549': 'hub_8_NFIA',\n",
       " 'chr1_61636340_61636562': 'hub_8_NFIA',\n",
       " 'chr1_61647119_61647431': 'hub_8_NFIA',\n",
       " 'chr1_61676459_61676612': 'hub_8_NFIA',\n",
       " 'chr1_61699925_61700539': 'hub_8_NFIA',\n",
       " 'chr1_61709731_61710194': 'hub_8_NFIA',\n",
       " 'chr1_61713859_61714124': 'hub_8_NFIA',\n",
       " 'chr1_61719057_61719381': 'hub_8_NFIA',\n",
       " 'chr1_61767798_61768337': 'hub_8_NFIA',\n",
       " 'chr1_61783901_61784291': 'hub_8_NFIA',\n",
       " 'chr1_61784548_61784787': 'hub_8_NFIA',\n",
       " 'chr1_61802901_61803514': 'hub_8_NFIA',\n",
       " 'chr1_61824491_61824773': 'hub_8_NFIA',\n",
       " 'chr1_61869087_61869346': 'hub_8_NFIA',\n",
       " 'chr1_61869507_61869926': 'hub_8_NFIA',\n",
       " 'chr1_61884922_61885832': 'hub_8_NFIA',\n",
       " 'chr1_61886036_61886188': 'hub_8_NFIA',\n",
       " 'chr1_61899949_61900169': 'hub_8_NFIA',\n",
       " 'chr1_61918502_61919422': 'hub_8_NFIA',\n",
       " 'chr1_61970560_61971083': 'hub_8_NFIA',\n",
       " 'chr1_66231343_66231718': 'hub_9_PDE4B',\n",
       " 'chr1_66257045_66257429': 'hub_9_PDE4B',\n",
       " 'chr1_66258002_66259296': 'hub_9_PDE4B',\n",
       " 'chr1_66277625_66278170': 'hub_9_PDE4B',\n",
       " 'chr1_66371019_66371255': 'hub_9_PDE4B',\n",
       " 'chr1_66477959_66478388': 'hub_9_PDE4B',\n",
       " 'chr1_66479199_66479469': 'hub_9_PDE4B',\n",
       " 'chr1_66501557_66501859': 'hub_9_PDE4B',\n",
       " 'chr1_66556579_66557171': 'hub_9_PDE4B',\n",
       " 'chr1_66602759_66603387': 'hub_9_PDE4B',\n",
       " 'chr1_66609141_66609729': 'hub_9_PDE4B',\n",
       " 'chr1_66689032_66689182': 'hub_9_PDE4B',\n",
       " 'chr1_66708371_66708685': 'hub_9_PDE4B',\n",
       " 'chr1_66709506_66709787': 'hub_9_PDE4B',\n",
       " 'chr1_66717229_66717761': 'hub_9_PDE4B',\n",
       " 'chr1_66718974_66719297': 'hub_9_PDE4B',\n",
       " 'chr1_66727105_66727351': 'hub_9_PDE4B',\n",
       " 'chr1_66736204_66736365': 'hub_9_PDE4B',\n",
       " 'chr1_66741799_66742261': 'hub_9_PDE4B',\n",
       " 'chr1_66748043_66748300': 'hub_9_PDE4B',\n",
       " 'chr1_66749311_66749815': 'hub_9_PDE4B',\n",
       " 'chr1_66760511_66760711': 'hub_9_PDE4B',\n",
       " 'chr1_66762100_66762250': 'hub_9_PDE4B',\n",
       " 'chr1_66766746_66767243': 'hub_9_PDE4B',\n",
       " 'chr1_66768440_66768992': 'hub_9_PDE4B',\n",
       " 'chr1_66781567_66781722': 'hub_9_PDE4B',\n",
       " 'chr1_66801510_66802306': 'hub_9_PDE4B',\n",
       " 'chr1_66815616_66816302': 'hub_9_PDE4B',\n",
       " 'chr1_66817670_66817871': 'hub_9_PDE4B',\n",
       " 'chr1_66823243_66823496': 'hub_9_PDE4B',\n",
       " 'chr1_66825753_66826591': 'hub_9_PDE4B',\n",
       " 'chr1_66832400_66832803': 'hub_9_PDE4B',\n",
       " 'chr1_66833220_66833545': 'hub_9_PDE4B',\n",
       " 'chr1_66838432_66838582': 'hub_9_PDE4B',\n",
       " 'chr1_66840289_66840843': 'hub_9_PDE4B',\n",
       " 'chr1_66850045_66850239': 'hub_9_PDE4B',\n",
       " 'chr1_66859395_66859889': 'hub_9_PDE4B',\n",
       " 'chr1_66869100_66869286': 'hub_9_PDE4B',\n",
       " 'chr1_66870732_66871211': 'hub_9_PDE4B',\n",
       " 'chr1_66878751_66879424': 'hub_9_PDE4B',\n",
       " 'chr1_90058294_90058664': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90064633_90065053': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90073701_90074114': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90085999_90086149': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90135169_90135326': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90158020_90159087': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90167293_90167763': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90171792_90172028': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90184524_90185544': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90190035_90191000': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90204851_90205029': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90220122_90220337': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90228358_90229234': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90246239_90246863': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90246989_90247608': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90264939_90265532': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90273201_90273412': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90278409_90278857': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90281064_90281561': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90289162_90289756': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90297100_90297323': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90299476_90299686': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90312895_90313112': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90321202_90321682': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90331138_90331580': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90339322_90339753': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90343401_90343551': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90344504_90345015': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90359729_90360064': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90361745_90362144': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90372252_90372588': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90373287_90373610': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90406556_90407175': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90408290_90408813': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90418500_90419188': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90427676_90428287': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90432999_90433536': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90438793_90439485': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90440601_90440937': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_90441142_90441436': 'hub_10_RP11-302M6.4',\n",
       " 'chr1_108067837_108068031': 'hub_11_VAV3',\n",
       " 'chr1_108077818_108078033': 'hub_11_VAV3',\n",
       " 'chr1_108091872_108092358': 'hub_11_VAV3',\n",
       " 'chr1_108125943_108126232': 'hub_11_VAV3',\n",
       " 'chr1_108191481_108192073': 'hub_11_VAV3',\n",
       " 'chr1_108215566_108216247': 'hub_11_VAV3',\n",
       " 'chr1_108281767_108282161': 'hub_11_VAV3',\n",
       " 'chr1_108297678_108298255': 'hub_11_VAV3',\n",
       " 'chr1_108328130_108328436': 'hub_11_VAV3',\n",
       " 'chr1_108328624_108329044': 'hub_11_VAV3',\n",
       " 'chr1_108329765_108330175': 'hub_11_VAV3',\n",
       " 'chr1_108345343_108345657': 'hub_11_VAV3',\n",
       " 'chr1_108346312_108346546': 'hub_11_VAV3',\n",
       " 'chr1_108346779_108347375': 'hub_11_VAV3',\n",
       " 'chr1_108347460_108347610': 'hub_11_VAV3',\n",
       " 'chr1_108352948_108353159': 'hub_11_VAV3',\n",
       " 'chr1_108363461_108363806': 'hub_11_VAV3',\n",
       " 'chr1_108375752_108375971': 'hub_11_VAV3',\n",
       " 'chr1_108377323_108377554': 'hub_11_VAV3',\n",
       " 'chr1_108380733_108380997': 'hub_11_VAV3',\n",
       " 'chr1_108383366_108383860': 'hub_11_VAV3',\n",
       " 'chr1_108387105_108387755': 'hub_11_VAV3',\n",
       " 'chr1_108404331_108404571': 'hub_11_VAV3',\n",
       " 'chr1_108411636_108412081': 'hub_11_VAV3',\n",
       " 'chr1_108426293_108426520': 'hub_11_VAV3',\n",
       " 'chr1_108439511_108440821': 'hub_11_VAV3',\n",
       " 'chr1_108442791_108443257': 'hub_11_VAV3',\n",
       " 'chr1_108455396_108455688': 'hub_11_VAV3',\n",
       " 'chr1_108456767_108457303': 'hub_11_VAV3',\n",
       " 'chr1_108462486_108463095': 'hub_11_VAV3',\n",
       " 'chr1_108463831_108463981': 'hub_11_VAV3',\n",
       " 'chr1_108469143_108469682': 'hub_11_VAV3',\n",
       " 'chr1_108470511_108471002': 'hub_11_VAV3',\n",
       " 'chr1_108479066_108480188': 'hub_11_VAV3',\n",
       " 'chr1_108483713_108484438': 'hub_11_VAV3',\n",
       " 'chr1_108493157_108493572': 'hub_11_VAV3',\n",
       " 'chr1_108495763_108496025': 'hub_11_VAV3',\n",
       " 'chr1_108506189_108506413': 'hub_11_VAV3',\n",
       " 'chr1_108506417_108507061': 'hub_11_VAV3',\n",
       " 'chr1_108549655_108549959': 'hub_11_VAV3',\n",
       " 'chr1_208145636_208146172': 'hub_12_PLXNA2',\n",
       " 'chr1_208170726_208171790': 'hub_12_PLXNA2',\n",
       " 'chr1_208195035_208195563': 'hub_12_PLXNA2',\n",
       " 'chr1_208201955_208202179': 'hub_12_PLXNA2',\n",
       " 'chr1_208210320_208210680': 'hub_12_PLXNA2',\n",
       " 'chr1_208218274_208218448': 'hub_12_PLXNA2',\n",
       " 'chr1_208222132_208222680': 'hub_12_PLXNA2',\n",
       " 'chr1_208239027_208239751': 'hub_12_PLXNA2',\n",
       " 'chr1_208251299_208251497': 'hub_12_PLXNA2',\n",
       " 'chr1_208259737_208260414': 'hub_12_PLXNA2',\n",
       " 'chr1_208267153_208267580': 'hub_12_PLXNA2',\n",
       " 'chr1_208267761_208267985': 'hub_12_PLXNA2',\n",
       " 'chr1_208268484_208268899': 'hub_12_PLXNA2',\n",
       " 'chr1_208296044_208296277': 'hub_12_PLXNA2',\n",
       " 'chr1_208299335_208299830': 'hub_12_PLXNA2',\n",
       " 'chr1_208305410_208306106': 'hub_12_PLXNA2',\n",
       " 'chr1_208312558_208312962': 'hub_12_PLXNA2',\n",
       " 'chr1_208325409_208325863': 'hub_12_PLXNA2',\n",
       " 'chr1_208327346_208327722': 'hub_12_PLXNA2',\n",
       " 'chr1_208332629_208333060': 'hub_12_PLXNA2',\n",
       " 'chr1_208337973_208338444': 'hub_12_PLXNA2',\n",
       " 'chr1_208338559_208338790': 'hub_12_PLXNA2',\n",
       " 'chr1_208344999_208345590': 'hub_12_PLXNA2',\n",
       " 'chr1_208346183_208346488': 'hub_12_PLXNA2',\n",
       " 'chr1_208357484_208357987': 'hub_12_PLXNA2',\n",
       " 'chr1_208374565_208374811': 'hub_12_PLXNA2',\n",
       " 'chr1_208377798_208377948': 'hub_12_PLXNA2',\n",
       " 'chr1_208378062_208378466': 'hub_12_PLXNA2',\n",
       " 'chr1_208380656_208381078': 'hub_12_PLXNA2',\n",
       " 'chr1_208385348_208385778': 'hub_12_PLXNA2',\n",
       " 'chr1_208386911_208387620': 'hub_12_PLXNA2',\n",
       " 'chr1_208392973_208393298': 'hub_12_PLXNA2',\n",
       " 'chr1_208393353_208393678': 'hub_12_PLXNA2',\n",
       " 'chr1_208396528_208396800': 'hub_12_PLXNA2',\n",
       " 'chr1_208408089_208408484': 'hub_12_PLXNA2',\n",
       " 'chr1_208409178_208410134': 'hub_12_PLXNA2',\n",
       " 'chr1_208412472_208412673': 'hub_12_PLXNA2',\n",
       " 'chr1_208417546_208418205': 'hub_12_PLXNA2',\n",
       " 'chr1_208418538_208418962': 'hub_12_PLXNA2',\n",
       " 'chr1_208454235_208454658': 'hub_12_PLXNA2',\n",
       " 'chr1_239524766_239525149': 'hub_13_CHRM3',\n",
       " 'chr1_239527292_239527540': 'hub_13_CHRM3',\n",
       " 'chr1_239543417_239543837': 'hub_13_CHRM3',\n",
       " 'chr1_239555698_239556004': 'hub_13_CHRM3',\n",
       " 'chr1_239577374_239577524': 'hub_13_CHRM3',\n",
       " 'chr1_239580414_239580887': 'hub_13_CHRM3',\n",
       " 'chr1_239582032_239582330': 'hub_13_CHRM3',\n",
       " 'chr1_239590455_239590749': 'hub_13_CHRM3',\n",
       " 'chr1_239601093_239601456': 'hub_13_CHRM3',\n",
       " 'chr1_239621210_239622004': 'hub_13_CHRM3',\n",
       " 'chr1_239635385_239635923': 'hub_13_CHRM3',\n",
       " 'chr1_239646563_239647047': 'hub_13_CHRM3',\n",
       " 'chr1_239685286_239685565': 'hub_13_CHRM3',\n",
       " 'chr1_239701164_239701362': 'hub_13_CHRM3',\n",
       " 'chr1_239722158_239722696': 'hub_13_CHRM3',\n",
       " 'chr1_239727062_239727339': 'hub_13_CHRM3',\n",
       " 'chr1_239750403_239750821': 'hub_13_CHRM3',\n",
       " 'chr1_239756812_239757095': 'hub_13_CHRM3',\n",
       " 'chr1_239779997_239780442': 'hub_13_CHRM3',\n",
       " 'chr1_239826439_239826590': 'hub_13_CHRM3',\n",
       " 'chr1_239839887_239840106': 'hub_13_CHRM3',\n",
       " 'chr1_239840834_239841101': 'hub_13_CHRM3',\n",
       " 'chr1_239882085_239883607': 'hub_13_CHRM3',\n",
       " 'chr1_239893809_239893977': 'hub_13_CHRM3',\n",
       " 'chr1_239898423_239898632': 'hub_13_CHRM3',\n",
       " 'chr1_239906028_239906227': 'hub_13_CHRM3',\n",
       " 'chr1_239934862_239935176': 'hub_13_CHRM3',\n",
       " 'chr1_239958585_239958960': 'hub_13_CHRM3',\n",
       " 'chr1_239967042_239967416': 'hub_13_CHRM3',\n",
       " 'chr1_239968224_239968707': 'hub_13_CHRM3',\n",
       " 'chr1_239969106_239969456': 'hub_13_CHRM3',\n",
       " 'chr1_239977493_239978200': 'hub_13_CHRM3',\n",
       " 'chr1_240006743_240007349': 'hub_13_CHRM3',\n",
       " 'chr1_240021397_240021637': 'hub_13_CHRM3',\n",
       " 'chr1_240025278_240025673': 'hub_13_CHRM3',\n",
       " 'chr1_240054921_240055236': 'hub_13_CHRM3',\n",
       " 'chr1_240056538_240056966': 'hub_13_CHRM3',\n",
       " 'chr1_240061301_240061768': 'hub_13_CHRM3',\n",
       " 'chr1_240073246_240073647': 'hub_13_CHRM3',\n",
       " 'chr1_240114915_240115146': 'hub_13_CHRM3'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_hub_dict\n",
    "# hub_peak_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8760dc4b-42a5-45c3-8459-86a21a458ab3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********start generate cell embedding...**********\n",
      "simulation type is continuous\n",
      "**********generate cell embedding finished**********\n",
      "**********start generate counts...**********\n",
      "correct_iter 1\n",
      "correct_iter 2\n",
      "**********start ZIP correction...**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/lichen/anaconda3/envs/pytorch2/lib/python3.8/site-packages/scipy/optimize/minpack.py:162: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********ZIP correction finished!**********\n",
      "**********generate counts finshed!**********\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"**********start generate cell embedding...**********\")\n",
    "print(\"simulation type is {0}\".format(simu_type))\n",
    "if simu_type=='discrete':\n",
    "    # 重复两次获得两个矩阵，后续使用参数two_embeds决定是用两个矩阵还是用一个\n",
    "    embeds_peak,meta=Get_Discrete_Embedding(pops_name[0],min_popsize,tree_text[0],\n",
    "                 n_cell_total,pops_size,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed)\n",
    "    embeds_lib,meta=Get_Discrete_Embedding(pops_name[0],min_popsize,tree_text[0],\n",
    "                 n_cell_total,pops_size,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed+1)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished**********\")\n",
    "    # 获得count\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero,\n",
    "                                real_param)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='continuous':\n",
    "    embeds_param={}\n",
    "    embeds_peak,meta=Get_Continuous_Embedding(tree_text[4],n_cell_total,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed)\n",
    "    embeds_lib,meta=Get_Continuous_Embedding(tree_text[4],n_cell_total,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed+1)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished**********\")\n",
    "    \n",
    "    print(\"**********start generate counts...**********\")\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero,\n",
    "                               real_param)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='single':\n",
    "    embeds_param={}\n",
    "    embeds_peak,meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "    embeds_lib,meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "    embeds_peak,embeds_lib=embeds_peak.values,embeds_lib.values\n",
    "    print(\"**********generate cell embedding finished!**********\")\n",
    "    \n",
    "    \n",
    "    print(\"**********start generate counts...**********\")\n",
    "    atac_counts=Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution,activation,bw_pm,bw_lib,bw_nozero)\n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "    \n",
    "elif simu_type=='cell_type':\n",
    "    adata=sc.read_h5ad(adata_dir)\n",
    "    counts_list,celltype_list,embed_peak_list,embed_lib_list=[],[],[],[]\n",
    "    lambdas_list,simu_param_nozero_list,simu_param_lib_list,simu_param_pm_list=[],[],[],[]#新加的list用来重新对lambdas进行spasity的修正\n",
    "    celltypes=np.unique(adata.obs.celltype)\n",
    "    for i in range(len(celltypes)):\n",
    "    # 可以分为直接从真实数据中进行采样或是从核密度估计中采样特定细胞数目，先做直接从真实数据中采样的结果\n",
    "        # print(celltypes[i])\n",
    "        print(\"simulating cell type: {}...\".format(celltypes[i]))\n",
    "        adata_part=adata[adata.obs.celltype==celltypes[i],:]\n",
    "\n",
    "        # 对每个celltype单独进行仿真\n",
    "        counts,embed_peak,embed_lib,lambdas,simu_param_nozero,simu_param_lib,simu_param_pm=Get_Celltype_Counts(adata_part,two_embeds,\n",
    "                                            embed_mean_same,embed_sd_same,\n",
    "                     n_embed_diff,n_embed_same,correct_iter,lib_simu=lib_simu,n_cell_total=None,\n",
    "                                        distribution=distribution,activation=activation,\n",
    "                    bw_pm=bw_pm,bw_lib=bw_lib,bw_nozero=bw_nozero,rand_seed=rand_seed) # peak*cell\n",
    "\n",
    "        counts_list.append(counts)\n",
    "        embed_peak_list.append(embed_peak)\n",
    "        embed_lib_list.append(embed_lib)\n",
    "        celltype_list.append([celltypes[i]]*counts.shape[1])\n",
    "        lambdas_list.append(lambdas)\n",
    "        simu_param_nozero_list.append(simu_param_nozero)\n",
    "        simu_param_lib_list.append(simu_param_lib)\n",
    "        simu_param_pm_list.append(simu_param_pm)\n",
    "        \n",
    "    if distribution=='Poisson':\n",
    "        # atac_counts=np.hstack(counts_list)\n",
    "        meta=np.hstack(celltype_list)\n",
    "        embeds_peak=np.hstack(embed_peak_list)\n",
    "        embeds_lib=np.hstack(embed_lib_list)\n",
    "        #对整体lambdas进行sparsity修正\n",
    "        lambdas=np.hstack(lambdas_list)\n",
    "        simu_param_nozero=np.hstack(simu_param_nozero_list)\n",
    "        simu_param_lib=np.hstack(simu_param_lib_list)\n",
    "        simu_param_pm=peak_mean\n",
    "\n",
    "        lambdas_sum=np.sum(lambdas,axis=0)\n",
    "\n",
    "#         print(\"**********start ZIP correction...**********\")\n",
    "#         k_list,pi_list=[],[]\n",
    "#         # 求解每个cell中lambda扩大的倍数和置零的比例\n",
    "#         for i in range(n_cell_total):\n",
    "#             iter_=i\n",
    "#             # print(i)\n",
    "#             def solve_function(unsolved_value):\n",
    "#                 k,pi=unsolved_value[0],unsolved_value[1]\n",
    "#                 return [\n",
    "#                     k*(1-pi)-simu_param_lib[iter_]/(lambdas_sum[iter_]),\n",
    "#                     n_peak*pi+(1-pi)*np.sum(np.exp(-lambdas[:,iter_]*k))-(n_peak-simu_param_nozero[iter_])\n",
    "#                 ]\n",
    "\n",
    "#             solved=fsolve(solve_function,[3,0.5],maxfev=2000)\n",
    "#             k,pi=solved[0],solved[1]\n",
    "#             simu1=k*(1-pi)*(lambdas_sum[iter_])\n",
    "#             real1=simu_param_lib[iter_]\n",
    "#             if abs(simu1-real1)/real1>0.1:\n",
    "#                 # print(i)\n",
    "#                 # print(simu1,real1)\n",
    "#                 # print('=================================')\n",
    "#                 solved=fsolve(solve_function,[20,0.5],maxfev=2000)\n",
    "#             k_list.append(solved[0])\n",
    "#             pi_list.append(solved[1])\n",
    "#         # 对每个cell的lambda置零并扩大相应倍数\n",
    "#         for i in range(n_cell_total):\n",
    "#             if k_list[i]==3 or k_list[i]==20 or pi_list[i]<0:\n",
    "#                 continue\n",
    "#             a=lambdas[:,i]*k_list[i]\n",
    "#             # b=atac_counts[:,i]\n",
    "#             a[np.random.choice(n_peak,replace=False,size=int(pi_list[i]*n_peak))]=0\n",
    "#             lambdas[:,i]=a\n",
    "#         print(\"**********ZIP correction finished!**********\")\n",
    "            \n",
    "        print(\"**********start ZIP correction...**********\")\n",
    "        batch_size = 1000 # 并行数目，全局字典\n",
    "        global k_dict,pi_dict\n",
    "        for i in range(0,n_cell_total,batch_size):\n",
    "            if i+batch_size<=n_cell_total:\n",
    "                my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, i+batch_size)]\n",
    "            else:\n",
    "                my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, n_cell_total)]\n",
    "            for thread_ in my_thread:\n",
    "                thread_.start()\n",
    "            for thread_ in my_thread:\n",
    "                thread_.join()\n",
    "        # 对每个cell的lambda置零并扩大相应倍数\n",
    "        for i in range(n_cell_total):\n",
    "            if k_dict[i]==3 or k_dict[i]==20 or pi_dict[i]<0 or k_dict[i]<0:\n",
    "                continue\n",
    "            a=lambdas[:,i]*k_dict[i]\n",
    "            # b=atac_counts[:,i]\n",
    "            a[np.random.choice(n_peak,replace=False,size=int(pi_dict[i]*n_peak))]=0\n",
    "            lambdas[:,i]=a\n",
    "            \n",
    "        print(\"**********ZIP correction finished!**********\")\n",
    "\n",
    "        # # spasity矫正完之后再来一轮peak mean和library size的矫正，保证都符合实际\n",
    "        # lambdas_copy=lambdas.copy()\n",
    "        # lambdas_copy=lambdas_copy/(np.sum(lambdas_copy,axis=1).reshape(-1,1)+1e-8)*(simu_param_pm.reshape(-1,1))*lambdas_copy.shape[1]\n",
    "        # lambdas_copy=lambdas_copy/(np.sum(lambdas_copy,axis=0).reshape(1,-1)+1e-8)*(simu_param_lib.reshape(1,-1))\n",
    "\n",
    "        atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "        \n",
    "    elif distribution=='Bernoulli':\n",
    "        atac_counts=np.hstack(counts_list)\n",
    "        meta=np.hstack(celltype_list)\n",
    "        embeds_peak=np.hstack(embed_peak_list)\n",
    "        embeds_lib=np.hstack(embed_lib_list)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('wrong distribution input!')\n",
    "    \n",
    "    print(\"**********generate counts finshed!**********\")\n",
    "\n",
    "else:\n",
    "    raise ValueError('wrong simulation type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ccd22d7f-43ee-43d7-96f2-a4e650e944db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1356791/2702350057.py:3: FutureWarning: X.dtype being converted to np.float32 from int64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  adata_chr_hub=anndata.AnnData(X=atac_counts.T)\n"
     ]
    }
   ],
   "source": [
    "# 将结果保存为anndata，方便cicero进行处理\n",
    "import anndata\n",
    "adata_chr_hub=anndata.AnnData(X=atac_counts.T)\n",
    "adata_chr_hub.obs['celltype']=meta\n",
    "adata_chr_hub.var=adata.var\n",
    "\n",
    "adata_chr_hub.write(os.path.join(save_dir,\"adata_chr_hub_test6.h5ad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca87da-9ab0-4b59-9e9c-fb85c2e44459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_chr_hub_test1: 使用相同的vector，方差0.1生成的结果\n",
    "# adata_chr_hub_test2: 使用相同的vector，方差0.01生成的结果\n",
    "# adata_chr_hub_test3: 使用相同的vector，方差0.01生成的结果;continuous\n",
    "# adata_chr_hub_test4: 使用相同的vector，方差0.01生成的结果;single\n",
    "# adata_chr_hub_test3_version2: 使用相同的vector，方差0.01生成的结果;continuous 重新根据长度筛选hub\n",
    "# adata_chr_hub_test5: 使用相同的vector，方差0.01生成的结果;continuous 不置零 并且修正了exp_linear\n",
    "# adata_chr_hub_test6: 使用相同的vector，方差0.01生成的结果;continuous 不置零 并且修正了exp_linear vector全用KAZN的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ffe99-274e-49b4-b22b-f42af87573dc",
   "metadata": {},
   "source": [
    "## 查看chromatin hub的表达是不是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68e22de5-95f9-40db-af83-5984681f590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1500 × 169221\n",
       "    obs: 'celltype'\n",
       "    var: 'n_cells', 'commonness'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_chr_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd51fabd-23e3-4350-9674-57c2140eae45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.25656993, 0.17457549, ..., 0.23292369, 0.15904043,\n",
       "        0.12197703],\n",
       "       [0.25656993, 1.        , 0.19781304, ..., 0.2771729 , 0.24634556,\n",
       "        0.13128214],\n",
       "       [0.17457549, 0.19781304, 1.        , ..., 0.08247865, 0.34071505,\n",
       "        0.06374816],\n",
       "       ...,\n",
       "       [0.23292369, 0.2771729 , 0.08247865, ..., 1.        , 0.17662384,\n",
       "        0.13140351],\n",
       "       [0.15904043, 0.24634556, 0.34071505, ..., 0.17662384, 1.        ,\n",
       "        0.15750633],\n",
       "       [0.12197703, 0.13128214, 0.06374816, ..., 0.13140351, 0.15750633,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_list=hub_peak_dict['hub_6_RNF220']\n",
    "hub_mtx=adata_chr_hub[:,peak_list].X\n",
    "\n",
    "# print(list(hub_mtx[:,0]))\n",
    "hub_corr=np.corrcoef(hub_mtx.T)\n",
    "hub_corr.shape\n",
    "\n",
    "hub_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6014e33e-3d0a-49a7-85a9-3b579486faeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00252437, 0.1025821 , ..., 0.10094914, 0.09973047,\n",
       "        0.14456209],\n",
       "       [0.00252437, 1.        , 0.12832482, ..., 0.0210712 , 0.07977713,\n",
       "        0.0649617 ],\n",
       "       [0.1025821 , 0.12832482, 1.        , ..., 0.02873921, 0.0809677 ,\n",
       "        0.13608194],\n",
       "       ...,\n",
       "       [0.10094914, 0.0210712 , 0.02873921, ..., 1.        , 0.03380439,\n",
       "        0.09612193],\n",
       "       [0.09973047, 0.07977713, 0.0809677 , ..., 0.03380439, 1.        ,\n",
       "        0.05427607],\n",
       "       [0.14456209, 0.0649617 , 0.13608194, ..., 0.09612193, 0.05427607,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_list=hub_peak_dict['hub_3_KAZN']\n",
    "hub_mtx=adata_chr_hub[:,peak_list].X\n",
    "\n",
    "# print(list(hub_mtx[:,0]))\n",
    "hub_corr=np.corrcoef(hub_mtx.T)\n",
    "hub_corr.shape\n",
    "\n",
    "hub_corr\n",
    "\n",
    "hub_4_MACF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8bdf07ac-7764-4ddc-a6e3-0488f16716f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.00485843,  0.27847454, ..., -0.00417491,\n",
       "        -0.0046541 ,  0.12349161],\n",
       "       [-0.00485843,  1.        ,  0.14271005, ..., -0.00423926,\n",
       "        -0.00472583, -0.0059545 ],\n",
       "       [ 0.27847454,  0.14271005,  1.        , ..., -0.00405397,\n",
       "         0.64729891, -0.00569425],\n",
       "       ...,\n",
       "       [-0.00417491, -0.00423926, -0.00405397, ...,  1.        ,\n",
       "        -0.00406097, -0.00511679],\n",
       "       [-0.0046541 , -0.00472583,  0.64729891, ..., -0.00406097,\n",
       "         1.        , -0.00570407],\n",
       "       [ 0.12349161, -0.0059545 , -0.00569425, ..., -0.00511679,\n",
       "        -0.00570407,  1.        ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_list=hub_peak_dict['hub_4_MACF1']\n",
    "hub_mtx=adata_chr_hub[:,peak_list].X\n",
    "\n",
    "# print(list(hub_mtx[:,0]))\n",
    "hub_corr=np.corrcoef(hub_mtx.T)\n",
    "hub_corr.shape\n",
    "\n",
    "hub_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bdac86cf-ecf3-4280-a82c-ce975b407edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.65057585, -1.45511417,  0.32076159,  0.81134963, -0.2408046 ,\n",
       "         0.16512108, -0.03350057,  0.08786458,  1.03414653, -1.06082656,\n",
       "        -1.01358446, -0.42023219]),\n",
       " array([ 0.75979188,  1.82743214, -0.66072709, -0.80780626,  0.88780012,\n",
       "        -0.21744745, -0.93952452,  0.59953832,  2.2231127 ,  1.00000546,\n",
       "         1.14967454, -0.15557629]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_vector_dict['hub_4_MACF1'],hub_vector_dict['hub_3_KAZN'],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce17f2a-286f-4437-968f-3ba9e900cf10",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5a241-af8b-4de9-b365-748df4f0bf96",
   "metadata": {},
   "source": [
    "## utils\n",
    "一些功能性函数<br>\n",
    "- cal_xxx计算数据的peak mean、library size、nozero等参数，输入均为anndata形式\n",
    "- Activation: 对矫正之前的参数矩阵使用的激活方式，建议在cell type仿真时使用sigmod，连续或离散<br>仿真时使用exp，可能在Bernoulli分布下需要使用sigmod（未尝试）\n",
    "- create_logger: 用来记录信息\n",
    "- Bernoulli_correction：仿真binary数据时使用的对参数矩阵进行矫正的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfe92dd-9f3a-47c0-a227-d413c0a26a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library size\n",
    "def cal_lib(adata):\n",
    "    return np.array(np.sum(adata.X,axis=1)).ravel()\n",
    "\n",
    "def cal_pm(adata): # peak mean\n",
    "    return np.array(np.mean(adata.X,axis=0)).ravel()\n",
    "\n",
    "def cal_pl(adata):# peak length\n",
    "    start=np.array([int(i.split('_')[1]) for i in adata.var.index])\n",
    "    end=np.array([int(i.split('_')[2]) for i in adata.var.index])\n",
    "    return (end-start).ravel()\n",
    "\n",
    "def cal_spa(adata):# sparsity\n",
    "    X=adata.X.copy()\n",
    "    X[X>0]=1\n",
    "    sparsity=np.sum(X,axis=1)/X.shape[1]\n",
    "    return np.array(sparsity).ravel()\n",
    "\n",
    "def cal_nozero(adata):# sparsity\n",
    "    X=adata.X.copy()\n",
    "    X[X>0]=1\n",
    "    sparsity=np.sum(X,axis=1)\n",
    "    return np.array(sparsity).ravel()\n",
    "\n",
    "def cal_peak_count(adata):\n",
    "    return np.array(np.sum(adata.X,axis=0)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0482f83f-15e5-4662-9c02-1d7d02207eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Activation(X,method='sigmod'):# 对peak_effect*cell_embedding 的矩阵进行激活操作，防止其值为0\n",
    "    if method=='sigmod':\n",
    "        return 1/(1+np.exp(-1*X))\n",
    "    elif method=='exp':\n",
    "        return np.exp(X)\n",
    "    elif method=='exp_linear':\n",
    "        exp_num=4\n",
    "        k=np.exp(exp_num)\n",
    "        # k=1\n",
    "        X_act=X.copy()\n",
    "        X_act[X_act>=exp_num]=k*X_act[X_act>=exp_num]+np.exp(exp_num)-exp_num*np.exp(exp_num)\n",
    "        X_act[X_act<exp_num]=np.exp(X_act[X_act<exp_num])\n",
    "        return X_act\n",
    "    elif method=='exp_sym':\n",
    "        X_act=X\n",
    "        exp_level=1.5\n",
    "        X_act[X_act<=0]=np.power(exp_level,X_act[X_act<=0])\n",
    "        X_act[X_act>0]=2-np.power(exp_level,-X_act[X_act>0])\n",
    "        \n",
    "        return X_act\n",
    "    else:\n",
    "        raise ValueError('wrong activation method!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811c4535-0377-413f-ad9c-3d3ef79b8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(name='', ch=True, fh='', levelname=logging.INFO):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(levelname)\n",
    "    \n",
    "#     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "    if ch:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "    if fh:\n",
    "        fh = logging.FileHandler(fh, mode='w')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b7e0dd-af1a-4054-b92c-faa3b3387e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    \"\"\"\n",
    "    Seed all necessary random number generators.\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = random.randint(1, 10000)\n",
    "    # torch.set_num_threads(1)  # Suggested for issues with deadlocks, etc.\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0f6648-915c-4373-aad5-08f17e359f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新之后快版本的bernoulli\n",
    "\n",
    "def Bernoulli_pm_correction(X_peak,param_pm): # X_peak:等待矫正的矩阵  param_pm:对应的采样得到的peak_mean\n",
    "    # peak mean correction\n",
    "    peak_p_list=[]\n",
    "    for i in range(0,X_peak.shape[0]):\n",
    "        peak_p=X_peak[i,:] # 单个peak对应的所有cell的值\n",
    "        peak_mean=np.mean(peak_p) # 当前矩阵的peak mean\n",
    "        peak_mean_ex=np.exp(param_pm[i])-1 # 期望的peak mean\n",
    "        \n",
    "        # 若期望的peak_mean都是0\n",
    "        if peak_mean_ex==0 or peak_mean==0:\n",
    "            peak_p_list.append(peak_p*peak_mean_ex)\n",
    "            continue\n",
    "        \n",
    "        if np.max(peak_p)/peak_mean*peak_mean_ex>1:\n",
    "            peak_p_sort=np.sort(peak_p)\n",
    "            idx=len(peak_p_sort)-1\n",
    "            while(1):\n",
    "                weight=(len(peak_p)*peak_mean_ex+idx-len(peak_p))/(np.sum(peak_p_sort[0:idx])+1e-8)\n",
    "                if peak_p_sort[idx-1]*weight<=1:\n",
    "                    # print(idx)\n",
    "                    break\n",
    "\n",
    "                for idx_2 in range(idx,-1,-1):  # 找到*weight<1 的idx\n",
    "                    if peak_p_sort[idx_2-1]*weight<=1:\n",
    "                        # print(idx_2)\n",
    "                        break\n",
    "                    # 如果实在没有idx能够使得值*weight<1,此时就会一直循环，需要及时跳出循环\n",
    "                    if idx_2<=1:\n",
    "                        break\n",
    "                idx=idx_2\n",
    "                if idx_2<=1:\n",
    "                    weight=(len(peak_p)*peak_mean_ex+idx-len(peak_p))/(np.sum(peak_p_sort[0:idx])+1e-8)\n",
    "                    break\n",
    "            peak_p=peak_p*weight\n",
    "            peak_p[peak_p>1]=1\n",
    "        else:\n",
    "            peak_p=peak_p/peak_mean*peak_mean_ex\n",
    "        peak_p_list.append(peak_p)\n",
    "    peak_p_matrix=np.vstack(peak_p_list)\n",
    "    return peak_p_matrix\n",
    "\n",
    "def Bernoulli_lib_correction(X_peak,param_lib):\n",
    "    peak_p_list=[]\n",
    "    for i in range(X_peak.shape[1]):\n",
    "        peak_p=X_peak[:,i]\n",
    "        lib_size=np.sum(peak_p)\n",
    "        lib_size_ex=np.exp(param_lib[i])-1\n",
    "        \n",
    "        # 若期望的library_size都是0\n",
    "        if lib_size_ex==0 or lib_size==0:\n",
    "            peak_p_list.append((peak_p*lib_size_ex).reshape(-1,1))\n",
    "            continue\n",
    "\n",
    "        if np.max(peak_p)/lib_size*lib_size_ex>1:\n",
    "            peak_p_sort=np.sort(peak_p)\n",
    "            idx=len(peak_p_sort)-1\n",
    "            while(1):\n",
    "                weight=(lib_size_ex+idx-len(peak_p))/(np.sum(peak_p_sort[0:idx])+1e-8)\n",
    "                if peak_p_sort[idx-1]*weight<=1:\n",
    "                    break\n",
    "                for idx_2 in range(idx,-1,-1):\n",
    "                    if peak_p_sort[idx_2-1]*weight<=1:\n",
    "                        # print(idx_2)\n",
    "                        break\n",
    "                    # 如果实在没有idx能够使得值*weight<1,此时就会一直循环，需要及时跳出循环\n",
    "                    if idx_2<=1:\n",
    "                        break\n",
    "                idx=idx_2\n",
    "                if idx_2<=1:\n",
    "                    weight=(len(peak_p)*peak_mean_ex+idx-len(peak_p))/(np.sum(peak_p_sort[0:idx])+1e-8)\n",
    "                    break\n",
    "            # 防止出现<1的部分全都是0\n",
    "            if np.sum(peak_p_sort[0:idx])==0:\n",
    "                peak_p[peak_p>1]=1\n",
    "            else:\n",
    "                peak_p=peak_p*weight\n",
    "                peak_p[peak_p>1]=1\n",
    "        else:\n",
    "            peak_p=peak_p/lib_size*lib_size_ex\n",
    "        peak_p_list.append(peak_p.reshape(-1,1))\n",
    "    peak_p_matrix=np.hstack(peak_p_list)\n",
    "    return peak_p_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ae906-7713-4b58-95ae-11f43f45a020",
   "metadata": {},
   "source": [
    "## Get Effect\n",
    "用来获取peak的effect vector: peak num * effect dim <br>\n",
    "和library size的effect vector：cell num * effect dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "385832d7-64d4-44c7-9754-3eaee0ee736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Effect(n_peak,n_cell_total,len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd):\n",
    "    # 生成peak effect和library size effect\n",
    "    # np.random.seed(rand_seed)\n",
    "    peak_effect=np.random.normal(effect_mean,effect_sd,(n_peak,len_cell_embed))\n",
    "    lib_size_effect=np.random.normal(effect_mean,effect_sd,(1,len_cell_embed))\n",
    "    \n",
    "    # 対生成的effect vevtor进行置零\n",
    "    if zero_set=='by_row':\n",
    "        # 对于每个peak的effect进行相同概率的置零\n",
    "        def set_zero(a,zero_prob=0.5):\n",
    "            a[np.random.choice(len(a),replace=False,size=int(len(a)*zero_prob))]=0\n",
    "            return a\n",
    "        peak_effect=np.apply_along_axis(set_zero,1,peak_effect,zero_prob=zero_prob)\n",
    "\n",
    "    if zero_set=='all':\n",
    "        # 对于所有index选择进行置零\n",
    "        indices = np.random.choice(peak_effect.shape[1]*peak_effect.shape[0], replace=False, size=int(peak_effect.shape[1]*peak_effect.shape[0]*zero_prob))\n",
    "        peak_effect[np.unravel_index(indices, peak_effect.shape)] = 0 \n",
    "        \n",
    "    return peak_effect,lib_size_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d905e27-5dc0-4441-a822-49a43994dc0f",
   "metadata": {},
   "source": [
    "## Get Embedding\n",
    "用来获取cell的embedding的函数，包括discrete embedding、continuous embedding、single embedding，<br>\n",
    "其中single embedding用来在cell type仿真的时候单独仿真每一个cell type的细胞然后合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39e60c9-0e0a-4bb9-987c-dd4d3f28431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same):\n",
    "    embed=np.random.normal(embed_mean_same,embed_sd_same,(n_embed_same+n_embed_diff,n_cell_total))\n",
    "    index=['embedding_'+str(m+1) for m in range(n_embed_same+n_embed_diff)]\n",
    "    columns=['single cluster' for m in  range(n_cell_total)]\n",
    "    df=pd.DataFrame(embed,columns=columns,index=index)\n",
    "    \n",
    "    return df,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6953544e-a5ab-4399-936e-be0f25957939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Discrete_Embedding(pops_name,min_popsize,tree_text,\n",
    "                 n_cell_total,pops_size,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed):\n",
    "    # np.random.seed(rand_seed)\n",
    "    n_pop=len(pops_name)\n",
    "    if(n_cell_total<min_popsize*n_pop):\n",
    "        raise ValueError(\"The size of the smallest population is too big for the total number of cells\")\n",
    "\n",
    "    if not pops_size:\n",
    "        if min_popsize:# 若设定了最小pop的size，则其他pop将原来的细胞数目平均分配\n",
    "            pop_size=np.floor((n_cell_total-min_popsize)/(len(pops_name)-1))\n",
    "            left_over=n_cell_total-min_popsize-pop_size*(len(pops_name)-1)\n",
    "            pop_name_size={} #每个pop对应的size\n",
    "            for name in pops_name:\n",
    "                if name==min_pop:\n",
    "                    pop_name_size[name]=min_popsize\n",
    "                else:\n",
    "                    pop_name_size[name]=pop_size\n",
    "            pop_name_size[pops_name[pops_name.index(min_pop)-1]]+=left_over\n",
    "        else:# 未设置最小pop，直接将每个pop的cell数目均分\n",
    "            pop_size=np.floor((n_cell_total)/(len(pops_name)))\n",
    "            left_over=n_cell_total-pop_size*(len(pops_name))\n",
    "            pop_name_size={}\n",
    "            for name in pops_name:\n",
    "                pop_name_size[name]=pop_size\n",
    "            pop_name_size[pops_name[0]]+=left_over\n",
    "\n",
    "    else:# 若直接对每个pop赋予size\n",
    "        pop_name_size={}\n",
    "        for (i,name) in enumerate(pops_name):\n",
    "            pop_name_size[name]=pops_size[i]\n",
    "    # 将float转化为int \n",
    "    for key,value in pop_name_size.items():\n",
    "        pop_name_size[key]=int(value)\n",
    "\n",
    "    #--------生成不同pop之间的协方差矩阵，这里需要在你的python环境中使用R包ape\n",
    "    ape = importr('ape')\n",
    "    phyla=ape.read_tree(text=tree_text)\n",
    "    corr_matrix=np.array(ape.vcv_phylo(phyla,cor=True))\n",
    "    \n",
    "    # corr_matrix=np.eye(len(pops_name))\n",
    "\n",
    "    #--------生成embed\n",
    "        \n",
    "    embed_same,embed_diff=[],[]\n",
    "    #生成差异embedding特征对应的均值，保证不同的pop之间的相关性\n",
    "    embed_diff_mean_mv = multivariate_normal.rvs(mean=[embed_mean_diff]*n_pop, cov=corr_matrix, size=n_embed_diff)\n",
    "    for (j,name) in enumerate(pops_name):\n",
    "        #生成每个pop对应的非差异embed部分\n",
    "        embed_same_pop=np.random.normal(embed_mean_same,embed_sd_same,(n_embed_same,pop_name_size[name]))\n",
    "\n",
    "        #生成每个pop对应的差异embed部分\n",
    "        embed_diff_pop=[]\n",
    "        for k in range(n_embed_diff):\n",
    "            embed=np.random.normal(embed_diff_mean_mv[k,j],embed_sd_diff,(pop_name_size[name],))\n",
    "            embed_diff_pop.append(embed)\n",
    "        embed_diff_pop=np.vstack(embed_diff_pop)\n",
    "\n",
    "        # 对每个pop差异/非差异embed进行汇总\n",
    "        embed_same.append(embed_same_pop) # n_embed_same*pop_size\n",
    "        embed_diff.append(embed_diff_pop) # n_embed_diff*pop_size\n",
    "\n",
    "    # embed_param: len_cell_embed*n_cell_total\n",
    "    embed_same=np.hstack(embed_same)\n",
    "    embed_diff=np.hstack(embed_diff)\n",
    "    embed_param=np.vstack([embed_same,embed_diff])\n",
    "\n",
    "    columns=np.hstack([[name]*pop_name_size[name] for name in pops_name])\n",
    "    index=['same_embedding_'+str(m+1) for m in range(n_embed_same)]+['diff_embedding_'+str(m+1) for m in range(n_embed_diff)]\n",
    "    df=pd.DataFrame(embed_param,columns=columns,index=index)\n",
    "\n",
    "    \n",
    "    return df,columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "189c151d-88a1-46b8-87c8-841adc72067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Tree_Sd(branches,root,depth=0,anchor=0,rand_seed=0):# depth就是到根节点的深度;一个递归函数,用来获取细胞在每个branch上的位置以及enbedding\n",
    "    # np.random.seed(rand_seed)\n",
    "    \n",
    "    start_nodes=[i.split('-')[0] for i in branches]\n",
    "    \n",
    "    df=pd.DataFrame({'branches':[],'cell_places':[],'embeddings':[]})\n",
    "    for i in range(len(start_nodes)): \n",
    "        if root==start_nodes[i]:# 该节点对应的所有branch\n",
    "            branch=branches[i]\n",
    "            start,end,branch_len,n_cells=branch.split('-')[0],\\\n",
    "                    branch.split('-')[1],float(branch.split('-')[2]),int(branch.split('-')[3])\n",
    "            interval=branch_len/(n_cells-1)#获取interval\n",
    "            cell_places=[depth+interval*i for i in range(n_cells-1)]+[depth+branch_len]#以interval为间隔获取cell在branch上的位置\n",
    "            \n",
    "            # 获取单维所有细胞的embedding\n",
    "            embeddings=np.array([0]+list(np.cumsum(np.random.normal(0,np.sqrt(interval),(n_cells-1)))))+anchor\n",
    "            \n",
    "            df_=pd.DataFrame({'branches':[branch]*len(cell_places),'cell_places':cell_places,'embeddings':embeddings})\n",
    "            df=pd.concat([df,df_,Generate_Tree_Sd(branches,end,depth+branch_len,anchor=embeddings[-1])],axis=0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def Get_Continuous_Embedding(tree_text,n_cell_total,\n",
    "                 embed_mean_same,embed_sd_same,\n",
    "                  embed_mean_diff,embed_sd_diff,\n",
    "                 n_embed_diff,n_embed_same,rand_seed):\n",
    "    # np.random.seed(rand_seed)\n",
    "    # 构建tree\n",
    "    tree = Phylo.read(StringIO(tree_text), \"newick\")\n",
    "    \n",
    "    # 获取不同的branch，形式为‘X-X-length’\n",
    "    clades = [i for i in tree.find_clades()]\n",
    "    branch_clades=[i for i in clades if i.branch_length]\n",
    "    branches=[tree.get_path(i)[-2:] for i in branch_clades]\n",
    "    branches=[branches[i][0].name+'-'+branches[i][1].name+'-'+str(branch_clades[i].branch_length) for i in range(len(branches))]\n",
    "    \n",
    "    # 获取所有branch的长度\n",
    "    total_branch_len=sum([float(i.split('-')[2]) for i in branches])\n",
    "    \n",
    "    \n",
    "    # 获取每个branch上的细胞数目（按照branch长度进行均分）\n",
    "    n_branches_cell=[]\n",
    "    for i in range(len(branches)):\n",
    "        branch_len=float(branches[i].split('-')[2])\n",
    "        n_cells=np.floor(n_cell_total*(branch_len/total_branch_len))\n",
    "        n_branches_cell.append(n_cells)\n",
    "\n",
    "    # 将偏置加到数目最多的分支上\n",
    "    n_branches_cell[n_branches_cell.index(max(n_branches_cell))]=n_branches_cell[n_branches_cell.index(max(n_branches_cell))]+n_cell_total-sum(n_branches_cell)\n",
    "    n_branches_cell=[int(i) for i in n_branches_cell]\n",
    "    \n",
    "    # 将细胞数目加入branch，最终branch格式：A-B-1.0-200\n",
    "    branches=[branches[i]+'-'+str(n_branches_cell[i]) for i in range(len(branches))]\n",
    "    \n",
    "    # 获取root名字\n",
    "    root=clades[1].name\n",
    "    \n",
    "    # 生成continuous的embedding\n",
    "    embed_same=np.random.normal(embed_mean_same,embed_sd_same,(n_embed_same,n_cell_total))\n",
    "    embed_diff=[]\n",
    "    for i in range(n_embed_diff):\n",
    "        df_continuous=Generate_Tree_Sd(branches,root,depth=0,anchor=embed_mean_diff,rand_seed=rand_seed+i)\n",
    "        embed_diff.append(np.array(df_continuous['embeddings']))\n",
    "    embed_diff=np.vstack(embed_diff)\n",
    "    # print(embed_same.shape,embed_diff.shape)\n",
    "    # print(branches)\n",
    "    embed=np.vstack([embed_same,embed_diff])\n",
    "\n",
    "    \n",
    "    # 加上columns和index\n",
    "    columns=list(df_continuous['branches'])\n",
    "    index=['same_embedding_'+str(m+1) for m in range(n_embed_same)]+['diff_embedding_'+str(m+1) for m in range(n_embed_diff)]\n",
    "    df=pd.DataFrame(embed,columns=columns,index=index)\n",
    "    \n",
    "    return df,columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805140f0-e31b-41bb-bf3a-f9160fdc2529",
   "metadata": {},
   "source": [
    "## Get count\n",
    "根据给定的effect矩阵和cell embedding矩阵相乘，然后使用从真实参数的分布中采样得到的参数进行修正，<br>\n",
    "修正方式根据数据为count或binary分为poisson分布和bernoulli分布<br>\n",
    "- Get_Tree_Counts: 生成离散或连续数据的时候使用的函数\n",
    "- Get_Celltype_Counts：生成真实数据对应的cell type数据时使用的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9ee884-b653-4f81-ae02-27cfc59a18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_correction(i,simu_param_lib_i,lambdas_i,lambdas_sum_i,simu_param_nozero_i,n_peak):\n",
    "    global k_dict,pi_dict\n",
    "    # print(i)\n",
    "    def solve_function(unsolved_value):\n",
    "        k,pi=unsolved_value[0],unsolved_value[1]\n",
    "        return [\n",
    "            k*(1-pi)-simu_param_lib_i/(lambdas_sum_i),\n",
    "            n_peak*pi+(1-pi)*np.sum(np.exp(-lambdas_i*k))-(n_peak-simu_param_nozero_i)\n",
    "        ]\n",
    "\n",
    "    solved=fsolve(solve_function,[3,0.5],maxfev=2000)\n",
    "    k,pi=solved[0],solved[1]\n",
    "    simu1=k*(1-pi)*(lambdas_sum_i)\n",
    "    real1=simu_param_lib_i\n",
    "    if abs(simu1-real1)/real1>0.1:\n",
    "        solved=fsolve(solve_function,[20,0.5],maxfev=2000)\n",
    "    k,pi=solved[0],solved[1]\n",
    "\n",
    "    k_dict[i]=solved[0]\n",
    "    pi_dict[i]=solved[1]\n",
    "\n",
    "class zip_correction_thread(threading.Thread):\n",
    "    def __init__(self,i,simu_param_lib_i,lambdas_i,lambdas_sum_i,simu_param_nozero_i,n_peak):\n",
    "        super(zip_correction_thread, self).__init__()\n",
    "        self.i  = i\n",
    "        self.simu_param_lib_i  = simu_param_lib_i\n",
    "        self.lambdas_i  = lambdas_i\n",
    "        self.lambdas_sum_i  = lambdas_sum_i\n",
    "        self.simu_param_nozero_i  = simu_param_nozero_i\n",
    "        self.n_peak  = n_peak\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        zip_correction(self.i,self.simu_param_lib_i,self.lambdas_i,self.lambdas_sum_i,self.simu_param_nozero_i,self.n_peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae00dd5a-1690-4baa-8844-4efb921f547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Tree_Counts(peak_mean,lib_size,nozero,n_peak,n_cell_total,rand_seed,\n",
    "                    embeds_peak,embeds_lib,correct_iter,distribution='Bernoulli',\n",
    "                    activation='exp',bw_pm=1e-4,bw_lib=0.05,bw_nozero=0.05,real_param=True):\n",
    "    # np.random.seed(rand_seed)\n",
    "    if distribution=='Bernoulli' and np.max(np.exp(peak_mean)-1)>1:\n",
    "        raise ValueError('you data may not be Bernoulli distribution!')\n",
    "    \n",
    "    if real_param: #如果直接使用真实参数，peak mean直接按照真实参数来，lib size抽样\n",
    "        param_pm=np.sort(peak_mean,axis=0).ravel()\n",
    "        param_lib=np.sort(np.random.choice(lib_size,size=n_cell_total),axis=0).ravel()\n",
    "        param_nozero=np.sort(np.random.choice(nozero,size=n_cell_total),axis=0).ravel()\n",
    "    else:\n",
    "        # kde\n",
    "        kde_pm = KernelDensity(kernel='gaussian', bandwidth=bw_pm).fit(peak_mean.reshape(-1,1))\n",
    "        kde_lib = KernelDensity(kernel='gaussian', bandwidth=bw_lib).fit(lib_size.reshape(-1,1))\n",
    "        kde_nozero = KernelDensity(kernel='gaussian', bandwidth=bw_nozero).fit(nozero.reshape(-1,1))\n",
    "\n",
    "        # 从kde中采样并进行排序（从小到大）\n",
    "        param_pm=kde_pm.sample(n_peak,random_state=rand_seed)\n",
    "        param_lib=kde_lib.sample(n_cell_total,random_state=rand_seed)\n",
    "        param_nozero=kde_nozero.sample(n_cell_total,random_state=rand_seed)\n",
    "\n",
    "        param_pm=np.sort(param_pm,axis=0).ravel()\n",
    "        param_lib=np.sort(param_lib,axis=0).ravel()\n",
    "        param_nozero=np.sort(param_nozero,axis=0).ravel()\n",
    "\n",
    "\n",
    "\n",
    "        # estimation_dis='one_logser' # 'NB'/'one_logser'/'gamma'/'zero_logser'\n",
    "        \n",
    "#         print('the estimation method is ',estimation_dis)\n",
    "        \n",
    "#         if estimation_dis=='gamma':\n",
    "#             peak_mean_real = np.exp(peak_mean)-1\n",
    "#             peak_mean_sqrt = np.sqrt(peak_mean_real)\n",
    "\n",
    "#             fit_alpha, fit_loc, fit_beta = stats.gamma.fit(peak_mean_sqrt,floc=np.min(peak_mean_sqrt)-0.001)\n",
    "#             peak_mean_sqrt_sample = stats.gamma.rvs(a=fit_alpha, loc=fit_loc, scale=fit_beta, size=n_peak, random_state=rand_seed)\n",
    "#             param_pm = np.sort(peak_mean_sqrt_sample)\n",
    "#             param_pm = np.log(param_pm**2+1)\n",
    "#         elif estimation_dis=='zero_logser':\n",
    "#             peak_count_simu=zero_logser(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "#         elif estimation_dis=='one_logser':\n",
    "#             peak_count_simu=one_logser(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "#         elif estimation_dis=='zero_NB':\n",
    "#             peak_count_simu=zero_NB(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "#         elif estimation_dis=='NB':\n",
    "#             peak_count_simu=NB(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "#         elif estimation_dis=='ZIP':\n",
    "#             peak_count_simu=ZIP(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "            \n",
    "#         elif estimation_dis=='ZINB':\n",
    "#             peak_count_simu=ZINB(peak_count)\n",
    "#             param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "#             param_pm=np.sort(param_pm)\n",
    "            \n",
    "#         else:\n",
    "#             raise ValueError('wrong estimation distribution!')\n",
    "        \n",
    "#         #n,random_state = 2,2022\n",
    "#         gmm_lz = GMM(2, random_state=rand_seed)\n",
    "#         gmm_lz.fit(lib_size_log.reshape(-1,1))\n",
    "#         # [sample[0] for sample in gmm.sample(1000)]\n",
    "#         lib_size_log_sample = gmm_lz.sample(n_cell_total)[0].reshape(-1)\n",
    "#         param_lib = np.sort(lib_size_log_sample)\n",
    "        \n",
    "#         non_zero_real = np.exp(nozero)-1\n",
    "#         non_zero_log = np.log(non_zero_real)\n",
    "#         gmm_nz = GMM(2, random_state=rand_seed)\n",
    "#         gmm_nz.fit(non_zero_log.reshape(-1,1))\n",
    "#         # [sample[0] for sample in gmm.sample(1000)]\n",
    "#         non_zero_log_sample = gmm_nz.sample(n_cell_total)[0].reshape(-1)\n",
    "#         param_nozero = np.log(np.exp(np.sort(non_zero_log_sample))+1)\n",
    "\n",
    "    # 从模拟矩阵的参数顺序对应到采样的真实参数\n",
    "    X_peak=np.dot(peak_effect,embeds_peak)# peak*cell\n",
    "    X_peak=Activation(X_peak,method=activation) # 防止出现负值\n",
    "    rank=np.arange(len(X_peak))[np.mean(X_peak,axis=1).argsort().argsort()]\n",
    "    param_pm=param_pm[rank]\n",
    "\n",
    "    if two_embeds:\n",
    "        X_lib=np.dot(lib_size_effect,embeds_lib).ravel()\n",
    "    else:\n",
    "        X_lib=np.dot(lib_size_effect,embeds_peak).ravel()\n",
    "    rank = np.arange(len(X_lib))[X_lib.argsort().argsort()]\n",
    "    param_lib=param_lib[rank]\n",
    "    param_nozero=param_nozero[rank]\n",
    "\n",
    "    # 对参数进行修正\n",
    "    # X_peak维度是peak*cell\n",
    "    simu_param_peak=X_peak\n",
    "    if distribution=='Poisson':\n",
    "        for i in range(correct_iter):\n",
    "            print('correct_iter '+str(i+1))\n",
    "            simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=1).reshape(-1,1)+1e-8)*((np.exp(param_pm)-1).reshape(-1,1))*simu_param_peak.shape[1]\n",
    "            simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=0).reshape(1,-1)+1e-8)*((np.exp(param_lib)-1).reshape(1,-1))\n",
    "            \n",
    "        simu_param_lib=np.exp(param_lib)-1\n",
    "        simu_param_nozero=np.exp(param_nozero)-1\n",
    "        #--------使用poisson分布生成ATAC\n",
    "        lambdas=simu_param_peak\n",
    "        # lambdas=simu_param_peak*(simu_param_lib.reshape(1,-1))\n",
    "        \n",
    "        # 对sparsity进行修正\n",
    "        lambdas_sum=np.sum(lambdas,axis=0)\n",
    "        \n",
    "#         print(\"**********start ZIP correction...**********\")\n",
    "#         k_list,pi_list=[],[]\n",
    "#         # 求解每个cell中lambda扩大的倍数和置零的比例\n",
    "#         for i in range(n_cell_total):\n",
    "#             iter_=i\n",
    "#             # print(i)\n",
    "#             def solve_function(unsolved_value):\n",
    "#                 k,pi=unsolved_value[0],unsolved_value[1]\n",
    "#                 return [\n",
    "#                     k*(1-pi)-simu_param_lib[iter_]/(lambdas_sum[iter_]),\n",
    "#                     n_peak*pi+(1-pi)*np.sum(np.exp(-lambdas[:,iter_]*k))-(n_peak-simu_param_nozero[iter_])\n",
    "#                 ]\n",
    "\n",
    "#             solved=fsolve(solve_function,[3,0.5],maxfev=2000)\n",
    "#             k,pi=solved[0],solved[1]\n",
    "#             simu1=k*(1-pi)*(lambdas_sum[iter_])\n",
    "#             real1=simu_param_lib[iter_]\n",
    "#             if abs(simu1-real1)/real1>0.1:\n",
    "#                 print('=================================')\n",
    "#                 print(i)\n",
    "#                 print(simu1,real1)\n",
    "#                 # print('=================================')\n",
    "#                 solved=fsolve(solve_function,[20,0.5],maxfev=2000)\n",
    "#             simu1=solved[0]*(1-solved[1])*(lambdas_sum[iter_])\n",
    "#             real1=simu_param_lib[iter_]\n",
    "#             if abs(simu1-real1)/real1>0.1:\n",
    "#                 print(i)\n",
    "#                 print(simu1,real1)\n",
    "#                 print(\"=================================\")\n",
    "                \n",
    "#             k_list.append(solved[0])\n",
    "#             pi_list.append(solved[1])\n",
    "#         # 对每个cell的lambda置零并扩大相应倍数\n",
    "#         for i in range(n_cell_total):\n",
    "#             if k_list[i]==3 or k_list[i]==20 or pi_list[i]<0:\n",
    "#                 continue\n",
    "#             a=lambdas[:,i]*k_list[i]\n",
    "#             # print(i)\n",
    "#             # print(k_list[i],pi_list[i])\n",
    "#             # print(\"=============================\")\n",
    "#             # b=atac_counts[:,i]\n",
    "#             a[np.random.choice(n_peak,replace=False,size=int(pi_list[i]*n_peak))]=0\n",
    "#             lambdas[:,i]=a\n",
    "#         print(\"**********ZIP correction finished!**********\")\n",
    "            \n",
    "        print(\"**********start ZIP correction...**********\")\n",
    "        batch_size = 1000 # 并行数目，全局字典\n",
    "        global k_dict,pi_dict\n",
    "        for i in range(0,n_cell_total,batch_size):\n",
    "            if i+batch_size<=n_cell_total:\n",
    "                my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, i+batch_size)]\n",
    "            else:\n",
    "                my_thread = [zip_correction_thread(j,simu_param_lib[j],lambdas[:,j],lambdas_sum[j],simu_param_nozero[j],n_peak) for j in range(i, n_cell_total)]\n",
    "            for thread_ in my_thread:\n",
    "                thread_.start()\n",
    "            for thread_ in my_thread:\n",
    "                thread_.join()\n",
    "        # 对每个cell的lambda置零并扩大相应倍数\n",
    "        for i in range(n_cell_total):\n",
    "            if k_dict[i]==3 or k_dict[i]==20 or pi_dict[i]<0:\n",
    "                continue\n",
    "            a=lambdas[:,i]*k_dict[i]\n",
    "            # b=atac_counts[:,i]\n",
    "            a[np.random.choice(n_peak,replace=False,size=int(pi_dict[i]*n_peak))]=0\n",
    "            lambdas[:,i]=a\n",
    "            \n",
    "        print(\"**********ZIP correction finished!**********\")\n",
    "            \n",
    "        atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "    elif distribution=='Bernoulli':\n",
    "        for i in range(correct_iter):\n",
    "            print('correct_iter '+str(i+1))\n",
    "            simu_param_peak=Bernoulli_pm_correction(simu_param_peak,param_pm)\n",
    "            simu_param_peak=Bernoulli_lib_correction(simu_param_peak,param_lib)\n",
    "        atac_counts=np.random.binomial(1,p=simu_param_peak,size=simu_param_peak.shape)\n",
    "    \n",
    "    return atac_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e169b1-0d5d-4eac-8a31-b528765d05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def kl_div(peak_count,peak_count_simu):\n",
    "    # -------- K-L散度\n",
    "    peak_count_combine=np.concatenate((peak_count,peak_count_simu))\n",
    "    value=np.sort(np.unique(peak_count_combine))\n",
    "    value_count_ori,value_count_simu=[],[]\n",
    "    for value_ in value:\n",
    "        value_count_ori.append(len(np.where(peak_count==value_)[0]))\n",
    "        value_count_simu.append(len(np.where(peak_count_simu==value_)[0]))\n",
    "\n",
    "    value_count_ori=np.array(value_count_ori)\n",
    "    value_count_ori=value_count_ori/sum(value_count_ori)\n",
    "    value_count_simu=np.array(value_count_simu)\n",
    "    value_count_simu=value_count_simu/sum(value_count_simu)\n",
    "    \n",
    "    epsilon = 0.00001\n",
    "    value_count_ori+=epsilon\n",
    "    value_count_simu+=epsilon\n",
    "    \n",
    "    # print('KL divergence:',sum(rel_entr(value_count_ori, value_count_simu)))\n",
    "    return sum(rel_entr(value_count_ori, value_count_simu))\n",
    "    \n",
    "def zero_logser(peak_count):\n",
    "    peak_count_new=np.delete(peak_count,np.where(peak_count == 0))\n",
    "    zero_prob_=len(np.where(peak_count == 0)[0])/len(peak_count)\n",
    "    def solve_function(unsolved_value):\n",
    "        p=unsolved_value[0]\n",
    "        return [\n",
    "            -1*p/(np.log(1-p)*(1-p))-np.mean(peak_count_new)\n",
    "        ]\n",
    "\n",
    "    solved=fsolve(solve_function,[0.995],maxfev=2000)\n",
    "    p=solved[0]\n",
    "    # print(-1*p/(np.log(1-p)*(1-p)),np.mean(peak_count_new))\n",
    "    peak_count_simu=logser.rvs(p,size=len(peak_count))*\\\n",
    "        stats.bernoulli.rvs(p = 1-zero_prob_, size = len(peak_count)) \n",
    "    \n",
    "    return peak_count_simu\n",
    "\n",
    "def one_logser(peak_count):\n",
    "    zero_prob_=len(np.where(peak_count == 0)[0])/len(peak_count)\n",
    "    one_prob=len(np.where(peak_count == 1)[0])/len(peak_count)\n",
    "    peak_count_new=np.delete(peak_count,np.where(peak_count == 0))\n",
    "    peak_count_new=np.delete(peak_count_new,np.where(peak_count_new == 1))-1\n",
    "    # 固定0、1的概率\n",
    "    idx_all=range(len(peak_count))\n",
    "    idx_zero=np.random.choice(idx_all,replace=False,size=int(len(peak_count)*(zero_prob_)))\n",
    "    idx_one=np.random.choice(np.delete(idx_all,idx_zero),replace=False,size=int(len(peak_count)*(one_prob)))\n",
    "\n",
    "    def solve_function(unsolved_value):\n",
    "        p=unsolved_value[0]\n",
    "        return [\n",
    "            -1*p/(np.log(1-p)*(1-p))-np.mean(peak_count_new)\n",
    "        ]\n",
    "\n",
    "    solved=fsolve(solve_function,[0.995],maxfev=2000)\n",
    "    p=solved[0]\n",
    "    # print(-1*p/(np.log(1-p)*(1-p)),np.mean(peak_count_new))\n",
    "\n",
    "    peak_count_simu=logser.rvs(p,size=len(peak_count))+1\n",
    "    peak_count_simu[idx_zero]=0\n",
    "    peak_count_simu[idx_one]=1\n",
    "    \n",
    "    return peak_count_simu\n",
    "\n",
    "def ZINB(peak_count):\n",
    "    model_zinb = ZeroInflatedNegativeBinomialP(peak_count, np.ones_like(peak_count), p=1)\n",
    "    res_zinb = model_zinb.fit(method='bfgs', maxiter=5000, maxfun=5000)\n",
    "    mu = np.exp(res_zinb.params[1])\n",
    "    alpha = res_zinb.params[2]\n",
    "    pi = expit(res_zinb.params[0])\n",
    "\n",
    "    p=1/(1+alpha)\n",
    "    n=mu*p/(1-p)\n",
    "\n",
    "    peak_count_simu=(nbinom.rvs(n,p,size=len(peak_count)))*\\\n",
    "        stats.bernoulli.rvs(p = 1-pi, size = len(peak_count))\n",
    "    \n",
    "    return peak_count_simu\n",
    "\n",
    "def zero_NB(peak_count):\n",
    "    zero_prob=len(np.where(peak_count == 0)[0])/len(peak_count)\n",
    "    peak_count_new=np.delete(peak_count,np.where(peak_count == 0))\n",
    "    res=sm.NegativeBinomial(peak_count_new-1, np.ones_like(peak_count_new)).fit(start_params=[1,1])\n",
    "    mu=np.exp(res.params[0])\n",
    "    p=1/(1+mu*res.params[1])\n",
    "    n=mu*p/(1-p)\n",
    "\n",
    "    peak_count_simu=(nbinom.rvs(n,p,size=len(peak_count))+1)*\\\n",
    "        stats.bernoulli.rvs(p = 1-zero_prob, size = len(peak_count))\n",
    "    \n",
    "    return peak_count_simu\n",
    "    \n",
    "def NB(peak_count):\n",
    "    res=sm.NegativeBinomial(peak_count, np.ones_like(peak_count)).fit(start_params=[1,1])\n",
    "    mu=np.exp(res.params[0])\n",
    "    p=1/(1+mu*res.params[1])\n",
    "    n=mu*p/(1-p)\n",
    "\n",
    "    peak_count_simu=nbinom.rvs(n,p,size=len(peak_count))\n",
    "    \n",
    "    return peak_count_simu\n",
    "\n",
    "def ZIP(peak_count):\n",
    "    zip_model = ZeroInflatedPoisson(endog = peak_count, exog= np.ones_like(peak_count)) \n",
    "    zip_res = zip_model.fit()\n",
    "    mu=zip_res.params[1]\n",
    "    pi = expit(zip_res.params[0])\n",
    "    peak_count_simu = stats.bernoulli.rvs(p = 1-pi, size = len(peak_count))*\\\n",
    "            stats.poisson.rvs(mu = mu, size = len(peak_count))\n",
    "    \n",
    "    return peak_count_simu\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e8f9aa3-93b6-434d-b79d-6884a77b8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Celltype_Counts(adata_part,two_embeds,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same,correct_iter=10,lib_simu='real',n_cell_total=None,\n",
    "                       distribution='Poisson',activation='sigmod'\n",
    "                       ,bw_pm=1e-4,bw_lib=0.05,bw_nozero=0.05,rand_seed=0):# 如果lib_simu为‘estimate’则需要提供对应的n_cell_total\n",
    "    \n",
    "    # np.random.seed(rand_seed)\n",
    "    # 计算真实参数\n",
    "    peak_mean=np.log(cal_pm(adata_part)+1)\n",
    "    lib_size=np.log(cal_lib(adata_part)+1)\n",
    "    nozero=np.log(cal_nozero(adata_part)+1)\n",
    "    peak_count=cal_peak_count(adata_part)\n",
    "    \n",
    "    if distribution=='Bernoulli' and np.max(np.exp(peak_mean)-1)>1:\n",
    "        raise ValueError('you data may not be Bernoulli distribution!')\n",
    "    \n",
    "    n_peak         =len(peak_mean)\n",
    "    n_cell_total   =len(lib_size) #总共的细胞数目\n",
    "    if lib_simu=='real':\n",
    "        # param_lib=lib_size\n",
    "        param_lib=np.sort(np.random.choice(lib_size,size=n_cell_total),axis=0).ravel()\n",
    "        param_nozero=np.sort(np.random.choice(nozero,size=n_cell_total),axis=0).ravel()\n",
    "    elif lib_simu=='estimate':\n",
    "        # kde_lib = KernelDensity(kernel='gaussian', bandwidth=bw_lib).fit(lib_size.reshape(-1,1))\n",
    "        # param_lib=kde_lib.sample(n_cell_total,random_state=rand_seed)\n",
    "        # param_lib=np.sort(param_lib)\n",
    "        \n",
    "        estimation_dis='one_logser' # 'NB'/'one_logser'/'gamma'/'zero_logser'\n",
    "        \n",
    "        print('the estimation method is ',estimation_dis)\n",
    "        \n",
    "        if estimation_dis=='gamma':\n",
    "            peak_mean_real = np.exp(peak_mean)-1\n",
    "            peak_mean_sqrt = np.sqrt(peak_mean_real)\n",
    "\n",
    "            fit_alpha, fit_loc, fit_beta = stats.gamma.fit(peak_mean_sqrt,floc=np.min(peak_mean_sqrt)-0.001)\n",
    "            peak_mean_sqrt_sample = stats.gamma.rvs(a=fit_alpha, loc=fit_loc, scale=fit_beta, size=n_peak, random_state=rand_seed)\n",
    "            param_pm = np.sort(peak_mean_sqrt_sample)\n",
    "            param_pm = np.log(param_pm**2+1)\n",
    "        elif estimation_dis=='zero_logser':\n",
    "            peak_count_simu=zero_logser(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "        elif estimation_dis=='one_logser':\n",
    "            peak_count_simu=one_logser(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "        elif estimation_dis=='zero_NB':\n",
    "            peak_count_simu=zero_NB(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "        elif estimation_dis=='NB':\n",
    "            peak_count_simu=NB(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "        elif estimation_dis=='ZIP':\n",
    "            peak_count_simu=ZIP(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "            \n",
    "        elif estimation_dis=='ZINB':\n",
    "            peak_count_simu=ZINB(peak_count)\n",
    "            param_pm=np.log(peak_count_simu/n_cell_total+1)\n",
    "            param_pm=np.sort(param_pm)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('wrong estimation distribution!')\n",
    "            \n",
    "        \n",
    "        \n",
    "        lib_size_real = np.exp(lib_size)-1\n",
    "        lib_size_log = np.log(lib_size_real)\n",
    "        \n",
    "        #n,random_state = 2,2022\n",
    "        gmm_lz = GMM(2, random_state=rand_seed)\n",
    "        gmm_lz.fit(lib_size_log.reshape(-1,1))\n",
    "        # [sample[0] for sample in gmm.sample(1000)]\n",
    "        lib_size_log_sample = gmm_lz.sample(n_cell_total)[0].reshape(-1)\n",
    "        param_lib = np.sort(lib_size_log_sample)\n",
    "        \n",
    "        non_zero_real = np.exp(nozero)-1\n",
    "        non_zero_log = np.log(non_zero_real)\n",
    "        gmm_nz = GMM(2, random_state=rand_seed)\n",
    "        gmm_nz.fit(non_zero_log.reshape(-1,1))\n",
    "        # [sample[0] for sample in gmm.sample(1000)]\n",
    "        non_zero_log_sample = gmm_nz.sample(n_cell_total)[0].reshape(-1)\n",
    "        param_nozero = np.log(np.exp(np.sort(non_zero_log_sample))+1)\n",
    "\n",
    "    param_pm=param_pm[peak_mean.argsort().argsort()]\n",
    "    # param_pm=np.sort(peak_mean)\n",
    "    # origin_peak=np.arange(len(peak_mean))[peak_mean.argsort()]#记录实际peak的位置，保证最后输出的与输入peak含义一致\n",
    "\n",
    "    # 生成effect和embedding\n",
    "    peak_effect,lib_size_effect=Get_Effect(n_peak,n_cell_total,\n",
    "                    len_cell_embed,rand_seed,zero_prob,zero_set,effect_mean,effect_sd)\n",
    "\n",
    "    # if simu_type=='single':\n",
    "    embeds_param={}\n",
    "    embeds_param['peak'],meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "    embeds_param['lib_size'],meta=Get_Single_Embedding(n_cell_total,embed_mean_same,embed_sd_same,\n",
    "                 n_embed_diff,n_embed_same)\n",
    "\n",
    "\n",
    "    # 从模拟矩阵的参数顺序对应到采样的真实参数\n",
    "    X_peak=np.dot(peak_effect,embeds_param['peak'].values)# peak*cell\n",
    "    X_peak=Activation(X_peak,method=activation)\n",
    "    # rank=np.arange(len(X_peak))[np.mean(X_peak,axis=1).argsort().argsort()]\n",
    "    # param_pm=param_pm[rank]\n",
    "    # origin_peak=origin_peak[rank]\n",
    "\n",
    "    if two_embeds:\n",
    "        X_lib=np.dot(lib_size_effect,embeds_param['lib_size'].values).ravel()\n",
    "    else:\n",
    "        X_lib=np.dot(lib_size_effect,embeds_param['peak'].values).ravel()\n",
    "    rank = np.arange(len(X_lib))[X_lib.argsort().argsort()]\n",
    "    param_lib=param_lib[rank]\n",
    "    param_nozero=param_nozero[rank]\n",
    "    \n",
    "\n",
    "    # 对参数进行修正\n",
    "    # X_peak维度是peak*cell\n",
    "    simu_param_peak=X_peak\n",
    "    if distribution=='Poisson':\n",
    "        for i in range(correct_iter):\n",
    "            print('correct_iter '+str(i+1))\n",
    "            simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=1).reshape(-1,1)+1e-8)*((np.exp(param_pm)-1).reshape(-1,1))*simu_param_peak.shape[1]\n",
    "            simu_param_peak=simu_param_peak/(np.sum(simu_param_peak,axis=0).reshape(1,-1)+1e-8)*((np.exp(param_lib)-1).reshape(1,-1))   # 分母加一个很小的数防止nan\n",
    "            \n",
    "        simu_param_lib=np.exp(param_lib)-1\n",
    "        simu_param_nozero=np.exp(param_nozero)-1\n",
    "        simu_param_pm=np.exp(param_pm)-1\n",
    "        #--------使用poisson分布生成ATAC\n",
    "        lambdas=simu_param_peak\n",
    "        # lambdas=lambdas[origin_peak.argsort(),:] #保证peak与输入peak一致\n",
    " \n",
    "        atac_counts=np.random.poisson(lambdas, lambdas.shape)\n",
    "    elif distribution=='Bernoulli':\n",
    "        for i in range(correct_iter):\n",
    "            print('correct_iter '+str(i+1))\n",
    "            simu_param_peak=Bernoulli_pm_correction(simu_param_peak,param_pm)\n",
    "            simu_param_peak=Bernoulli_lib_correction(simu_param_peak,param_lib)\n",
    "        atac_counts=np.random.binomial(1,p=simu_param_peak,size=simu_param_peak.shape)\n",
    "        \n",
    "        lambdas,simu_param_nozero,simu_param_lib,simu_param_pm=None,None,None,None\n",
    "    \n",
    "    return atac_counts,embeds_param['peak'].values,embeds_param['lib_size'].values,lambdas,simu_param_nozero,simu_param_lib,simu_param_pm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
